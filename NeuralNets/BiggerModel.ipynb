{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=512, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        #cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "        \n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "        \n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 512)\n",
       "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5646382076899789  train loss:  1.3789129701767178 --time: 26.448330402374268\n",
      "0 validation:  0.8095991561181435 --max 0.8095991561181435 --time: 30.79268193244934\n",
      "train acc:  0.8152329051511613  train loss:  0.5727711971952112 --time: 26.35055184364319\n",
      "1 validation:  0.8842299578059072 --max 0.8842299578059072 --time: 30.55283260345459\n",
      "train acc:  0.8778599541542321  train loss:  0.3801162282406296 --time: 27.00781273841858\n",
      "2 validation:  0.8900316455696202 --max 0.8900316455696202 --time: 31.25471329689026\n",
      "train acc:  0.90532416417975  train loss:  0.29274620436831733 --time: 27.329838275909424\n",
      "3 validation:  0.9029535864978903 --max 0.9029535864978903 --time: 31.518543004989624\n",
      "train acc:  0.9309718437783833  train loss:  0.21585812526677853 --time: 26.9039409160614\n",
      "4 validation:  0.9074367088607594 --max 0.9074367088607594 --time: 31.24772000312805\n",
      "train acc:  0.948877643700532  train loss:  0.16206378016353312 --time: 27.159830808639526\n",
      "5 validation:  0.9106012658227848 --max 0.9106012658227848 --time: 31.54194664955139\n",
      "train acc:  0.9621988668310194  train loss:  0.1244222053753737 --time: 26.892889976501465\n",
      "6 validation:  0.9100738396624473 --max 0.9106012658227848 --time: 31.030250072479248\n",
      "train acc:  0.970892262445396  train loss:  0.09367977716646142 --time: 27.515459299087524\n",
      "7 validation:  0.9100738396624473 --max 0.9106012658227848 --time: 31.729995489120483\n",
      "train acc:  0.9782881363262834  train loss:  0.07475950933210758 --time: 27.570846557617188\n",
      "8 validation:  0.9069092827004219 --max 0.9106012658227848 --time: 31.67995309829712\n",
      "train acc:  0.9836944768824878  train loss:  0.05830303642343092 --time: 27.52783465385437\n",
      "9 validation:  0.9074367088607594 --max 0.9106012658227848 --time: 31.530794620513916\n",
      "train acc:  0.9868950304917607  train loss:  0.048123700605983234 --time: 27.682941913604736\n",
      "10 validation:  0.9135021097046413 --max 0.9135021097046413 --time: 31.686716079711914\n",
      "train acc:  0.9854677565849228  train loss:  0.049436401251641754 --time: 27.575294017791748\n",
      "11 validation:  0.9098101265822784 --max 0.9135021097046413 --time: 31.57755422592163\n",
      "train acc:  0.9860732667272177  train loss:  0.04893296989946734 --time: 27.80411672592163\n",
      "12 validation:  0.9050632911392406 --max 0.9135021097046413 --time: 31.822964668273926\n",
      "train acc:  0.9892738203364906  train loss:  0.037321461421226595 --time: 27.341710329055786\n",
      "13 validation:  0.9103375527426161 --max 0.9135021097046413 --time: 31.32243013381958\n",
      "train acc:  0.9930366333636088  train loss:  0.025822405599170872 --time: 27.515265464782715\n",
      "14 validation:  0.9121835443037974 --max 0.9135021097046413 --time: 31.535810470581055\n",
      "train acc:  0.9849054971670775  train loss:  0.049347211802326846 --time: 27.3802969455719\n",
      "15 validation:  0.9021624472573839 --max 0.9135021097046413 --time: 31.492209672927856\n",
      "train acc:  0.9867652783184119  train loss:  0.0449950373088375 --time: 27.063040494918823\n",
      "16 validation:  0.9032172995780591 --max 0.9135021097046413 --time: 31.236854076385498\n",
      "train acc:  0.9934691406081052  train loss:  0.02391333372501955 --time: 26.90735697746277\n",
      "17 validation:  0.9169303797468354 --max 0.9169303797468354 --time: 31.233436107635498\n",
      "train acc:  0.9957181782794862  train loss:  0.017120528014284143 --time: 26.953872203826904\n",
      "18 validation:  0.9132383966244726 --max 0.9169303797468354 --time: 31.227813243865967\n",
      "train acc:  0.9961506855239826  train loss:  0.014149322279327605 --time: 27.23322606086731\n",
      "19 validation:  0.9129746835443038 --max 0.9169303797468354 --time: 31.509084224700928\n",
      "train acc:  0.9894468232342891  train loss:  0.03632543144557234 --time: 27.10104537010193\n",
      "20 validation:  0.9013713080168776 --max 0.9169303797468354 --time: 31.388543605804443\n",
      "train acc:  0.9807966783443622  train loss:  0.057110845173055626 --time: 27.01035475730896\n",
      "21 validation:  0.9077004219409283 --max 0.9169303797468354 --time: 31.227957010269165\n",
      "train acc:  0.987457289909606  train loss:  0.03924841532298461 --time: 26.89162516593933\n",
      "22 validation:  0.9119198312236287 --max 0.9169303797468354 --time: 31.05441951751709\n",
      "train acc:  0.9944206565459971  train loss:  0.019023363527363342 --time: 27.093668937683105\n",
      "23 validation:  0.9142932489451476 --max 0.9169303797468354 --time: 31.251007556915283\n",
      "train acc:  0.9966696942173782  train loss:  0.011964182983968119 --time: 27.281810522079468\n",
      "24 validation:  0.9129746835443038 --max 0.9169303797468354 --time: 31.328721046447754\n",
      "train acc:  0.9961506855239826  train loss:  0.01458902345862219 --time: 27.479593992233276\n",
      "25 validation:  0.917457805907173 --max 0.917457805907173 --time: 31.349674940109253\n",
      "train acc:  0.9981402188486657  train loss:  0.007456658313016134 --time: 27.277817726135254\n",
      "26 validation:  0.9193037974683544 --max 0.9193037974683544 --time: 31.29586386680603\n",
      "train acc:  0.9976644608797197  train loss:  0.007089184409969625 --time: 27.374385356903076\n",
      "27 validation:  0.9158755274261603 --max 0.9193037974683544 --time: 31.19886088371277\n",
      "train acc:  0.9970589507374249  train loss:  0.009915532794130133 --time: 27.690746068954468\n",
      "28 validation:  0.9042721518987342 --max 0.9193037974683544 --time: 31.319666385650635\n",
      "train acc:  0.9875870420829549  train loss:  0.04032608274838666 --time: 27.645710706710815\n",
      "29 validation:  0.8900316455696202 --max 0.9193037974683544 --time: 31.264616012573242\n",
      "train acc:  0.9808831797932616  train loss:  0.05716114845744319 --time: 27.985941648483276\n",
      "30 validation:  0.9034810126582279 --max 0.9193037974683544 --time: 31.730424165725708\n",
      "train acc:  0.9903550884477315  train loss:  0.031183373810754297 --time: 27.674782037734985\n",
      "31 validation:  0.9098101265822784 --max 0.9193037974683544 --time: 31.388359785079956\n",
      "train acc:  0.9915661087323213  train loss:  0.02810361024084918 --time: 27.518548250198364\n",
      "32 validation:  0.9063818565400844 --max 0.9193037974683544 --time: 31.264090299606323\n",
      "train acc:  0.995977682626184  train loss:  0.014517603055844135 --time: 27.698859453201294\n",
      "33 validation:  0.915084388185654 --max 0.9193037974683544 --time: 31.618037700653076\n",
      "train acc:  0.9971887029107738  train loss:  0.009130892839066642 --time: 27.7551212310791\n",
      "34 validation:  0.9158755274261603 --max 0.9193037974683544 --time: 31.722545623779297\n",
      "train acc:  0.9981402188486657  train loss:  0.00584816830846975 --time: 27.655874729156494\n",
      "35 validation:  0.9135021097046413 --max 0.9193037974683544 --time: 31.53777813911438\n",
      "train acc:  0.9982699710220146  train loss:  0.00578131824757554 --time: 27.737261056900024\n",
      "36 validation:  0.9171940928270043 --max 0.9193037974683544 --time: 31.74839425086975\n",
      "train acc:  0.998226720297565  train loss:  0.005388365636463277 --time: 27.570637464523315\n",
      "37 validation:  0.9158755274261603 --max 0.9193037974683544 --time: 31.55162739753723\n",
      "train acc:  0.998226720297565  train loss:  0.005198558333242384 --time: 27.769653797149658\n",
      "38 validation:  0.9148206751054853 --max 0.9193037974683544 --time: 31.82929515838623\n",
      "train acc:  0.9973184550841226  train loss:  0.00790484673880653 --time: 27.65517234802246\n",
      "39 validation:  0.9113924050632911 --max 0.9193037974683544 --time: 31.778144121170044\n",
      "train acc:  0.995631676830587  train loss:  0.013844102683145223 --time: 27.64102339744568\n",
      "40 validation:  0.9100738396624473 --max 0.9193037974683544 --time: 31.66066813468933\n",
      "train acc:  0.9693352363652091  train loss:  0.09082435421357497 --time: 27.467677116394043\n",
      "41 validation:  0.8995253164556962 --max 0.9193037974683544 --time: 31.483518600463867\n",
      "train acc:  0.9856407594827213  train loss:  0.04500499623313094 --time: 27.6393883228302\n",
      "42 validation:  0.9132383966244726 --max 0.9193037974683544 --time: 31.493759632110596\n",
      "train acc:  0.9950694174127417  train loss:  0.017421591578010286 --time: 27.635164737701416\n",
      "43 validation:  0.9153481012658228 --max 0.9193037974683544 --time: 31.587724924087524\n",
      "train acc:  0.9974482072574715  train loss:  0.008937736530659606 --time: 27.777942419052124\n",
      "44 validation:  0.9182489451476793 --max 0.9193037974683544 --time: 31.605945110321045\n",
      "train acc:  0.9981834695731153  train loss:  0.005623444654652077 --time: 27.447425365447998\n",
      "45 validation:  0.9195675105485233 --max 0.9195675105485233 --time: 31.204657554626465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9981402188486657  train loss:  0.004987733037749985 --time: 27.562050580978394\n",
      "46 validation:  0.9179852320675106 --max 0.9195675105485233 --time: 31.30233097076416\n",
      "train acc:  0.9983132217464643  train loss:  0.004782414254915736 --time: 27.91076898574829\n",
      "47 validation:  0.9200949367088608 --max 0.9200949367088608 --time: 31.51047158241272\n",
      "train acc:  0.9984429739198132  train loss:  0.004682355720111817 --time: 27.84130573272705\n",
      "48 validation:  0.9206223628691983 --max 0.9206223628691983 --time: 31.418871879577637\n",
      "train acc:  0.9984862246442628  train loss:  0.004101992317363466 --time: 27.8574116230011\n",
      "49 validation:  0.9211497890295358 --max 0.9211497890295358 --time: 31.058103322982788\n",
      "train acc:  0.9983564724709139  train loss:  0.004740759586427844 --time: 27.724494695663452\n",
      "50 validation:  0.922204641350211 --max 0.922204641350211 --time: 31.110470056533813\n",
      "train acc:  0.9983564724709139  train loss:  0.004565700749707156 --time: 28.075292587280273\n",
      "51 validation:  0.9211497890295358 --max 0.922204641350211 --time: 31.545506238937378\n",
      "train acc:  0.9981402188486657  train loss:  0.004828982524627187 --time: 28.068963766098022\n",
      "52 validation:  0.9208860759493671 --max 0.922204641350211 --time: 31.684393405914307\n",
      "train acc:  0.9981834695731153  train loss:  0.00462444462345203 --time: 27.943821907043457\n",
      "53 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.69334053993225\n",
      "train acc:  0.998226720297565  train loss:  0.004533043548815951 --time: 27.65526294708252\n",
      "54 validation:  0.9211497890295358 --max 0.922204641350211 --time: 31.669381618499756\n",
      "train acc:  0.9983564724709139  train loss:  0.004546344630051667 --time: 27.516492128372192\n",
      "55 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.614954233169556\n",
      "train acc:  0.9983132217464643  train loss:  0.0043475300435652795 --time: 27.584118843078613\n",
      "56 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.759838342666626\n",
      "train acc:  0.9983997231953635  train loss:  0.004534162073981439 --time: 27.53951358795166\n",
      "57 validation:  0.922204641350211 --max 0.922204641350211 --time: 31.862487316131592\n",
      "train acc:  0.9984429739198132  train loss:  0.004308058432295651 --time: 27.169525384902954\n",
      "58 validation:  0.9211497890295358 --max 0.922204641350211 --time: 31.48907709121704\n",
      "train acc:  0.9982699710220146  train loss:  0.004705845607069671 --time: 27.09009575843811\n",
      "59 validation:  0.9206223628691983 --max 0.922204641350211 --time: 31.352668285369873\n",
      "train acc:  0.998226720297565  train loss:  0.00427324537545422 --time: 27.148833990097046\n",
      "60 validation:  0.9208860759493671 --max 0.922204641350211 --time: 31.394852876663208\n",
      "train acc:  0.9983132217464643  train loss:  0.004223005804858744 --time: 27.199217319488525\n",
      "61 validation:  0.9208860759493671 --max 0.922204641350211 --time: 31.465149641036987\n",
      "train acc:  0.9983997231953635  train loss:  0.0038580967639013355 --time: 27.06440281867981\n",
      "62 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.377438068389893\n",
      "train acc:  0.9981402188486657  train loss:  0.004718301532324082 --time: 27.18180203437805\n",
      "63 validation:  0.9203586497890295 --max 0.922204641350211 --time: 31.307957649230957\n",
      "train acc:  0.9980969681242161  train loss:  0.004570066977624501 --time: 27.115872144699097\n",
      "64 validation:  0.919831223628692 --max 0.922204641350211 --time: 31.46146273612976\n",
      "train acc:  0.9978374637775183  train loss:  0.005814437247225526 --time: 27.103496551513672\n",
      "65 validation:  0.9090189873417721 --max 0.922204641350211 --time: 31.465077877044678\n",
      "train acc:  0.9314043510228797  train loss:  0.21170322998627528 --time: 26.95138669013977\n",
      "66 validation:  0.9077004219409283 --max 0.922204641350211 --time: 31.39626932144165\n",
      "train acc:  0.9817914450067039  train loss:  0.05536052299591389 --time: 27.119184732437134\n",
      "67 validation:  0.9121835443037974 --max 0.922204641350211 --time: 31.497069597244263\n",
      "train acc:  0.9954154232083388  train loss:  0.015975976166011797 --time: 27.02519202232361\n",
      "68 validation:  0.9166666666666666 --max 0.922204641350211 --time: 31.449806928634644\n",
      "train acc:  0.9978807145019679  train loss:  0.0073018972099455105 --time: 27.04209876060486\n",
      "69 validation:  0.9190400843881856 --max 0.922204641350211 --time: 31.44960641860962\n",
      "train acc:  0.9979239652264176  train loss:  0.005897706426945077 --time: 26.979161977767944\n",
      "70 validation:  0.9206223628691983 --max 0.922204641350211 --time: 31.373384952545166\n",
      "train acc:  0.9981402188486657  train loss:  0.0047727306013237375 --time: 26.99143695831299\n",
      "71 validation:  0.9185126582278481 --max 0.922204641350211 --time: 31.371045112609863\n",
      "train acc:  0.9979672159508671  train loss:  0.0045299888754106584 --time: 27.14048194885254\n",
      "72 validation:  0.9195675105485233 --max 0.922204641350211 --time: 31.509666442871094\n",
      "train acc:  0.9983997231953635  train loss:  0.004319691707673406 --time: 27.022689819335938\n",
      "73 validation:  0.9203586497890295 --max 0.922204641350211 --time: 31.389206647872925\n",
      "train acc:  0.9983132217464643  train loss:  0.00411144909108037 --time: 27.047487497329712\n",
      "74 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.241286516189575\n",
      "train acc:  0.998226720297565  train loss:  0.004335299988234806 --time: 27.24841570854187\n",
      "75 validation:  0.9182489451476793 --max 0.922204641350211 --time: 31.424464225769043\n",
      "train acc:  0.9983564724709139  train loss:  0.003945491248381566 --time: 27.36897921562195\n",
      "76 validation:  0.919831223628692 --max 0.922204641350211 --time: 31.46098804473877\n",
      "train acc:  0.9983564724709139  train loss:  0.003998288958965677 --time: 27.354795455932617\n",
      "77 validation:  0.9195675105485233 --max 0.922204641350211 --time: 31.363710165023804\n",
      "train acc:  0.998572726093162  train loss:  0.0038732219061242464 --time: 27.469614028930664\n",
      "78 validation:  0.919831223628692 --max 0.922204641350211 --time: 31.405974864959717\n",
      "train acc:  0.9983132217464643  train loss:  0.004168181585032848 --time: 27.699490785598755\n",
      "79 validation:  0.9211497890295358 --max 0.922204641350211 --time: 31.736183881759644\n",
      "train acc:  0.9983997231953635  train loss:  0.004020917567666311 --time: 27.275383234024048\n",
      "80 validation:  0.9195675105485233 --max 0.922204641350211 --time: 31.467241048812866\n",
      "train acc:  0.9981834695731153  train loss:  0.004153362410937343 --time: 27.389840602874756\n",
      "81 validation:  0.9219409282700421 --max 0.922204641350211 --time: 31.557034492492676\n",
      "train acc:  0.9981834695731153  train loss:  0.004187193663484669 --time: 27.41420269012451\n",
      "82 validation:  0.9203586497890295 --max 0.922204641350211 --time: 31.499616384506226\n",
      "train acc:  0.9982699710220146  train loss:  0.004027428208301693 --time: 27.561461448669434\n",
      "83 validation:  0.9187763713080169 --max 0.922204641350211 --time: 31.66264319419861\n",
      "train acc:  0.9983132217464643  train loss:  0.0037390187691664968 --time: 27.504287719726562\n",
      "84 validation:  0.9208860759493671 --max 0.922204641350211 --time: 31.512076377868652\n",
      "train acc:  0.9983997231953635  train loss:  0.004001181744967166 --time: 27.471073150634766\n",
      "85 validation:  0.9208860759493671 --max 0.922204641350211 --time: 31.52672576904297\n",
      "train acc:  0.9983564724709139  train loss:  0.003938151241959656 --time: 27.479727029800415\n",
      "86 validation:  0.9211497890295358 --max 0.922204641350211 --time: 31.335691213607788\n",
      "train acc:  0.998226720297565  train loss:  0.004142682990850366 --time: 27.766629934310913\n",
      "87 validation:  0.922204641350211 --max 0.922204641350211 --time: 31.621427536010742\n",
      "train acc:  0.9982699710220146  train loss:  0.003996637605776424 --time: 27.731080055236816\n",
      "88 validation:  0.917457805907173 --max 0.922204641350211 --time: 31.55677366256714\n",
      "train acc:  0.9973184550841226  train loss:  0.00793017697540975 --time: 27.705549955368042\n",
      "89 validation:  0.9013713080168776 --max 0.922204641350211 --time: 31.47953200340271\n",
      "train acc:  0.948401885731586  train loss:  0.15897461872300883 --time: 27.649927139282227\n",
      "90 validation:  0.9032172995780591 --max 0.922204641350211 --time: 31.597360134124756\n",
      "train acc:  0.9841702348514337  train loss:  0.04571725511203072 --time: 27.370721101760864\n",
      "91 validation:  0.9061181434599156 --max 0.922204641350211 --time: 31.41765284538269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9951991695860906  train loss:  0.01542196069195743 --time: 27.60678791999817\n",
      "92 validation:  0.9124472573839663 --max 0.922204641350211 --time: 31.686565399169922\n",
      "train acc:  0.9971454521863241  train loss:  0.009364481126498533 --time: 27.20387101173401\n",
      "93 validation:  0.9140295358649789 --max 0.922204641350211 --time: 31.300166368484497\n",
      "train acc:  0.9981402188486657  train loss:  0.00524707282192295 --time: 27.26246953010559\n",
      "94 validation:  0.9177215189873418 --max 0.922204641350211 --time: 31.401384353637695\n",
      "train acc:  0.9984429739198132  train loss:  0.004659941887549891 --time: 27.35904049873352\n",
      "95 validation:  0.9153481012658228 --max 0.922204641350211 --time: 31.537846565246582\n",
      "train acc:  0.9983564724709139  train loss:  0.004183907303971995 --time: 27.441622495651245\n",
      "96 validation:  0.9169303797468354 --max 0.922204641350211 --time: 31.625519514083862\n",
      "train acc:  0.998226720297565  train loss:  0.004117662497591587 --time: 27.437138557434082\n",
      "97 validation:  0.9171940928270043 --max 0.922204641350211 --time: 31.52119255065918\n",
      "train acc:  0.9981834695731153  train loss:  0.004163867975514413 --time: 27.606226444244385\n",
      "98 validation:  0.9171940928270043 --max 0.922204641350211 --time: 31.603589057922363\n",
      "train acc:  0.9981834695731153  train loss:  0.0038390947932419907 --time: 27.386454582214355\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 layer, 512, only 2 CNN contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
