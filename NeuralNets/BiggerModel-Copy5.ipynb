{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'domain'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=512, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "        #input = F.relu(cnn_output)\n",
    "        \n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 512)\n",
       "  (linear): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain train\n",
      "train acc:  0.6836209506509234  train loss:  0.787785612087882 --time: 27.492029905319214\n",
      "0 validation:  0.8591772151898734 --max 0.8591772151898734 --time: 31.47409701347351\n",
      "train acc:  0.864149474503698  train loss:  0.36246624029143737 --time: 27.853115797042847\n",
      "1 validation:  0.9140295358649789 --max 0.9140295358649789 --time: 31.722460508346558\n",
      "train acc:  0.9064919337398901  train loss:  0.2502493569781767 --time: 27.731736421585083\n",
      "2 validation:  0.9332805907172996 --max 0.9332805907172996 --time: 31.641839742660522\n",
      "train acc:  0.9295013191470957  train loss:  0.19262074617391134 --time: 27.909013986587524\n",
      "3 validation:  0.9332805907172996 --max 0.9332805907172996 --time: 31.68420958518982\n",
      "train acc:  0.9482288828337875  train loss:  0.14559737438572704 --time: 27.641254663467407\n",
      "4 validation:  0.930379746835443 --max 0.9332805907172996 --time: 31.581095695495605\n",
      "train acc:  0.9633233856667099  train loss:  0.09995953808667252 --time: 27.579711198806763\n",
      "5 validation:  0.9385548523206751 --max 0.9385548523206751 --time: 31.728381633758545\n",
      "train acc:  0.9716707754854894  train loss:  0.07912063119078867 --time: 27.661205053329468\n",
      "6 validation:  0.9324894514767933 --max 0.9385548523206751 --time: 31.81447124481201\n",
      "train acc:  0.9791964015397258  train loss:  0.05931551376285639 --time: 27.62857151031494\n",
      "7 validation:  0.9330168776371308 --max 0.9385548523206751 --time: 31.735601663589478\n",
      "train acc:  0.9840837334025345  train loss:  0.046861970812617414 --time: 27.40487790107727\n",
      "8 validation:  0.9306434599156118 --max 0.9385548523206751 --time: 31.74320340156555\n",
      "train acc:  0.9841702348514337  train loss:  0.04423109051457591 --time: 27.48913836479187\n",
      "9 validation:  0.9348628691983122 --max 0.9385548523206751 --time: 31.58157444000244\n",
      "train acc:  0.9868950304917607  train loss:  0.03801666520206638 --time: 27.540330171585083\n",
      "10 validation:  0.9353902953586498 --max 0.9385548523206751 --time: 31.925610303878784\n",
      "train acc:  0.9911336014878249  train loss:  0.02516793917018138 --time: 27.18366265296936\n",
      "11 validation:  0.9335443037974683 --max 0.9385548523206751 --time: 31.601007223129272\n",
      "train acc:  0.989100817438692  train loss:  0.03117408228070779 --time: 27.141778707504272\n",
      "12 validation:  0.9319620253164557 --max 0.9385548523206751 --time: 31.46341633796692\n",
      "train acc:  0.9916526101812205  train loss:  0.024253035559787195 --time: 27.054714679718018\n",
      "13 validation:  0.9367088607594937 --max 0.9385548523206751 --time: 31.429582118988037\n",
      "train acc:  0.9951559188616409  train loss:  0.014601401734173566 --time: 26.982343912124634\n",
      "14 validation:  0.9364451476793249 --max 0.9385548523206751 --time: 31.2236487865448\n",
      "train acc:  0.9930366333636088  train loss:  0.020614503565738337 --time: 27.197078227996826\n",
      "15 validation:  0.9306434599156118 --max 0.9385548523206751 --time: 31.562981128692627\n",
      "train acc:  0.9883223043985987  train loss:  0.032722490084568526 --time: 27.246095180511475\n",
      "16 validation:  0.9272151898734177 --max 0.9385548523206751 --time: 31.607786417007446\n",
      "train acc:  0.9907443449677782  train loss:  0.028109424973925504 --time: 27.149901151657104\n",
      "17 validation:  0.9335443037974683 --max 0.9385548523206751 --time: 31.428204774856567\n",
      "train acc:  0.9964101898706803  train loss:  0.012198068582625206 --time: 27.53994369506836\n",
      "18 validation:  0.9356540084388185 --max 0.9385548523206751 --time: 31.863484382629395\n",
      "train acc:  0.9959344319017344  train loss:  0.013202880619502838 --time: 27.461151361465454\n",
      "19 validation:  0.9367088607594937 --max 0.9385548523206751 --time: 31.53351140022278\n",
      "train acc:  0.9963236884217811  train loss:  0.01173358974021278 --time: 27.24449586868286\n",
      "20 validation:  0.9338080168776371 --max 0.9385548523206751 --time: 31.368393182754517\n",
      "train acc:  0.9923446217724147  train loss:  0.022809496186725096 --time: 27.548907041549683\n",
      "21 validation:  0.9390822784810127 --max 0.9390822784810127 --time: 31.458959579467773\n",
      "train acc:  0.995026166688292  train loss:  0.015443877476198053 --time: 27.68182945251465\n",
      "22 validation:  0.9396097046413502 --max 0.9396097046413502 --time: 31.546035528182983\n",
      "train acc:  0.9969291985640759  train loss:  0.009792880076115555 --time: 27.681161642074585\n",
      "23 validation:  0.9335443037974683 --max 0.9396097046413502 --time: 31.424449682235718\n",
      "train acc:  0.994204402923749  train loss:  0.017041613772326245 --time: 27.682560682296753\n",
      "24 validation:  0.9348628691983122 --max 0.9396097046413502 --time: 31.456822633743286\n",
      "train acc:  0.9934691406081052  train loss:  0.019879152652941316 --time: 27.747163772583008\n",
      "25 validation:  0.9311708860759493 --max 0.9396097046413502 --time: 31.41277503967285\n",
      "train acc:  0.9973184550841226  train loss:  0.008551974083491195 --time: 28.033511638641357\n",
      "26 validation:  0.931698312236287 --max 0.9396097046413502 --time: 31.731975078582764\n",
      "train acc:  0.997404956533022  train loss:  0.0082067583906445 --time: 27.833276510238647\n",
      "27 validation:  0.9409282700421941 --max 0.9409282700421941 --time: 31.496957302093506\n",
      "train acc:  0.9954586739327884  train loss:  0.01258679164725662 --time: 27.771836519241333\n",
      "28 validation:  0.9364451476793249 --max 0.9409282700421941 --time: 31.632426738739014\n",
      "train acc:  0.9919553652523679  train loss:  0.023910740036028565 --time: 27.676841259002686\n",
      "29 validation:  0.9287974683544303 --max 0.9409282700421941 --time: 31.52505373954773\n",
      "train acc:  0.9910471000389256  train loss:  0.027889231624447057 --time: 27.751831769943237\n",
      "30 validation:  0.9338080168776371 --max 0.9409282700421941 --time: 31.66227126121521\n",
      "train acc:  0.9943774058215475  train loss:  0.015061536799017752 --time: 27.763405799865723\n",
      "31 validation:  0.9359177215189873 --max 0.9409282700421941 --time: 31.82034683227539\n",
      "train acc:  0.9969291985640759  train loss:  0.008534012171497712 --time: 27.742905139923096\n",
      "32 validation:  0.9356540084388185 --max 0.9409282700421941 --time: 31.698902130126953\n",
      "train acc:  0.998226720297565  train loss:  0.005374883955869713 --time: 27.705918550491333\n",
      "33 validation:  0.9372362869198312 --max 0.9409282700421941 --time: 31.659988403320312\n",
      "train acc:  0.9983997231953635  train loss:  0.004064900307790363 --time: 27.707133531570435\n",
      "34 validation:  0.9377637130801688 --max 0.9409282700421941 --time: 31.615859270095825\n",
      "train acc:  0.9982699710220146  train loss:  0.003606052202631164 --time: 27.493945360183716\n",
      "35 validation:  0.9356540084388185 --max 0.9409282700421941 --time: 31.434430837631226\n",
      "train acc:  0.9983997231953635  train loss:  0.0034252866588562513 --time: 27.695997953414917\n",
      "36 validation:  0.9367088607594937 --max 0.9409282700421941 --time: 31.843425035476685\n",
      "train acc:  0.9986159768176117  train loss:  0.0033484757650964854 --time: 27.709659814834595\n",
      "37 validation:  0.9375 --max 0.9409282700421941 --time: 31.818774223327637\n",
      "train acc:  0.9985294753687124  train loss:  0.0030690556863322534 --time: 27.722350597381592\n",
      "38 validation:  0.9382911392405063 --max 0.9409282700421941 --time: 31.759076595306396\n",
      "train acc:  0.9984429739198132  train loss:  0.0031907148168202636 --time: 27.49764370918274\n",
      "39 validation:  0.9382911392405063 --max 0.9409282700421941 --time: 31.51081395149231\n",
      "train acc:  0.9985294753687124  train loss:  0.0030727945083240302 --time: 27.627535820007324\n",
      "40 validation:  0.9380274261603375 --max 0.9409282700421941 --time: 31.65455675125122\n",
      "train acc:  0.9986159768176117  train loss:  0.00297526044784381 --time: 27.56316041946411\n",
      "41 validation:  0.9382911392405063 --max 0.9409282700421941 --time: 31.55071520805359\n",
      "train acc:  0.9984429739198132  train loss:  0.0031020727637951495 --time: 27.841941595077515\n",
      "42 validation:  0.939873417721519 --max 0.9409282700421941 --time: 31.667799472808838\n",
      "train acc:  0.9984429739198132  train loss:  0.003285189832481768 --time: 27.677765130996704\n",
      "43 validation:  0.9367088607594937 --max 0.9409282700421941 --time: 31.620742797851562\n",
      "train acc:  0.9983132217464643  train loss:  0.002980408716709427 --time: 27.636735677719116\n",
      "44 validation:  0.9393459915611815 --max 0.9409282700421941 --time: 31.44145655632019\n",
      "train acc:  0.9986592275420614  train loss:  0.002978437628195597 --time: 27.613247394561768\n",
      "45 validation:  0.9385548523206751 --max 0.9409282700421941 --time: 31.365138292312622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.998572726093162  train loss:  0.0028406720176256824 --time: 27.853556394577026\n",
      "46 validation:  0.9385548523206751 --max 0.9409282700421941 --time: 31.689669132232666\n",
      "train acc:  0.9984429739198132  train loss:  0.003086320090558488 --time: 27.856521606445312\n",
      "47 validation:  0.9375 --max 0.9409282700421941 --time: 31.5884428024292\n",
      "train acc:  0.9986592275420614  train loss:  0.0027616945907493824 --time: 27.91258692741394\n",
      "48 validation:  0.9390822784810127 --max 0.9409282700421941 --time: 31.623303651809692\n",
      "train acc:  0.9983132217464643  train loss:  0.002949312599497646 --time: 28.093059539794922\n",
      "49 validation:  0.9396097046413502 --max 0.9409282700421941 --time: 31.54794955253601\n",
      "train acc:  0.9986159768176117  train loss:  0.002930222492520659 --time: 28.29374122619629\n",
      "50 validation:  0.9385548523206751 --max 0.9409282700421941 --time: 31.87895917892456\n",
      "train acc:  0.9985294753687124  train loss:  0.0028460596160826018 --time: 28.122591972351074\n",
      "51 validation:  0.9385548523206751 --max 0.9409282700421941 --time: 31.64470100402832\n",
      "train acc:  0.9986159768176117  train loss:  0.0028845948960747217 --time: 28.18379020690918\n",
      "52 validation:  0.9375 --max 0.9409282700421941 --time: 31.747479677200317\n",
      "train acc:  0.9985294753687124  train loss:  0.0027891255421766585 --time: 28.093913316726685\n",
      "53 validation:  0.9377637130801688 --max 0.9409282700421941 --time: 31.60751509666443\n",
      "train acc:  0.9615501059642749  train loss:  0.1077525661223152 --time: 28.01187801361084\n",
      "54 validation:  0.9319620253164557 --max 0.9409282700421941 --time: 31.646877765655518\n",
      "train acc:  0.9844297391981316  train loss:  0.043961869943784086 --time: 28.15417456626892\n",
      "55 validation:  0.9404008438818565 --max 0.9409282700421941 --time: 31.71393084526062\n",
      "train acc:  0.9954586739327884  train loss:  0.013454175271538926 --time: 27.79723024368286\n",
      "56 validation:  0.9382911392405063 --max 0.9409282700421941 --time: 31.68309235572815\n",
      "train acc:  0.9980537173997664  train loss:  0.005415577644376774 --time: 27.80464458465576\n",
      "57 validation:  0.9375 --max 0.9409282700421941 --time: 31.8047354221344\n",
      "train acc:  0.9988754811643095  train loss:  0.003463175728803279 --time: 27.635467529296875\n",
      "58 validation:  0.9425105485232067 --max 0.9425105485232067 --time: 31.80912470817566\n",
      "train acc:  0.9986159768176117  train loss:  0.003041575308882719 --time: 27.426937103271484\n",
      "59 validation:  0.9417194092827004 --max 0.9425105485232067 --time: 31.693247079849243\n",
      "train acc:  0.9987024782665109  train loss:  0.0028836241872943888 --time: 27.293336391448975\n",
      "60 validation:  0.9438291139240507 --max 0.9438291139240507 --time: 31.575453519821167\n",
      "train acc:  0.9986592275420614  train loss:  0.002805131108997562 --time: 27.327924966812134\n",
      "61 validation:  0.9435654008438819 --max 0.9438291139240507 --time: 31.6196072101593\n",
      "train acc:  0.9986159768176117  train loss:  0.0028082041452315285 --time: 27.371570110321045\n",
      "62 validation:  0.9427742616033755 --max 0.9438291139240507 --time: 31.702284574508667\n",
      "train acc:  0.9986592275420614  train loss:  0.002694868868757464 --time: 27.233581066131592\n",
      "63 validation:  0.9451476793248945 --max 0.9451476793248945 --time: 31.758641719818115\n",
      "train acc:  0.9987024782665109  train loss:  0.002640167812342294 --time: 27.367647409439087\n",
      "64 validation:  0.9435654008438819 --max 0.9451476793248945 --time: 31.863533973693848\n",
      "train acc:  0.9987457289909606  train loss:  0.002698144946688801 --time: 27.089600563049316\n",
      "65 validation:  0.9443565400843882 --max 0.9451476793248945 --time: 31.442497730255127\n",
      "train acc:  0.9987889797154103  train loss:  0.002692843263503164 --time: 27.119520902633667\n",
      "66 validation:  0.9427742616033755 --max 0.9451476793248945 --time: 31.619019508361816\n",
      "train acc:  0.9987457289909606  train loss:  0.0025849522151886995 --time: 27.0669105052948\n",
      "67 validation:  0.9443565400843882 --max 0.9451476793248945 --time: 31.42370915412903\n",
      "train acc:  0.9985294753687124  train loss:  0.0028224867681635978 --time: 27.072099208831787\n",
      "68 validation:  0.944620253164557 --max 0.9451476793248945 --time: 31.413671255111694\n",
      "train acc:  0.9984862246442628  train loss:  0.0026810369622053158 --time: 27.232349395751953\n",
      "69 validation:  0.943301687763713 --max 0.9451476793248945 --time: 31.67272639274597\n",
      "train acc:  0.9986159768176117  train loss:  0.0027210712306838525 --time: 27.227792978286743\n",
      "70 validation:  0.942246835443038 --max 0.9451476793248945 --time: 31.624526500701904\n",
      "train acc:  0.9984429739198132  train loss:  0.0027967595851353293 --time: 27.125595331192017\n",
      "71 validation:  0.9427742616033755 --max 0.9451476793248945 --time: 31.590709924697876\n",
      "train acc:  0.9987024782665109  train loss:  0.002603473367605414 --time: 26.969717741012573\n",
      "72 validation:  0.9440928270042194 --max 0.9451476793248945 --time: 31.359933137893677\n",
      "train acc:  0.998572726093162  train loss:  0.0026286162195379268 --time: 27.15259838104248\n",
      "73 validation:  0.943301687763713 --max 0.9451476793248945 --time: 31.571337699890137\n",
      "train acc:  0.9985294753687124  train loss:  0.0025179999345468613 --time: 27.15008783340454\n",
      "74 validation:  0.943301687763713 --max 0.9451476793248945 --time: 31.58506989479065\n",
      "train acc:  0.9987457289909606  train loss:  0.00262381625844627 --time: 27.106757164001465\n",
      "75 validation:  0.942246835443038 --max 0.9451476793248945 --time: 31.45006775856018\n",
      "train acc:  0.9985294753687124  train loss:  0.002710433694219771 --time: 27.11613917350769\n",
      "76 validation:  0.9425105485232067 --max 0.9451476793248945 --time: 31.536776781082153\n",
      "train acc:  0.9986159768176117  train loss:  0.0025921928538072234 --time: 27.248781442642212\n",
      "77 validation:  0.943301687763713 --max 0.9451476793248945 --time: 31.63875961303711\n",
      "train acc:  0.998572726093162  train loss:  0.002545474162907742 --time: 27.12771964073181\n",
      "78 validation:  0.9419831223628692 --max 0.9451476793248945 --time: 31.61766505241394\n",
      "train acc:  0.9693352363652091  train loss:  0.08952156628978859 --time: 27.25097417831421\n",
      "79 validation:  0.9322257383966245 --max 0.9451476793248945 --time: 31.766321659088135\n",
      "train acc:  0.9860300160027681  train loss:  0.04030280972146609 --time: 27.23317790031433\n",
      "80 validation:  0.9372362869198312 --max 0.9451476793248945 --time: 31.638168811798096\n",
      "train acc:  0.9961939362484321  train loss:  0.011551699805966173 --time: 27.306180477142334\n",
      "81 validation:  0.9377637130801688 --max 0.9451476793248945 --time: 31.64664578437805\n",
      "train acc:  0.9982699710220146  train loss:  0.004734686661935514 --time: 27.178911209106445\n",
      "82 validation:  0.942246835443038 --max 0.9451476793248945 --time: 31.220662355422974\n",
      "train acc:  0.9983997231953635  train loss:  0.0032408605290297674 --time: 27.437349557876587\n",
      "83 validation:  0.943301687763713 --max 0.9451476793248945 --time: 31.42790937423706\n",
      "train acc:  0.9987024782665109  train loss:  0.0028120931426065753 --time: 27.54908013343811\n",
      "84 validation:  0.9425105485232067 --max 0.9451476793248945 --time: 31.51421070098877\n",
      "train acc:  0.9986592275420614  train loss:  0.0027484033618637525 --time: 27.484922885894775\n",
      "85 validation:  0.9438291139240507 --max 0.9451476793248945 --time: 31.474507570266724\n",
      "train acc:  0.9986159768176117  train loss:  0.00264563344921325 --time: 27.69314980506897\n",
      "86 validation:  0.9435654008438819 --max 0.9451476793248945 --time: 31.722078561782837\n",
      "train acc:  0.9986159768176117  train loss:  0.002753264573941365 --time: 27.82993197441101\n",
      "87 validation:  0.9440928270042194 --max 0.9451476793248945 --time: 31.812685251235962\n",
      "train acc:  0.9986592275420614  train loss:  0.002578233845129311 --time: 27.63411831855774\n",
      "88 validation:  0.9440928270042194 --max 0.9451476793248945 --time: 31.64514470100403\n",
      "train acc:  0.9986159768176117  train loss:  0.0026151925161865524 --time: 27.647257089614868\n",
      "89 validation:  0.9448839662447257 --max 0.9451476793248945 --time: 31.584604263305664\n",
      "train acc:  0.9987024782665109  train loss:  0.0025887916787789276 --time: 27.87495470046997\n",
      "90 validation:  0.9443565400843882 --max 0.9451476793248945 --time: 31.826770544052124\n",
      "train acc:  0.9984862246442628  train loss:  0.0025960554527216886 --time: 27.931334018707275\n",
      "91 validation:  0.9443565400843882 --max 0.9451476793248945 --time: 31.79304552078247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9987024782665109  train loss:  0.002590599928627843 --time: 27.826766967773438\n",
      "92 validation:  0.9443565400843882 --max 0.9451476793248945 --time: 31.625261783599854\n",
      "train acc:  0.9984862246442628  train loss:  0.00262878247550105 --time: 28.033705472946167\n",
      "93 validation:  0.9448839662447257 --max 0.9451476793248945 --time: 31.718806743621826\n",
      "train acc:  0.9986592275420614  train loss:  0.002564741048591005 --time: 27.964536905288696\n",
      "94 validation:  0.945675105485232 --max 0.945675105485232 --time: 31.656715154647827\n",
      "train acc:  0.9988754811643095  train loss:  0.002492348921966182 --time: 27.879729986190796\n",
      "95 validation:  0.944620253164557 --max 0.945675105485232 --time: 31.85266613960266\n",
      "train acc:  0.9986159768176117  train loss:  0.0026224660800810286 --time: 27.88354182243347\n",
      "96 validation:  0.9435654008438819 --max 0.945675105485232 --time: 31.880082607269287\n",
      "train acc:  0.9986592275420614  train loss:  0.002500162743998877 --time: 27.525024890899658\n",
      "97 validation:  0.945675105485232 --max 0.945675105485232 --time: 31.61613154411316\n",
      "train acc:  0.998572726093162  train loss:  0.0025647237971548062 --time: 27.618641138076782\n",
      "98 validation:  0.9448839662447257 --max 0.945675105485232 --time: 31.79157042503357\n",
      "train acc:  0.9985294753687124  train loss:  0.0025702551162909977 --time: 21.609625577926636\n",
      "99 validation:  0.9438291139240507 --max 0.945675105485232 --time: 23.575137615203857\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 layer, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
