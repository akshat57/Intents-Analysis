{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati_marathi_bengali_hindi_10'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/3_lang_variations/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=69, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(69, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati_marathi_bengali_hindi_10 hindi\n",
      "train acc:  0.25959904858987426  train loss:  0.45741138380506763 --time: 4.225004196166992\n",
      "validation:  0.17666666666666667 --max 0.17666666666666667 --time: 4.759572505950928\n",
      "train acc:  0.30852871219843697  train loss:  0.43093280040699505 --time: 3.663520097732544\n",
      "validation:  0.16666666666666666 --max 0.17666666666666667 --time: 3.9448039531707764\n",
      "train acc:  0.33741080530071355  train loss:  0.42067789772282477 --time: 4.278337001800537\n",
      "validation:  0.21333333333333335 --max 0.21333333333333335 --time: 4.7403724193573\n",
      "train acc:  0.3781855249745158  train loss:  0.4075748518757198 --time: 3.644920825958252\n",
      "validation:  0.22 --max 0.22 --time: 4.057848215103149\n",
      "train acc:  0.42269792728508326  train loss:  0.38828541144080786 --time: 4.741681814193726\n",
      "validation:  0.2966666666666667 --max 0.2966666666666667 --time: 5.167564630508423\n",
      "train acc:  0.49983010533469246  train loss:  0.35527373008106067 --time: 4.231004238128662\n",
      "validation:  0.35333333333333333 --max 0.35333333333333333 --time: 4.550978899002075\n",
      "train acc:  0.564729867482161  train loss:  0.32352699922478717 --time: 4.807498216629028\n",
      "validation:  0.39 --max 0.39 --time: 5.180756092071533\n",
      "train acc:  0.6224940536867143  train loss:  0.292353357957757 --time: 4.5888566970825195\n",
      "validation:  0.45666666666666667 --max 0.45666666666666667 --time: 5.125998497009277\n",
      "train acc:  0.6877336051647979  train loss:  0.25660562839197076 --time: 3.92116641998291\n",
      "validation:  0.48333333333333334 --max 0.48333333333333334 --time: 4.537330627441406\n",
      "train acc:  0.7397213727488957  train loss:  0.2215702922447868 --time: 3.1212096214294434\n",
      "validation:  0.5733333333333334 --max 0.5733333333333334 --time: 3.7084014415740967\n",
      "train acc:  0.782195039075773  train loss:  0.1949592958325925 --time: 3.3345813751220703\n",
      "validation:  0.5833333333333334 --max 0.5833333333333334 --time: 3.9086573123931885\n",
      "train acc:  0.8025823989126741  train loss:  0.1781703162452449 --time: 3.9488446712493896\n",
      "validation:  0.6133333333333333 --max 0.6133333333333333 --time: 4.486603021621704\n",
      "train acc:  0.8515120625212368  train loss:  0.14599778956693152 --time: 4.309736967086792\n",
      "validation:  0.65 --max 0.65 --time: 4.925114154815674\n",
      "train acc:  0.8702004757050629  train loss:  0.13363004767376443 --time: 4.458806037902832\n",
      "validation:  0.6733333333333333 --max 0.6733333333333333 --time: 4.93747878074646\n",
      "train acc:  0.8875297315664288  train loss:  0.11949004394852597 --time: 4.483166694641113\n",
      "validation:  0.68 --max 0.68 --time: 4.94189715385437\n",
      "train acc:  0.9147128780156303  train loss:  0.09399492325990097 --time: 4.624025106430054\n",
      "validation:  0.7033333333333334 --max 0.7033333333333334 --time: 5.218609094619751\n",
      "train acc:  0.9357798165137615  train loss:  0.0773333226856978 --time: 4.31486964225769\n",
      "validation:  0.68 --max 0.7033333333333334 --time: 4.77372145652771\n",
      "train acc:  0.9476724430852871  train loss:  0.06556014538459155 --time: 4.359503269195557\n",
      "validation:  0.7033333333333334 --max 0.7033333333333334 --time: 4.944298267364502\n",
      "train acc:  0.9398572884811417  train loss:  0.06736883990790533 --time: 4.18160080909729\n",
      "validation:  0.6966666666666667 --max 0.7033333333333334 --time: 4.830216407775879\n",
      "train acc:  0.9490316004077471  train loss:  0.062179187717645065 --time: 4.295483112335205\n",
      "validation:  0.6966666666666667 --max 0.7033333333333334 --time: 4.89517879486084\n",
      "train acc:  0.964661909616038  train loss:  0.044689618698928665 --time: 3.9857497215270996\n",
      "validation:  0.71 --max 0.71 --time: 4.6033570766448975\n",
      "train acc:  0.9728168535507985  train loss:  0.0395169861452735 --time: 4.197941303253174\n",
      "validation:  0.7066666666666667 --max 0.71 --time: 4.866170167922974\n",
      "train acc:  0.9656812776078831  train loss:  0.04039602937257808 --time: 4.304309606552124\n",
      "validation:  0.6933333333333334 --max 0.71 --time: 4.918761491775513\n",
      "train acc:  0.963302752293578  train loss:  0.04099880984943846 --time: 4.285700082778931\n",
      "validation:  0.6633333333333333 --max 0.71 --time: 4.915144205093384\n",
      "train acc:  0.9731566428814136  train loss:  0.03865408767824587 --time: 4.06964898109436\n",
      "validation:  0.73 --max 0.73 --time: 4.676983833312988\n",
      "train acc:  0.9891267414203194  train loss:  0.02192876078998265 --time: 4.126518249511719\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 4.726016044616699\n",
      "train acc:  0.9955827387020048  train loss:  0.014038387362075888 --time: 4.3302388191223145\n",
      "validation:  0.7166666666666667 --max 0.73 --time: 4.889039516448975\n",
      "train acc:  0.9925246347264696  train loss:  0.0143808891956249 --time: 4.226608991622925\n",
      "validation:  0.73 --max 0.73 --time: 4.77079176902771\n",
      "train acc:  0.9925246347264696  train loss:  0.015473487746456394 --time: 4.131750583648682\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 4.818509340286255\n",
      "train acc:  0.9867482161060143  train loss:  0.02087431020386841 --time: 4.144855260848999\n",
      "validation:  0.7033333333333334 --max 0.73 --time: 4.7902514934539795\n",
      "train acc:  0.981651376146789  train loss:  0.022197346688936585 --time: 3.980534315109253\n",
      "validation:  0.7 --max 0.73 --time: 4.539043188095093\n",
      "train acc:  0.9741760108732586  train loss:  0.02748891452084417 --time: 3.993612766265869\n",
      "validation:  0.6633333333333333 --max 0.73 --time: 4.471492767333984\n",
      "train acc:  0.9894665307509344  train loss:  0.017445057529308226 --time: 4.1845808029174805\n",
      "validation:  0.69 --max 0.73 --time: 4.760204076766968\n",
      "train acc:  0.9945633707101597  train loss:  0.012651986806936886 --time: 4.5546300411224365\n",
      "validation:  0.72 --max 0.73 --time: 5.106446027755737\n",
      "train acc:  0.9925246347264696  train loss:  0.011886813212186098 --time: 4.498965263366699\n",
      "validation:  0.75 --max 0.75 --time: 5.006729364395142\n",
      "train acc:  0.9935440027183147  train loss:  0.013119986483260342 --time: 4.478404521942139\n",
      "validation:  0.7233333333333334 --max 0.75 --time: 4.955735445022583\n",
      "train acc:  0.99932042133877  train loss:  0.0053556122634883805 --time: 4.569839954376221\n",
      "validation:  0.75 --max 0.75 --time: 4.97661566734314\n",
      "train acc:  0.9996602106693849  train loss:  0.003855463319822498 --time: 4.419287204742432\n",
      "validation:  0.7466666666666667 --max 0.75 --time: 4.925931453704834\n",
      "train acc:  0.9972816853550799  train loss:  0.006157115041075841 --time: 4.761544942855835\n",
      "validation:  0.7333333333333333 --max 0.75 --time: 5.131716012954712\n",
      "train acc:  0.9966021066938499  train loss:  0.007793859669776714 --time: 4.59863018989563\n",
      "validation:  0.74 --max 0.75 --time: 5.052284240722656\n",
      "train acc:  0.9969418960244648  train loss:  0.006741769642204694 --time: 4.428167819976807\n",
      "validation:  0.7066666666666667 --max 0.75 --time: 4.921797275543213\n",
      "train acc:  0.9921848453958546  train loss:  0.011842062565214608 --time: 4.28974986076355\n",
      "validation:  0.7366666666666667 --max 0.75 --time: 4.960102319717407\n",
      "train acc:  0.9728168535507985  train loss:  0.028433459689435753 --time: 4.245788097381592\n",
      "validation:  0.6666666666666666 --max 0.75 --time: 4.855297565460205\n",
      "train acc:  0.9779136935100238  train loss:  0.024282754014205675 --time: 3.613281011581421\n",
      "validation:  0.7066666666666667 --max 0.75 --time: 4.1644933223724365\n",
      "train acc:  0.9949031600407747  train loss:  0.010013567961995368 --time: 3.113579273223877\n",
      "validation:  0.72 --max 0.75 --time: 3.7601685523986816\n",
      "train acc:  0.9942235813795447  train loss:  0.007909518423611704 --time: 3.5265095233917236\n",
      "validation:  0.72 --max 0.75 --time: 4.19273567199707\n",
      "train acc:  0.9918450560652395  train loss:  0.009994279724829223 --time: 4.239670515060425\n",
      "validation:  0.69 --max 0.75 --time: 4.712656021118164\n",
      "train acc:  0.9983010533469249  train loss:  0.005412284021630236 --time: 4.347151041030884\n",
      "validation:  0.7166666666666667 --max 0.75 --time: 4.892014026641846\n",
      "train acc:  0.9983010533469249  train loss:  0.003909625993717624 --time: 4.404494047164917\n",
      "validation:  0.7033333333333334 --max 0.75 --time: 4.80588173866272\n",
      "train acc:  1.0  train loss:  0.002513690289798314 --time: 4.451025724411011\n",
      "validation:  0.72 --max 0.75 --time: 4.944566011428833\n",
      "train acc:  1.0  train loss:  0.002039780501154778 --time: 4.445344924926758\n",
      "validation:  0.7266666666666667 --max 0.75 --time: 5.033695936203003\n",
      "train acc:  0.99932042133877  train loss:  0.003082286389342145 --time: 4.453656435012817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:  0.7233333333333334 --max 0.75 --time: 4.9941794872283936\n",
      "train acc:  0.9976214746856948  train loss:  0.004986170598346254 --time: 4.471682548522949\n",
      "validation:  0.7233333333333334 --max 0.75 --time: 4.996967792510986\n",
      "train acc:  0.9969418960244648  train loss:  0.004817546370601201 --time: 3.976024866104126\n",
      "validation:  0.72 --max 0.75 --time: 4.540877342224121\n",
      "train acc:  0.9996602106693849  train loss:  0.0030707435166139317 --time: 4.14293646812439\n",
      "validation:  0.74 --max 0.75 --time: 4.80773138999939\n",
      "train acc:  1.0  train loss:  0.0018699833434884963 --time: 4.010794401168823\n",
      "validation:  0.7266666666666667 --max 0.75 --time: 4.597053289413452\n",
      "train acc:  0.9966021066938499  train loss:  0.004947787859355626 --time: 4.060754060745239\n",
      "validation:  0.7133333333333334 --max 0.75 --time: 4.6537299156188965\n",
      "train acc:  0.9894665307509344  train loss:  0.011338397496096466 --time: 4.223047971725464\n",
      "validation:  0.7133333333333334 --max 0.75 --time: 4.881927967071533\n",
      "train acc:  0.9599048589874278  train loss:  0.03936210878031409 --time: 4.177780389785767\n",
      "validation:  0.6833333333333333 --max 0.75 --time: 4.720037937164307\n",
      "train acc:  0.982330954808019  train loss:  0.02078964346614869 --time: 4.003051996231079\n",
      "validation:  0.7133333333333334 --max 0.75 --time: 4.6717000007629395\n",
      "train acc:  0.9949031600407747  train loss:  0.008653504247574703 --time: 4.027236461639404\n",
      "validation:  0.7133333333333334 --max 0.75 --time: 4.6464033126831055\n",
      "train acc:  0.9972816853550799  train loss:  0.005239655486429515 --time: 4.206249237060547\n",
      "validation:  0.74 --max 0.75 --time: 4.802163600921631\n",
      "train acc:  0.9996602106693849  train loss:  0.002234304678099959 --time: 4.2009596824646\n",
      "validation:  0.7233333333333334 --max 0.75 --time: 4.687488079071045\n",
      "train acc:  1.0  train loss:  0.0014504098697848942 --time: 4.372388601303101\n",
      "validation:  0.72 --max 0.75 --time: 4.822498798370361\n",
      "train acc:  1.0  train loss:  0.0012006172783794286 --time: 4.403234958648682\n",
      "validation:  0.73 --max 0.75 --time: 4.859534740447998\n",
      "train acc:  1.0  train loss:  0.0010932842826308763 --time: 4.5026068687438965\n",
      "validation:  0.7333333333333333 --max 0.75 --time: 4.93042254447937\n",
      "train acc:  1.0  train loss:  0.0010252566780368595 --time: 4.325056552886963\n",
      "validation:  0.7333333333333333 --max 0.75 --time: 4.920631408691406\n",
      "train acc:  1.0  train loss:  0.0009582711095192834 --time: 4.111073017120361\n",
      "validation:  0.7333333333333333 --max 0.75 --time: 4.709066867828369\n",
      "train acc:  1.0  train loss:  0.0009706039032291459 --time: 4.026581287384033\n",
      "validation:  0.7266666666666667 --max 0.75 --time: 4.640838623046875\n",
      "train acc:  1.0  train loss:  0.0008732606859310814 --time: 4.04397988319397\n",
      "validation:  0.74 --max 0.75 --time: 4.6469480991363525\n",
      "train acc:  1.0  train loss:  0.0008284988833348389 --time: 4.07348108291626\n",
      "validation:  0.7433333333333333 --max 0.75 --time: 4.711085796356201\n",
      "train acc:  1.0  train loss:  0.0007941728007331815 --time: 3.838831663131714\n",
      "validation:  0.7433333333333333 --max 0.75 --time: 4.488781690597534\n",
      "train acc:  1.0  train loss:  0.0007607348202525274 --time: 3.943897247314453\n",
      "validation:  0.7433333333333333 --max 0.75 --time: 4.569952964782715\n",
      "train acc:  1.0  train loss:  0.0007306006104361428 --time: 3.9634125232696533\n",
      "validation:  0.74 --max 0.75 --time: 4.605761766433716\n",
      "train acc:  1.0  train loss:  0.0007014672433876473 --time: 4.207230091094971\n",
      "validation:  0.74 --max 0.75 --time: 4.908618211746216\n",
      "train acc:  1.0  train loss:  0.0006741167744621634 --time: 4.148907423019409\n",
      "validation:  0.74 --max 0.75 --time: 4.8039162158966064\n",
      "train acc:  1.0  train loss:  0.0006527003715746105 --time: 4.38154673576355\n",
      "validation:  0.74 --max 0.75 --time: 4.903012990951538\n",
      "train acc:  1.0  train loss:  0.0006270118528450637 --time: 4.558814287185669\n",
      "validation:  0.7466666666666667 --max 0.75 --time: 5.147192478179932\n",
      "train acc:  1.0  train loss:  0.0006074903193739769 --time: 4.276697397232056\n",
      "validation:  0.7466666666666667 --max 0.75 --time: 4.6821370124816895\n",
      "train acc:  1.0  train loss:  0.0005833470875032893 --time: 4.666472434997559\n",
      "validation:  0.75 --max 0.75 --time: 5.0103514194488525\n",
      "train acc:  1.0  train loss:  0.0005655262477533972 --time: 4.63024115562439\n",
      "validation:  0.7466666666666667 --max 0.75 --time: 5.140803098678589\n",
      "train acc:  1.0  train loss:  0.0005492228083312511 --time: 4.752640008926392\n",
      "validation:  0.75 --max 0.75 --time: 5.2969810962677\n",
      "train acc:  1.0  train loss:  0.0005282256942060169 --time: 4.53611946105957\n",
      "validation:  0.7433333333333333 --max 0.75 --time: 4.9295501708984375\n",
      "train acc:  1.0  train loss:  0.0005119330902664882 --time: 4.4447314739227295\n",
      "validation:  0.75 --max 0.75 --time: 5.0328826904296875\n",
      "train acc:  1.0  train loss:  0.0004921656977587744 --time: 4.124643564224243\n",
      "validation:  0.75 --max 0.75 --time: 4.753864049911499\n",
      "train acc:  1.0  train loss:  0.00048020523567886454 --time: 3.680349349975586\n",
      "validation:  0.75 --max 0.75 --time: 4.281104326248169\n",
      "train acc:  1.0  train loss:  0.00045776588123534685 --time: 3.924027681350708\n",
      "validation:  0.75 --max 0.75 --time: 4.537785291671753\n",
      "train acc:  1.0  train loss:  0.0004446756071917227 --time: 3.4475653171539307\n",
      "validation:  0.75 --max 0.75 --time: 4.053149461746216\n",
      "train acc:  1.0  train loss:  0.0004327193613710773 --time: 3.9078288078308105\n",
      "validation:  0.75 --max 0.75 --time: 4.532082796096802\n",
      "train acc:  1.0  train loss:  0.0004243380326838435 --time: 4.52631139755249\n",
      "validation:  0.75 --max 0.75 --time: 5.208479881286621\n",
      "train acc:  1.0  train loss:  0.0004080812922557411 --time: 4.780966520309448\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 5.817802667617798\n",
      "train acc:  1.0  train loss:  0.00039667800529455036 --time: 7.708582162857056\n",
      "validation:  0.75 --max 0.7533333333333333 --time: 8.6975679397583\n",
      "train acc:  1.0  train loss:  0.0003889604932223649 --time: 7.085529804229736\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 8.0207998752594\n",
      "train acc:  1.0  train loss:  0.0003744728557016377 --time: 8.170327186584473\n",
      "validation:  0.75 --max 0.7533333333333333 --time: 9.229070663452148\n",
      "train acc:  1.0  train loss:  0.00036553318647708255 --time: 6.97539758682251\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 7.958794355392456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5aab3af95490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
