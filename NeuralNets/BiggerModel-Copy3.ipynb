{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=384, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        #cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "        \n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "        \n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 384, num_layers=2)\n",
       "  (linear): Linear(in_features=384, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5572855845335409  train loss:  1.4047449942451815 --time: 26.705965757369995\n",
      "0 validation:  0.8011603375527426 --max 0.8011603375527426 --time: 30.92095971107483\n",
      "train acc:  0.8101293196661044  train loss:  0.6023494163270813 --time: 27.402801275253296\n",
      "1 validation:  0.8705168776371308 --max 0.8705168776371308 --time: 31.775688409805298\n",
      "train acc:  0.8689503049176074  train loss:  0.4126541575015579 --time: 27.438852071762085\n",
      "2 validation:  0.8876582278481012 --max 0.8876582278481012 --time: 31.779524087905884\n",
      "train acc:  0.8999178236235457  train loss:  0.3152097823376155 --time: 27.49565100669861\n",
      "3 validation:  0.9018987341772152 --max 0.9018987341772152 --time: 31.701982259750366\n",
      "train acc:  0.9252194974265819  train loss:  0.23318761578910258 --time: 27.33124804496765\n",
      "4 validation:  0.9095464135021097 --max 0.9095464135021097 --time: 31.634666442871094\n",
      "train acc:  0.9408330089529  train loss:  0.1865027605334698 --time: 27.25049591064453\n",
      "5 validation:  0.9047995780590717 --max 0.9095464135021097 --time: 31.54813838005066\n",
      "train acc:  0.9545002378789845  train loss:  0.14648210856219682 --time: 27.398111581802368\n",
      "6 validation:  0.9129746835443038 --max 0.9129746835443038 --time: 31.754537343978882\n",
      "train acc:  0.9666104407248821  train loss:  0.11129789367235826 --time: 27.467710733413696\n",
      "7 validation:  0.9116561181434599 --max 0.9129746835443038 --time: 31.799596071243286\n",
      "train acc:  0.9718870291077376  train loss:  0.0931344749560343 --time: 27.320616245269775\n",
      "8 validation:  0.9071729957805907 --max 0.9129746835443038 --time: 31.786720752716064\n",
      "train acc:  0.9768608624194455  train loss:  0.07928938216925985 --time: 27.359159231185913\n",
      "9 validation:  0.9113924050632911 --max 0.9129746835443038 --time: 31.845338821411133\n",
      "train acc:  0.980364171099866  train loss:  0.06617218170068047 --time: 27.1137957572937\n",
      "10 validation:  0.9113924050632911 --max 0.9129746835443038 --time: 31.644314527511597\n",
      "train acc:  0.989230569612041  train loss:  0.041792385521967436 --time: 27.531108856201172\n",
      "11 validation:  0.9145569620253164 --max 0.9145569620253164 --time: 31.876320838928223\n",
      "train acc:  0.9914363565589723  train loss:  0.03571096599842962 --time: 27.412659645080566\n",
      "12 validation:  0.9113924050632911 --max 0.9145569620253164 --time: 31.66701579093933\n",
      "train acc:  0.9934258898836555  train loss:  0.028345505670947922 --time: 27.64106845855713\n",
      "13 validation:  0.9063818565400844 --max 0.9145569620253164 --time: 31.82156777381897\n",
      "train acc:  0.9886250594697461  train loss:  0.038695335208093595 --time: 27.493776559829712\n",
      "14 validation:  0.8982067510548524 --max 0.9145569620253164 --time: 31.627463579177856\n",
      "train acc:  0.985554258033822  train loss:  0.049384983436549235 --time: 27.562893629074097\n",
      "15 validation:  0.9092827004219409 --max 0.9145569620253164 --time: 31.74738883972168\n",
      "train acc:  0.9819644479045024  train loss:  0.058788021131115066 --time: 27.80918860435486\n",
      "16 validation:  0.9066455696202531 --max 0.9145569620253164 --time: 31.912914037704468\n",
      "train acc:  0.9882358029496994  train loss:  0.03976047423910041 --time: 27.831969261169434\n",
      "17 validation:  0.9108649789029536 --max 0.9145569620253164 --time: 31.870038509368896\n",
      "train acc:  0.9959344319017344  train loss:  0.01658627158416775 --time: 27.795467615127563\n",
      "18 validation:  0.9161392405063291 --max 0.9161392405063291 --time: 31.8278226852417\n",
      "train acc:  0.9977942130530686  train loss:  0.00928711396216245 --time: 28.018115997314453\n",
      "19 validation:  0.9166666666666666 --max 0.9166666666666666 --time: 31.879700422286987\n",
      "train acc:  0.9981834695731153  train loss:  0.007695820271963696 --time: 27.773093700408936\n",
      "20 validation:  0.9193037974683544 --max 0.9193037974683544 --time: 31.69772720336914\n",
      "train acc:  0.9978374637775183  train loss:  0.007192305557735944 --time: 28.042979955673218\n",
      "21 validation:  0.9158755274261603 --max 0.9193037974683544 --time: 31.937376737594604\n",
      "train acc:  0.9979672159508671  train loss:  0.007302486049898936 --time: 27.82125186920166\n",
      "22 validation:  0.9166666666666666 --max 0.9193037974683544 --time: 31.72755193710327\n",
      "train acc:  0.9980537173997664  train loss:  0.006930292510358071 --time: 27.75601840019226\n",
      "23 validation:  0.9182489451476793 --max 0.9193037974683544 --time: 31.70461130142212\n",
      "train acc:  0.9978807145019679  train loss:  0.007136569084735968 --time: 27.75637435913086\n",
      "24 validation:  0.9137658227848101 --max 0.9193037974683544 --time: 31.830968856811523\n",
      "train acc:  0.9552355001946282  train loss:  0.13409528722189037 --time: 27.734568119049072\n",
      "25 validation:  0.9042721518987342 --max 0.9193037974683544 --time: 31.79129457473755\n",
      "train acc:  0.9739198131568704  train loss:  0.07845949051253374 --time: 27.646960496902466\n",
      "26 validation:  0.9058544303797469 --max 0.9193037974683544 --time: 31.69974970817566\n",
      "train acc:  0.9933393884347563  train loss:  0.02648866907770627 --time: 27.54222822189331\n",
      "27 validation:  0.9121835443037974 --max 0.9193037974683544 --time: 31.727415084838867\n",
      "train acc:  0.9963236884217811  train loss:  0.01606840651883687 --time: 27.39012360572815\n",
      "28 validation:  0.917457805907173 --max 0.9193037974683544 --time: 31.659415245056152\n",
      "train acc:  0.9980969681242161  train loss:  0.006866736128509497 --time: 27.56353521347046\n",
      "29 validation:  0.9164029535864979 --max 0.9193037974683544 --time: 31.948482990264893\n",
      "train acc:  0.9983132217464643  train loss:  0.005906448596464573 --time: 27.342562675476074\n",
      "30 validation:  0.915084388185654 --max 0.9193037974683544 --time: 31.737380981445312\n",
      "train acc:  0.9979239652264176  train loss:  0.006271851583170442 --time: 27.36284041404724\n",
      "31 validation:  0.9164029535864979 --max 0.9193037974683544 --time: 31.817381620407104\n",
      "train acc:  0.998226720297565  train loss:  0.0055782222644862 --time: 27.299675703048706\n",
      "32 validation:  0.9161392405063291 --max 0.9193037974683544 --time: 31.708106517791748\n",
      "train acc:  0.9983564724709139  train loss:  0.0048827411391764615 --time: 27.204652786254883\n",
      "33 validation:  0.917457805907173 --max 0.9193037974683544 --time: 31.621673822402954\n",
      "train acc:  0.9980969681242161  train loss:  0.005401555777434774 --time: 27.336939334869385\n",
      "34 validation:  0.9158755274261603 --max 0.9193037974683544 --time: 31.71263098716736\n",
      "train acc:  0.9939881493015008  train loss:  0.020378946938979032 --time: 27.20567512512207\n",
      "35 validation:  0.9034810126582279 --max 0.9193037974683544 --time: 31.575950145721436\n",
      "train acc:  0.9630206305955624  train loss:  0.10658963091506814 --time: 27.193861722946167\n",
      "36 validation:  0.9018987341772152 --max 0.9193037974683544 --time: 31.537963151931763\n",
      "train acc:  0.9865490246961637  train loss:  0.04202715411926337 --time: 27.325024843215942\n",
      "37 validation:  0.9103375527426161 --max 0.9193037974683544 --time: 31.693614721298218\n",
      "train acc:  0.9952856710349899  train loss:  0.017887869094066238 --time: 27.241091012954712\n",
      "38 validation:  0.9171940928270043 --max 0.9193037974683544 --time: 31.627068758010864\n",
      "train acc:  0.9977077116041694  train loss:  0.008553708594122604 --time: 27.277507066726685\n",
      "39 validation:  0.9166666666666666 --max 0.9193037974683544 --time: 31.685803174972534\n",
      "train acc:  0.9981834695731153  train loss:  0.0063479793562234805 --time: 27.371680974960327\n",
      "40 validation:  0.9177215189873418 --max 0.9193037974683544 --time: 31.78934359550476\n",
      "train acc:  0.9981402188486657  train loss:  0.005292402158776885 --time: 27.23966097831726\n",
      "41 validation:  0.919831223628692 --max 0.919831223628692 --time: 31.636454343795776\n",
      "train acc:  0.9982699710220146  train loss:  0.005133888837128868 --time: 27.59244132041931\n",
      "42 validation:  0.9161392405063291 --max 0.919831223628692 --time: 31.986111879348755\n",
      "train acc:  0.9980969681242161  train loss:  0.005184440036305581 --time: 27.349757194519043\n",
      "43 validation:  0.9156118143459916 --max 0.919831223628692 --time: 31.704694032669067\n",
      "train acc:  0.9981834695731153  train loss:  0.005295827163310042 --time: 27.285994052886963\n",
      "44 validation:  0.9195675105485233 --max 0.919831223628692 --time: 31.687299966812134\n",
      "train acc:  0.9983997231953635  train loss:  0.004840754457969393 --time: 27.03834104537964\n",
      "45 validation:  0.9182489451476793 --max 0.919831223628692 --time: 31.362337589263916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9979672159508671  train loss:  0.00566703486027973 --time: 27.313268661499023\n",
      "46 validation:  0.9193037974683544 --max 0.919831223628692 --time: 31.70516347885132\n",
      "train acc:  0.9983997231953635  train loss:  0.0045897259051494756 --time: 27.395315885543823\n",
      "47 validation:  0.9190400843881856 --max 0.919831223628692 --time: 31.817664623260498\n",
      "train acc:  0.9983132217464643  train loss:  0.0042811128078832535 --time: 27.209380865097046\n",
      "48 validation:  0.9200949367088608 --max 0.9200949367088608 --time: 31.650450706481934\n",
      "train acc:  0.9983564724709139  train loss:  0.004696090828298684 --time: 27.623483657836914\n",
      "49 validation:  0.9190400843881856 --max 0.9200949367088608 --time: 32.0739004611969\n",
      "train acc:  0.9983997231953635  train loss:  0.004389501606498186 --time: 27.416046380996704\n",
      "50 validation:  0.9211497890295358 --max 0.9211497890295358 --time: 31.694512605667114\n",
      "train acc:  0.9983132217464643  train loss:  0.004439710741481572 --time: 27.353562355041504\n",
      "51 validation:  0.919831223628692 --max 0.9211497890295358 --time: 31.592456340789795\n",
      "train acc:  0.9981834695731153  train loss:  0.004571163857700457 --time: 27.643503665924072\n",
      "52 validation:  0.9179852320675106 --max 0.9211497890295358 --time: 31.930635452270508\n",
      "train acc:  0.9984862246442628  train loss:  0.004401726700389341 --time: 27.47142219543457\n",
      "53 validation:  0.9206223628691983 --max 0.9211497890295358 --time: 31.674006462097168\n",
      "train acc:  0.998226720297565  train loss:  0.004768334882261839 --time: 27.675716400146484\n",
      "54 validation:  0.9171940928270043 --max 0.9211497890295358 --time: 31.86257839202881\n",
      "train acc:  0.998226720297565  train loss:  0.004647555720961192 --time: 27.633805513381958\n",
      "55 validation:  0.9214135021097046 --max 0.9214135021097046 --time: 31.825525283813477\n",
      "train acc:  0.9982699710220146  train loss:  0.0044278371900249944 --time: 27.844280004501343\n",
      "56 validation:  0.9206223628691983 --max 0.9214135021097046 --time: 31.975841283798218\n",
      "train acc:  0.9980537173997664  train loss:  0.00457844138559493 --time: 27.659769535064697\n",
      "57 validation:  0.9195675105485233 --max 0.9214135021097046 --time: 31.755082845687866\n",
      "train acc:  0.998226720297565  train loss:  0.004061341616460148 --time: 27.484431982040405\n",
      "58 validation:  0.9185126582278481 --max 0.9214135021097046 --time: 31.4773006439209\n",
      "train acc:  0.9469313611002984  train loss:  0.16685297243360292 --time: 27.503801345825195\n",
      "59 validation:  0.9008438818565401 --max 0.9214135021097046 --time: 31.449676990509033\n",
      "train acc:  0.9716707754854894  train loss:  0.08251017240488398 --time: 28.07015347480774\n",
      "60 validation:  0.9055907172995781 --max 0.9214135021097046 --time: 32.15945267677307\n",
      "train acc:  0.9938151464037023  train loss:  0.02232182589076583 --time: 27.888134002685547\n",
      "61 validation:  0.9061181434599156 --max 0.9214135021097046 --time: 31.812088012695312\n",
      "train acc:  0.9967129449418278  train loss:  0.011203108592092661 --time: 27.915584564208984\n",
      "62 validation:  0.9121835443037974 --max 0.9214135021097046 --time: 31.903174877166748\n",
      "train acc:  0.9974914579819212  train loss:  0.008821123775521819 --time: 27.944199085235596\n",
      "63 validation:  0.9153481012658228 --max 0.9214135021097046 --time: 31.903802633285522\n",
      "train acc:  0.9981834695731153  train loss:  0.005485485369939027 --time: 28.04637122154236\n",
      "64 validation:  0.9145569620253164 --max 0.9214135021097046 --time: 32.085025787353516\n",
      "train acc:  0.998226720297565  train loss:  0.005033641059089256 --time: 27.755607843399048\n",
      "65 validation:  0.9142932489451476 --max 0.9214135021097046 --time: 31.77220845222473\n",
      "train acc:  0.9982699710220146  train loss:  0.004084472973009016 --time: 27.710925579071045\n",
      "66 validation:  0.9132383966244726 --max 0.9214135021097046 --time: 31.82059597969055\n",
      "train acc:  0.9983997231953635  train loss:  0.00424975518218257 --time: 27.740899324417114\n",
      "67 validation:  0.9148206751054853 --max 0.9214135021097046 --time: 31.948836088180542\n",
      "train acc:  0.9983132217464643  train loss:  0.004186762457916729 --time: 27.684821367263794\n",
      "68 validation:  0.917457805907173 --max 0.9214135021097046 --time: 31.84272575378418\n",
      "train acc:  0.9983132217464643  train loss:  0.004288507593301696 --time: 27.506109714508057\n",
      "69 validation:  0.9158755274261603 --max 0.9214135021097046 --time: 31.78777837753296\n",
      "train acc:  0.9983997231953635  train loss:  0.004316570962382383 --time: 27.784292697906494\n",
      "70 validation:  0.9142932489451476 --max 0.9214135021097046 --time: 32.05565357208252\n",
      "train acc:  0.9983132217464643  train loss:  0.004293866212755051 --time: 27.59034562110901\n",
      "71 validation:  0.9171940928270043 --max 0.9214135021097046 --time: 31.737217903137207\n",
      "train acc:  0.9983564724709139  train loss:  0.004064079838289072 --time: 27.495479583740234\n",
      "72 validation:  0.9164029535864979 --max 0.9214135021097046 --time: 31.878068447113037\n",
      "train acc:  0.9984429739198132  train loss:  0.0038858227072317716 --time: 27.60573410987854\n",
      "73 validation:  0.9145569620253164 --max 0.9214135021097046 --time: 32.00006628036499\n",
      "train acc:  0.9984429739198132  train loss:  0.004075916908174568 --time: 27.471790552139282\n",
      "74 validation:  0.9161392405063291 --max 0.9214135021097046 --time: 31.758412837982178\n",
      "train acc:  0.9984862246442628  train loss:  0.004126153115508885 --time: 27.51120090484619\n",
      "75 validation:  0.9156118143459916 --max 0.9214135021097046 --time: 31.80413007736206\n",
      "train acc:  0.9910903507633753  train loss:  0.0277284142599526 --time: 27.85264563560486\n",
      "76 validation:  0.8847573839662447 --max 0.9214135021097046 --time: 31.962235689163208\n",
      "train acc:  0.9592578175684443  train loss:  0.12125636781282846 --time: 27.99247407913208\n",
      "77 validation:  0.9095464135021097 --max 0.9214135021097046 --time: 32.09879970550537\n",
      "train acc:  0.9898360797543359  train loss:  0.031123060028156194 --time: 27.591211080551147\n",
      "78 validation:  0.9156118143459916 --max 0.9214135021097046 --time: 31.507410764694214\n",
      "train acc:  0.9971022014618744  train loss:  0.010913194564315207 --time: 27.936774492263794\n",
      "79 validation:  0.9171940928270043 --max 0.9214135021097046 --time: 31.83920454978943\n",
      "train acc:  0.9983564724709139  train loss:  0.006611032472236484 --time: 28.019163131713867\n",
      "80 validation:  0.9179852320675106 --max 0.9214135021097046 --time: 31.94559669494629\n",
      "train acc:  0.9983132217464643  train loss:  0.004932745189234892 --time: 28.149142742156982\n",
      "81 validation:  0.9182489451476793 --max 0.9214135021097046 --time: 31.84942078590393\n",
      "train acc:  0.9980969681242161  train loss:  0.004587080401531806 --time: 27.799120664596558\n",
      "82 validation:  0.9235232067510548 --max 0.9235232067510548 --time: 31.68502712249756\n",
      "train acc:  0.9983132217464643  train loss:  0.004073158054061373 --time: 27.84729814529419\n",
      "83 validation:  0.9227320675105485 --max 0.9235232067510548 --time: 31.715078830718994\n",
      "train acc:  0.9983564724709139  train loss:  0.00418065628205691 --time: 28.079655647277832\n",
      "84 validation:  0.919831223628692 --max 0.9235232067510548 --time: 31.978781938552856\n",
      "train acc:  0.9982699710220146  train loss:  0.0040061502016049124 --time: 27.85008406639099\n",
      "85 validation:  0.9211497890295358 --max 0.9235232067510548 --time: 31.857897996902466\n",
      "train acc:  0.9983997231953635  train loss:  0.004193975753375993 --time: 27.70938777923584\n",
      "86 validation:  0.9206223628691983 --max 0.9235232067510548 --time: 31.6945858001709\n",
      "train acc:  0.9983997231953635  train loss:  0.00412071150266411 --time: 27.861759662628174\n",
      "87 validation:  0.9227320675105485 --max 0.9235232067510548 --time: 31.949139833450317\n",
      "train acc:  0.998226720297565  train loss:  0.004101220248778382 --time: 27.73673677444458\n",
      "88 validation:  0.9211497890295358 --max 0.9235232067510548 --time: 31.93073797225952\n",
      "train acc:  0.9985294753687124  train loss:  0.004072391713303076 --time: 27.524010181427002\n",
      "89 validation:  0.9195675105485233 --max 0.9235232067510548 --time: 31.678478956222534\n",
      "train acc:  0.9983997231953635  train loss:  0.003774760715317403 --time: 27.701504707336426\n",
      "90 validation:  0.9190400843881856 --max 0.9235232067510548 --time: 31.882615327835083\n",
      "train acc:  0.9983997231953635  train loss:  0.004044712808435636 --time: 27.545051097869873\n",
      "91 validation:  0.9187763713080169 --max 0.9235232067510548 --time: 31.653773069381714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9984862246442628  train loss:  0.0040700883426988445 --time: 27.62381076812744\n",
      "92 validation:  0.9203586497890295 --max 0.9235232067510548 --time: 31.76709008216858\n",
      "train acc:  0.9982699710220146  train loss:  0.0041623716699268195 --time: 27.62008500099182\n",
      "93 validation:  0.9206223628691983 --max 0.9235232067510548 --time: 31.85851788520813\n",
      "train acc:  0.9981834695731153  train loss:  0.004307697893079185 --time: 27.723470211029053\n",
      "94 validation:  0.9219409282700421 --max 0.9235232067510548 --time: 31.831550121307373\n",
      "train acc:  0.9981834695731153  train loss:  0.004332335088167273 --time: 27.51062297821045\n",
      "95 validation:  0.9200949367088608 --max 0.9235232067510548 --time: 31.719999313354492\n",
      "train acc:  0.9981402188486657  train loss:  0.004110167049781936 --time: 27.650099515914917\n",
      "96 validation:  0.9206223628691983 --max 0.9235232067510548 --time: 31.778928995132446\n",
      "train acc:  0.9983997231953635  train loss:  0.003968508821053145 --time: 27.55846071243286\n",
      "97 validation:  0.9200949367088608 --max 0.9235232067510548 --time: 31.735412120819092\n",
      "train acc:  0.9983564724709139  train loss:  0.003765995570936675 --time: 27.841041326522827\n",
      "98 validation:  0.9185126582278481 --max 0.9235232067510548 --time: 31.96726155281067\n",
      "train acc:  0.9982699710220146  train loss:  0.003915788181271154 --time: 25.85909938812256\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 lstm layer, 384, 2CNN context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
