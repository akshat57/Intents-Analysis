{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "        embeddings = input##saved for attention\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "        \n",
    "        #doing attention\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        batch_size = hn[-1].shape[0]\n",
    "        emb_size = embeddings.shape[1]\n",
    "        seq_len = embeddings.shape[2]\n",
    "\n",
    "        attention = torch.zeros([batch_size, seq_len]).to(device)\n",
    "        \n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            attention[:,t] = torch.diagonal(torch.matmul(embeddings[:,:,t], hn[-1].T))\n",
    "            #attention[:,t] = cos(embeddings[:,:,t], hn[-1])\n",
    "            \n",
    "        #normalize = torch.sum(attention, dim=1).reshape(-1,1)\n",
    "        #attention = attention/normalize\n",
    "        attention /= (128**0.5)\n",
    "\n",
    "        \n",
    "        output = torch.zeros([batch_size, emb_size]).to(device)\n",
    "        for t in range(seq_len):\n",
    "            weights = attention[:,t].reshape(-1,1)\n",
    "            output += weights*embeddings[:,:,t]\n",
    "        \n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "#opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "opt = SGD(model.parameters(), lr=0.1)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.218144750254842  train loss:  0.7995624516321265 --time: 14.550006628036499\n",
      "validation:  0.16 --max 0.16 --time: 15.205307960510254\n",
      "train acc:  0.23581379544682296  train loss:  0.5176900508611099 --time: 8.637072086334229\n",
      "validation:  0.19666666666666666 --max 0.19666666666666666 --time: 9.053658246994019\n",
      "train acc:  0.25042473666326875  train loss:  0.4769960862138997 --time: 12.892724752426147\n",
      "validation:  0.20666666666666667 --max 0.20666666666666667 --time: 13.677891731262207\n",
      "train acc:  0.2762487257900102  train loss:  0.46165764720543573 --time: 14.754877090454102\n",
      "validation:  0.22333333333333333 --max 0.22333333333333333 --time: 15.454407215118408\n",
      "train acc:  0.290859667006456  train loss:  0.46193427106608514 --time: 14.98074984550476\n",
      "validation:  0.24333333333333335 --max 0.24333333333333335 --time: 15.816989421844482\n",
      "train acc:  0.2983350322799864  train loss:  0.44643122605655505 --time: 13.691144704818726\n",
      "validation:  0.25333333333333335 --max 0.25333333333333335 --time: 14.482341289520264\n",
      "train acc:  0.3163438668025824  train loss:  0.44160554979158484 --time: 14.353563785552979\n",
      "validation:  0.25666666666666665 --max 0.25666666666666665 --time: 15.110824584960938\n",
      "train acc:  0.34624532789670404  train loss:  0.4329612708610037 --time: 14.63528323173523\n",
      "validation:  0.2966666666666667 --max 0.2966666666666667 --time: 15.444802045822144\n",
      "train acc:  0.3489636425416242  train loss:  0.4292172711828481 --time: 15.492033243179321\n",
      "validation:  0.30333333333333334 --max 0.30333333333333334 --time: 16.31283140182495\n",
      "train acc:  0.37172952769283046  train loss:  0.42361223179361096 --time: 14.405574083328247\n",
      "validation:  0.30333333333333334 --max 0.30333333333333334 --time: 15.119779348373413\n",
      "train acc:  0.3982330954808019  train loss:  0.4151870504669521 --time: 14.653721332550049\n",
      "validation:  0.33666666666666667 --max 0.33666666666666667 --time: 15.338984966278076\n",
      "train acc:  0.4053686714237173  train loss:  0.41888223782829614 --time: 14.347112655639648\n",
      "validation:  0.31333333333333335 --max 0.33666666666666667 --time: 15.040582418441772\n",
      "train acc:  0.4451240231056745  train loss:  0.4014921019906583 --time: 15.224553346633911\n",
      "validation:  0.33666666666666667 --max 0.33666666666666667 --time: 15.613033294677734\n",
      "train acc:  0.4750254841997961  train loss:  0.3880524285461592 --time: 15.193251609802246\n",
      "validation:  0.36666666666666664 --max 0.36666666666666664 --time: 15.918739080429077\n",
      "train acc:  0.5141012572205232  train loss:  0.36931339165438776 --time: 13.819875001907349\n",
      "validation:  0.4033333333333333 --max 0.4033333333333333 --time: 14.540712356567383\n",
      "train acc:  0.5443425076452599  train loss:  0.350035864373912 --time: 13.818790912628174\n",
      "validation:  0.41333333333333333 --max 0.41333333333333333 --time: 14.211598873138428\n",
      "train acc:  0.5229357798165137  train loss:  0.353301210247952 --time: 12.31087327003479\n",
      "validation:  0.36333333333333334 --max 0.41333333333333333 --time: 12.84751296043396\n",
      "train acc:  0.5107033639143731  train loss:  0.3720381389493528 --time: 8.133360624313354\n",
      "validation:  0.42333333333333334 --max 0.42333333333333334 --time: 8.429375648498535\n",
      "train acc:  0.5671083927964662  train loss:  0.3241891303788061 --time: 6.834143877029419\n",
      "validation:  0.42333333333333334 --max 0.42333333333333334 --time: 7.1319098472595215\n",
      "train acc:  0.5756031260618416  train loss:  0.31362558706946997 --time: 6.595399618148804\n",
      "validation:  0.4533333333333333 --max 0.4533333333333333 --time: 6.895399332046509\n",
      "train acc:  0.6078831124702684  train loss:  0.29539196905882464 --time: 10.688577890396118\n",
      "validation:  0.45666666666666667 --max 0.45666666666666667 --time: 11.32210087776184\n",
      "train acc:  0.618416581719334  train loss:  0.2871534947467887 --time: 12.1735999584198\n",
      "validation:  0.47333333333333333 --max 0.47333333333333333 --time: 12.672671556472778\n",
      "train acc:  0.6394835202174651  train loss:  0.27631745778996014 --time: 13.237642765045166\n",
      "validation:  0.4766666666666667 --max 0.4766666666666667 --time: 13.80045747756958\n",
      "train acc:  0.6350662589194699  train loss:  0.28687804159910785 --time: 13.177937030792236\n",
      "validation:  0.48333333333333334 --max 0.48333333333333334 --time: 13.78333854675293\n",
      "train acc:  0.6670064559972817  train loss:  0.2591558312592299 --time: 13.97204327583313\n",
      "validation:  0.5333333333333333 --max 0.5333333333333333 --time: 14.594733476638794\n",
      "train acc:  0.6979272850832484  train loss:  0.2465975083734678 --time: 13.069700956344604\n",
      "validation:  0.5666666666666667 --max 0.5666666666666667 --time: 13.517007827758789\n",
      "train acc:  0.7200135915732246  train loss:  0.2320888463569724 --time: 12.787890911102295\n",
      "validation:  0.6 --max 0.6 --time: 13.330830812454224\n",
      "train acc:  0.7169554875976895  train loss:  0.2378738820552826 --time: 10.27036738395691\n",
      "validation:  0.6333333333333333 --max 0.6333333333333333 --time: 10.596445798873901\n",
      "train acc:  0.746517159361196  train loss:  0.2117499002943868 --time: 7.230060577392578\n",
      "validation:  0.6633333333333333 --max 0.6633333333333333 --time: 7.54570746421814\n",
      "train acc:  0.7743798844716276  train loss:  0.19888405566630157 --time: 13.27878713607788\n",
      "validation:  0.69 --max 0.69 --time: 13.691060304641724\n",
      "train acc:  0.799524294937139  train loss:  0.1816077199967011 --time: 12.790924072265625\n",
      "validation:  0.7066666666666667 --max 0.7066666666666667 --time: 13.279540538787842\n",
      "train acc:  0.8171933401291199  train loss:  0.16960636364377063 --time: 12.155498027801514\n",
      "validation:  0.7233333333333334 --max 0.7233333333333334 --time: 12.802721500396729\n",
      "train acc:  0.8022426095820592  train loss:  0.17703235926835434 --time: 11.925546884536743\n",
      "validation:  0.7333333333333333 --max 0.7333333333333333 --time: 12.563941717147827\n",
      "train acc:  0.8389398572884812  train loss:  0.14872644647308017 --time: 13.311512231826782\n",
      "validation:  0.73 --max 0.7333333333333333 --time: 13.882494688034058\n",
      "train acc:  0.8416581719334013  train loss:  0.1415532095276791 --time: 13.92676854133606\n",
      "validation:  0.7566666666666667 --max 0.7566666666666667 --time: 14.434577465057373\n",
      "train acc:  0.8474345905538566  train loss:  0.14059624561797018 --time: 12.04302191734314\n",
      "validation:  0.73 --max 0.7566666666666667 --time: 12.438763856887817\n",
      "train acc:  0.8606863744478424  train loss:  0.12940106346555377 --time: 13.63008165359497\n",
      "validation:  0.7566666666666667 --max 0.7566666666666667 --time: 14.189777851104736\n",
      "train acc:  0.8749575263336731  train loss:  0.11840446358141692 --time: 13.046095609664917\n",
      "validation:  0.75 --max 0.7566666666666667 --time: 13.54939866065979\n",
      "train acc:  0.8290859667006456  train loss:  0.15186969126048294 --time: 13.049233675003052\n",
      "validation:  0.7366666666666667 --max 0.7566666666666667 --time: 13.668570041656494\n",
      "train acc:  0.8865103635745838  train loss:  0.10633309444655543 --time: 12.934741020202637\n",
      "validation:  0.76 --max 0.76 --time: 13.428634881973267\n",
      "train acc:  0.8980632008154944  train loss:  0.09852954712898834 --time: 12.682442426681519\n",
      "validation:  0.76 --max 0.76 --time: 13.370567560195923\n",
      "train acc:  0.9075773020727149  train loss:  0.08971672569927962 --time: 13.273442506790161\n",
      "validation:  0.7733333333333333 --max 0.7733333333333333 --time: 13.835629224777222\n",
      "train acc:  0.9119945633707102  train loss:  0.08495891758281252 --time: 12.772465944290161\n",
      "validation:  0.7633333333333333 --max 0.7733333333333333 --time: 13.3324613571167\n",
      "train acc:  0.9133537206931702  train loss:  0.08684743503513544 --time: 12.786868333816528\n",
      "validation:  0.76 --max 0.7733333333333333 --time: 13.296825885772705\n",
      "train acc:  0.8953448861705743  train loss:  0.0992929549968761 --time: 13.110587120056152\n",
      "validation:  0.8066666666666666 --max 0.8066666666666666 --time: 13.45499324798584\n",
      "train acc:  0.9143730886850153  train loss:  0.08417105107851651 --time: 13.655478715896606\n",
      "validation:  0.7766666666666666 --max 0.8066666666666666 --time: 13.979460954666138\n",
      "train acc:  0.9113149847094801  train loss:  0.08524036342683046 --time: 13.884451389312744\n",
      "validation:  0.79 --max 0.8066666666666666 --time: 14.25226616859436\n",
      "train acc:  0.9245667686034659  train loss:  0.06988562983663185 --time: 13.497837543487549\n",
      "validation:  0.81 --max 0.81 --time: 14.067805051803589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9347604485219164  train loss:  0.06628985913551372 --time: 13.109495401382446\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 13.621591091156006\n",
      "train acc:  0.9354400271831464  train loss:  0.06569252632882285 --time: 13.269471883773804\n",
      "validation:  0.8166666666666667 --max 0.8166666666666667 --time: 13.875720024108887\n",
      "train acc:  0.928644240570846  train loss:  0.07233656359755475 --time: 13.270312786102295\n",
      "validation:  0.7966666666666666 --max 0.8166666666666667 --time: 13.864251136779785\n",
      "train acc:  0.9283044512402311  train loss:  0.07698445297453714 --time: 12.52097225189209\n",
      "validation:  0.8 --max 0.8166666666666667 --time: 13.172520637512207\n",
      "train acc:  0.9395174991505266  train loss:  0.06188473591338033 --time: 12.91022515296936\n",
      "validation:  0.8233333333333334 --max 0.8233333333333334 --time: 13.570988893508911\n",
      "train acc:  0.9473326537546721  train loss:  0.056985205120366554 --time: 13.566781997680664\n",
      "validation:  0.8166666666666667 --max 0.8233333333333334 --time: 14.213906049728394\n",
      "train acc:  0.9558273870200475  train loss:  0.0508606660625209 --time: 12.996593236923218\n",
      "validation:  0.8266666666666667 --max 0.8266666666666667 --time: 13.471824407577515\n",
      "train acc:  0.9531090723751274  train loss:  0.04840364857860233 --time: 11.873664617538452\n",
      "validation:  0.8266666666666667 --max 0.8266666666666667 --time: 12.414054155349731\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([ [1,2,3], [10,20,30],[100,200,300], [1000,2000,3000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3],\n",
       "        [  10,   20,   30],\n",
       "        [ 100,  200,  300],\n",
       "        [1000, 2000, 3000]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     2,     3],\n",
       "        [   20,    40,    60],\n",
       "        [  300,   600,   900],\n",
       "        [ 4000,  8000, 12000]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0000,   2.0000,   3.0000],\n",
       "        [  5.0000,  10.0000,  15.0000],\n",
       "        [ 33.3333,  66.6667, 100.0000],\n",
       "        [250.0000, 500.0000, 750.0000]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.float()/a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
