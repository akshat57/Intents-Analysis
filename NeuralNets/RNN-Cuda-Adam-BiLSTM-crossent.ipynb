{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati'\n",
    "test_language = 'gujarati'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=256, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=42, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(42, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati gujarati\n",
      "train acc:  0.3571185864763846  train loss:  1.5910624006520147 --time: 1.6299612522125244\n",
      "validation:  0.35 --max 0.35 --time: 1.83648681640625\n",
      "train acc:  0.5555555555555556  train loss:  1.1956838447114695 --time: 1.523390769958496\n",
      "validation:  0.57 --max 0.57 --time: 1.730407476425171\n",
      "train acc:  0.7319062181447502  train loss:  0.7994544078474459 --time: 1.5503251552581787\n",
      "validation:  0.7066666666666667 --max 0.7066666666666667 --time: 1.7594478130340576\n",
      "train acc:  0.8287461773700305  train loss:  0.5085781955200693 --time: 1.5201780796051025\n",
      "validation:  0.7833333333333333 --max 0.7833333333333333 --time: 1.7272000312805176\n",
      "train acc:  0.854909955827387  train loss:  0.42173308717167896 --time: 1.5511629581451416\n",
      "validation:  0.8166666666666667 --max 0.8166666666666667 --time: 1.7603442668914795\n",
      "train acc:  0.9113149847094801  train loss:  0.28259850293397903 --time: 1.5416905879974365\n",
      "validation:  0.8733333333333333 --max 0.8733333333333333 --time: 1.7523324489593506\n",
      "train acc:  0.9303431872239212  train loss:  0.21505568225098692 --time: 1.5338976383209229\n",
      "validation:  0.8666666666666667 --max 0.8733333333333333 --time: 1.7430531978607178\n",
      "train acc:  0.9425756031260618  train loss:  0.18658793280306069 --time: 1.501258134841919\n",
      "validation:  0.8266666666666667 --max 0.8733333333333333 --time: 1.707963228225708\n",
      "train acc:  0.9561671763506626  train loss:  0.14499243433870698 --time: 1.5197179317474365\n",
      "validation:  0.8733333333333333 --max 0.8733333333333333 --time: 1.7247841358184814\n",
      "train acc:  0.9785932721712538  train loss:  0.0840436770864155 --time: 1.5231244564056396\n",
      "validation:  0.8966666666666666 --max 0.8966666666666666 --time: 1.7360925674438477\n",
      "train acc:  0.9711179068977234  train loss:  0.0895684452160545 --time: 1.549677848815918\n",
      "validation:  0.8866666666666667 --max 0.8966666666666666 --time: 1.758714199066162\n",
      "train acc:  0.9867482161060143  train loss:  0.0470006150983112 --time: 1.5381872653961182\n",
      "validation:  0.9033333333333333 --max 0.9033333333333333 --time: 1.7436130046844482\n",
      "train acc:  0.9826707441386341  train loss:  0.06239658486826912 --time: 1.541926622390747\n",
      "validation:  0.88 --max 0.9033333333333333 --time: 1.748551368713379\n",
      "train acc:  0.9799524294937139  train loss:  0.06600234491507644 --time: 1.530604600906372\n",
      "validation:  0.8666666666666667 --max 0.9033333333333333 --time: 1.7390024662017822\n",
      "train acc:  0.9850492694529391  train loss:  0.053217148205832294 --time: 1.5300962924957275\n",
      "validation:  0.8733333333333333 --max 0.9033333333333333 --time: 1.7392289638519287\n",
      "train acc:  0.9962623173632348  train loss:  0.02298177261431904 --time: 1.5349462032318115\n",
      "validation:  0.8733333333333333 --max 0.9033333333333333 --time: 1.7428398132324219\n",
      "train acc:  0.9840299014610941  train loss:  0.04920738763378366 --time: 1.5556120872497559\n",
      "validation:  0.8866666666666667 --max 0.9033333333333333 --time: 1.76350736618042\n",
      "train acc:  0.9898063200815495  train loss:  0.04155083680930345 --time: 1.5437366962432861\n",
      "validation:  0.8933333333333333 --max 0.9033333333333333 --time: 1.7574570178985596\n",
      "train acc:  0.9843696907917091  train loss:  0.0529043969657758 --time: 1.5540175437927246\n",
      "validation:  0.8633333333333333 --max 0.9033333333333333 --time: 1.759537696838379\n",
      "train acc:  0.983010533469249  train loss:  0.054593204642119614 --time: 1.5306875705718994\n",
      "validation:  0.8833333333333333 --max 0.9033333333333333 --time: 1.7364845275878906\n",
      "train acc:  0.9942235813795447  train loss:  0.024743991003007344 --time: 1.5407886505126953\n",
      "validation:  0.91 --max 0.91 --time: 1.7574617862701416\n",
      "train acc:  0.99932042133877  train loss:  0.005856129720442645 --time: 1.5401098728179932\n",
      "validation:  0.8833333333333333 --max 0.91 --time: 1.7532730102539062\n",
      "train acc:  0.9966021066938499  train loss:  0.01484513480681926 --time: 1.5284199714660645\n",
      "validation:  0.88 --max 0.91 --time: 1.7365310192108154\n",
      "train acc:  0.9867482161060143  train loss:  0.039273247021533876 --time: 1.537172794342041\n",
      "validation:  0.8933333333333333 --max 0.91 --time: 1.7454121112823486\n",
      "train acc:  0.9891267414203194  train loss:  0.03086301595296549 --time: 1.5314273834228516\n",
      "validation:  0.89 --max 0.91 --time: 1.7479238510131836\n",
      "train acc:  1.0  train loss:  0.0032602134439081924 --time: 1.5320730209350586\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.7405438423156738\n",
      "train acc:  1.0  train loss:  0.0014084195996553678 --time: 1.542032241821289\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.7506685256958008\n",
      "train acc:  1.0  train loss:  0.0009761258527008897 --time: 1.5543606281280518\n",
      "validation:  0.8933333333333333 --max 0.91 --time: 1.7621021270751953\n",
      "train acc:  1.0  train loss:  0.0008053670639840319 --time: 1.546264886856079\n",
      "validation:  0.8933333333333333 --max 0.91 --time: 1.7525596618652344\n",
      "train acc:  1.0  train loss:  0.0006875978642315401 --time: 1.5239484310150146\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.731720209121704\n",
      "train acc:  1.0  train loss:  0.0006284364030959651 --time: 1.537360429763794\n",
      "validation:  0.9 --max 0.91 --time: 1.744596004486084\n",
      "train acc:  1.0  train loss:  0.0005455633508972824 --time: 1.556330919265747\n",
      "validation:  0.9 --max 0.91 --time: 1.7642672061920166\n",
      "train acc:  1.0  train loss:  0.0004956080361886922 --time: 1.533048152923584\n",
      "validation:  0.9 --max 0.91 --time: 1.7407445907592773\n",
      "train acc:  1.0  train loss:  0.00046893697151023406 --time: 1.5502338409423828\n",
      "validation:  0.9 --max 0.91 --time: 1.7580440044403076\n",
      "train acc:  1.0  train loss:  0.00042065412012364146 --time: 1.535764455795288\n",
      "validation:  0.9 --max 0.91 --time: 1.7447905540466309\n",
      "train acc:  1.0  train loss:  0.00037806210773693317 --time: 1.5351557731628418\n",
      "validation:  0.9033333333333333 --max 0.91 --time: 1.738508701324463\n",
      "train acc:  1.0  train loss:  0.0003576507999399758 --time: 1.5408384799957275\n",
      "validation:  0.9 --max 0.91 --time: 1.748164415359497\n",
      "train acc:  1.0  train loss:  0.0003278855526688225 --time: 1.5209765434265137\n",
      "validation:  0.9 --max 0.91 --time: 1.7267570495605469\n",
      "train acc:  1.0  train loss:  0.00030519941640744474 --time: 1.547872543334961\n",
      "validation:  0.9033333333333333 --max 0.91 --time: 1.7557122707366943\n",
      "train acc:  1.0  train loss:  0.00029104716035679144 --time: 1.5322215557098389\n",
      "validation:  0.9 --max 0.91 --time: 1.7377510070800781\n",
      "train acc:  1.0  train loss:  0.00027191402857525924 --time: 1.5365991592407227\n",
      "validation:  0.9033333333333333 --max 0.91 --time: 1.7504422664642334\n",
      "train acc:  1.0  train loss:  0.0002547950381362487 --time: 1.5224974155426025\n",
      "validation:  0.9 --max 0.91 --time: 1.7308075428009033\n",
      "train acc:  1.0  train loss:  0.0002415027189746742 --time: 1.543924331665039\n",
      "validation:  0.8933333333333333 --max 0.91 --time: 1.7494206428527832\n",
      "train acc:  1.0  train loss:  0.0002332697476958856 --time: 1.5547075271606445\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.7631423473358154\n",
      "train acc:  0.9996602106693849  train loss:  0.0017004798017138535 --time: 1.5432965755462646\n",
      "validation:  0.8866666666666667 --max 0.91 --time: 1.753556489944458\n",
      "train acc:  0.9789330615018689  train loss:  0.06632531500856756 --time: 1.52897310256958\n",
      "validation:  0.8433333333333334 --max 0.91 --time: 1.741943120956421\n",
      "train acc:  0.963302752293578  train loss:  0.1057449612442566 --time: 1.5501010417938232\n",
      "validation:  0.8866666666666667 --max 0.91 --time: 1.7559053897857666\n",
      "train acc:  0.9891267414203194  train loss:  0.031133586454772107 --time: 1.5402040481567383\n",
      "validation:  0.8733333333333333 --max 0.91 --time: 1.749669075012207\n",
      "train acc:  0.9955827387020048  train loss:  0.013010810373563563 --time: 1.5418851375579834\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.746978759765625\n",
      "train acc:  0.9918450560652395  train loss:  0.026964448025429865 --time: 1.5318098068237305\n",
      "validation:  0.88 --max 0.91 --time: 1.7386624813079834\n",
      "train acc:  0.9952429493713897  train loss:  0.019358134208469772 --time: 1.5613329410552979\n",
      "validation:  0.8966666666666666 --max 0.91 --time: 1.771845817565918\n",
      "train acc:  0.9962623173632348  train loss:  0.012515597459202147 --time: 1.5498032569885254\n",
      "validation:  0.92 --max 0.92 --time: 1.7530932426452637\n",
      "train acc:  0.9969418960244648  train loss:  0.012337552134261427 --time: 1.5285325050354004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:  0.8933333333333333 --max 0.92 --time: 1.732860803604126\n",
      "train acc:  0.9908256880733946  train loss:  0.022875100267929552 --time: 1.5314404964447021\n",
      "validation:  0.9 --max 0.92 --time: 1.7424962520599365\n",
      "train acc:  0.9932042133876996  train loss:  0.022072219746642866 --time: 1.5334994792938232\n",
      "validation:  0.9 --max 0.92 --time: 1.7391016483306885\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
