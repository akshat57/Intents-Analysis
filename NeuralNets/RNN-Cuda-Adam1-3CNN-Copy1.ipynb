{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.2599388379204893  train loss:  0.462859650021014 --time: 2.0463192462921143\n",
      "validation:  0.20666666666666667 --max 0.20666666666666667 --time: 2.3215293884277344\n",
      "train acc:  0.30479102956167176  train loss:  0.42803356828896894 --time: 2.011157751083374\n",
      "validation:  0.25 --max 0.25 --time: 2.257289409637451\n",
      "train acc:  0.32483860006795784  train loss:  0.4196511597737022 --time: 1.9788057804107666\n",
      "validation:  0.2633333333333333 --max 0.2633333333333333 --time: 2.260561227798462\n",
      "train acc:  0.34794427454977916  train loss:  0.4134252239828524 --time: 1.9069104194641113\n",
      "validation:  0.25333333333333335 --max 0.2633333333333333 --time: 2.150759696960449\n",
      "train acc:  0.3724091063540605  train loss:  0.4064413995846458 --time: 1.9566500186920166\n",
      "validation:  0.30333333333333334 --max 0.30333333333333334 --time: 2.208808660507202\n",
      "train acc:  0.41522256201155283  train loss:  0.39293039881664776 --time: 1.8750548362731934\n",
      "validation:  0.31 --max 0.31 --time: 2.118864059448242\n",
      "train acc:  0.47298674821610603  train loss:  0.37479677278062573 --time: 1.9372680187225342\n",
      "validation:  0.42333333333333334 --max 0.42333333333333334 --time: 2.181591272354126\n",
      "train acc:  0.5599728168535508  train loss:  0.3372517228126526 --time: 1.8342499732971191\n",
      "validation:  0.49333333333333335 --max 0.49333333333333335 --time: 2.0688536167144775\n",
      "train acc:  0.6398233095480802  train loss:  0.2891194055909696 --time: 1.9145753383636475\n",
      "validation:  0.5333333333333333 --max 0.5333333333333333 --time: 2.1708824634552\n",
      "train acc:  0.7186544342507645  train loss:  0.24197513901669046 --time: 1.9235813617706299\n",
      "validation:  0.6733333333333333 --max 0.6733333333333333 --time: 2.1522207260131836\n",
      "train acc:  0.782195039075773  train loss:  0.1976139590792034 --time: 1.8981282711029053\n",
      "validation:  0.68 --max 0.68 --time: 2.141216516494751\n",
      "train acc:  0.8267074413863404  train loss:  0.1678594609965449 --time: 1.8556139469146729\n",
      "validation:  0.7733333333333333 --max 0.7733333333333333 --time: 2.0930283069610596\n",
      "train acc:  0.8749575263336731  train loss:  0.13419798761606216 --time: 1.8250148296356201\n",
      "validation:  0.8133333333333334 --max 0.8133333333333334 --time: 2.06937575340271\n",
      "train acc:  0.9072375127420998  train loss:  0.10745539095090784 --time: 1.7850148677825928\n",
      "validation:  0.8466666666666667 --max 0.8466666666666667 --time: 2.0172078609466553\n",
      "train acc:  0.9344206591913014  train loss:  0.08324137815962666 --time: 1.8605883121490479\n",
      "validation:  0.88 --max 0.88 --time: 2.1101338863372803\n",
      "train acc:  0.9520897043832823  train loss:  0.06554688156946846 --time: 1.8681511878967285\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 2.1085638999938965\n",
      "train acc:  0.9595650696568128  train loss:  0.056285876294840935 --time: 1.851011037826538\n",
      "validation:  0.8333333333333334 --max 0.8933333333333333 --time: 2.0754318237304688\n",
      "train acc:  0.9585457016649678  train loss:  0.05369553885058216 --time: 1.833371639251709\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 2.0772457122802734\n",
      "train acc:  0.9813115868161739  train loss:  0.03542812144302804 --time: 1.8324108123779297\n",
      "validation:  0.88 --max 0.8933333333333333 --time: 2.110112190246582\n",
      "train acc:  0.9833503227998641  train loss:  0.02940093183323093 --time: 1.9696061611175537\n",
      "validation:  0.8833333333333333 --max 0.8933333333333333 --time: 2.267415761947632\n",
      "train acc:  0.9775739041794088  train loss:  0.031818144104403 --time: 1.8791253566741943\n",
      "validation:  0.88 --max 0.8933333333333333 --time: 2.1612024307250977\n",
      "train acc:  0.9870880054366293  train loss:  0.026169957917021668 --time: 2.775362968444824\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 3.1658589839935303\n",
      "train acc:  0.9874277947672443  train loss:  0.02189576642020889 --time: 2.7435736656188965\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 3.1362130641937256\n",
      "train acc:  0.9836901121304791  train loss:  0.023002772349054398 --time: 2.7454402446746826\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 3.12197208404541\n",
      "train acc:  0.9962623173632348  train loss:  0.012413311992650446 --time: 2.8408408164978027\n",
      "validation:  0.9033333333333333 --max 0.9033333333333333 --time: 3.1561355590820312\n",
      "train acc:  0.9972816853550799  train loss:  0.008568122395840675 --time: 2.9547038078308105\n",
      "validation:  0.92 --max 0.92 --time: 3.217254400253296\n",
      "train acc:  0.9972816853550799  train loss:  0.008126949081602304 --time: 2.8028500080108643\n",
      "validation:  0.91 --max 0.92 --time: 3.052717685699463\n",
      "train acc:  0.9966021066938499  train loss:  0.008532350518457268 --time: 2.786508321762085\n",
      "validation:  0.8733333333333333 --max 0.92 --time: 3.1436867713928223\n",
      "train acc:  0.9898063200815495  train loss:  0.014568445373974417 --time: 2.8197648525238037\n",
      "validation:  0.8733333333333333 --max 0.92 --time: 3.0832107067108154\n",
      "train acc:  0.9928644240570846  train loss:  0.012365512387908024 --time: 2.91052508354187\n",
      "validation:  0.8933333333333333 --max 0.92 --time: 3.231496572494507\n",
      "train acc:  0.9915052667346246  train loss:  0.011198452997790731 --time: 3.029510498046875\n",
      "validation:  0.8333333333333334 --max 0.92 --time: 3.3522863388061523\n",
      "train acc:  0.9755351681957186  train loss:  0.02854115301338227 --time: 2.8929920196533203\n",
      "validation:  0.8766666666666667 --max 0.92 --time: 3.2044131755828857\n",
      "train acc:  0.9901461094121644  train loss:  0.015094683748548445 --time: 2.9392282962799072\n",
      "validation:  0.8766666666666667 --max 0.92 --time: 3.368617057800293\n",
      "train acc:  0.9928644240570846  train loss:  0.010486113221344092 --time: 2.762890100479126\n",
      "validation:  0.91 --max 0.92 --time: 3.160405397415161\n",
      "train acc:  0.9976214746856948  train loss:  0.0064075301077378835 --time: 3.6516170501708984\n",
      "validation:  0.8833333333333333 --max 0.92 --time: 4.0957865715026855\n",
      "train acc:  0.9989806320081549  train loss:  0.004947266683144414 --time: 3.501077890396118\n",
      "validation:  0.89 --max 0.92 --time: 3.8396382331848145\n",
      "train acc:  0.99932042133877  train loss:  0.0039195655946336365 --time: 3.5632429122924805\n",
      "validation:  0.8933333333333333 --max 0.92 --time: 3.9713282585144043\n",
      "train acc:  0.9996602106693849  train loss:  0.0028777389544183793 --time: 3.884967565536499\n",
      "validation:  0.89 --max 0.92 --time: 4.195084810256958\n",
      "train acc:  0.9996602106693849  train loss:  0.00251670200985087 --time: 3.6778225898742676\n",
      "validation:  0.8933333333333333 --max 0.92 --time: 4.0028297901153564\n",
      "train acc:  0.9996602106693849  train loss:  0.002263215051599495 --time: 3.824223756790161\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
