{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati'\n",
    "test_language = 'gujarati'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=42, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(42, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati gujarati\n",
      "train acc:  0.3126061841658172  train loss:  0.44813953663991846 --time: 2.360799789428711\n",
      "validation:  0.28 --max 0.28 --time: 2.691272258758545\n",
      "train acc:  0.4332313965341488  train loss:  0.396387306244477 --time: 2.378668785095215\n",
      "validation:  0.34 --max 0.34 --time: 2.68068528175354\n",
      "train acc:  0.5385660890248046  train loss:  0.3567292949427729 --time: 2.3981332778930664\n",
      "validation:  0.4766666666666667 --max 0.4766666666666667 --time: 2.6495909690856934\n",
      "train acc:  0.6224940536867143  train loss:  0.3149246029231859 --time: 2.4795114994049072\n",
      "validation:  0.5066666666666667 --max 0.5066666666666667 --time: 2.753471851348877\n",
      "train acc:  0.6901121304791029  train loss:  0.278765553365583 --time: 2.5649852752685547\n",
      "validation:  0.5933333333333334 --max 0.5933333333333334 --time: 2.9114267826080322\n",
      "train acc:  0.7550118926265715  train loss:  0.23166200000306833 --time: 2.58225417137146\n",
      "validation:  0.7033333333333334 --max 0.7033333333333334 --time: 2.8435142040252686\n",
      "train acc:  0.8103975535168195  train loss:  0.18705174391684326 --time: 2.521332025527954\n",
      "validation:  0.7033333333333334 --max 0.7033333333333334 --time: 2.79964280128479\n",
      "train acc:  0.8501529051987767  train loss:  0.1566215429617011 --time: 2.432939052581787\n",
      "validation:  0.74 --max 0.74 --time: 2.7004029750823975\n",
      "train acc:  0.8702004757050629  train loss:  0.13692787160044131 --time: 2.4301815032958984\n",
      "validation:  0.7966666666666666 --max 0.7966666666666666 --time: 2.6992080211639404\n",
      "train acc:  0.90927624872579  train loss:  0.11176501117322755 --time: 2.4393184185028076\n",
      "validation:  0.83 --max 0.83 --time: 2.6880555152893066\n",
      "train acc:  0.9327217125382263  train loss:  0.08814142547223879 --time: 2.4387781620025635\n",
      "validation:  0.8466666666666667 --max 0.8466666666666667 --time: 2.7033543586730957\n",
      "train acc:  0.9435949711179069  train loss:  0.07877462260101152 --time: 2.4405133724212646\n",
      "validation:  0.8533333333333334 --max 0.8533333333333334 --time: 2.6981284618377686\n",
      "train acc:  0.9354400271831464  train loss:  0.07846535758479782 --time: 2.436851739883423\n",
      "validation:  0.8733333333333333 --max 0.8733333333333333 --time: 2.719592332839966\n",
      "train acc:  0.946313285762827  train loss:  0.06967670632445294 --time: 2.4267027378082275\n",
      "validation:  0.88 --max 0.88 --time: 2.700488567352295\n",
      "train acc:  0.9660210669384981  train loss:  0.05044639450700387 --time: 2.428953170776367\n",
      "validation:  0.86 --max 0.88 --time: 2.6830499172210693\n",
      "train acc:  0.9595650696568128  train loss:  0.05068041934915211 --time: 2.433925151824951\n",
      "validation:  0.8833333333333333 --max 0.8833333333333333 --time: 2.7130744457244873\n",
      "train acc:  0.9486918110771322  train loss:  0.058613724196734635 --time: 2.449733257293701\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 2.723440170288086\n",
      "train acc:  0.9565069656812776  train loss:  0.05044247898394647 --time: 2.414407253265381\n",
      "validation:  0.8866666666666667 --max 0.8866666666666667 --time: 2.754371404647827\n",
      "train acc:  0.9758749575263337  train loss:  0.034952387051737824 --time: 2.3622446060180664\n",
      "validation:  0.8866666666666667 --max 0.8866666666666667 --time: 2.6931440830230713\n",
      "train acc:  0.9857288481141692  train loss:  0.025778473719306614 --time: 2.2802772521972656\n",
      "validation:  0.8833333333333333 --max 0.8866666666666667 --time: 2.6379570960998535\n",
      "train acc:  0.9887869520897044  train loss:  0.021807006114850872 --time: 2.170229434967041\n",
      "validation:  0.8766666666666667 --max 0.8866666666666667 --time: 2.5251753330230713\n",
      "train acc:  0.9908256880733946  train loss:  0.01991675453989402 --time: 2.1366474628448486\n",
      "validation:  0.9 --max 0.9 --time: 2.488959550857544\n",
      "train acc:  0.9918450560652395  train loss:  0.016606413516337456 --time: 2.1514501571655273\n",
      "validation:  0.89 --max 0.9 --time: 2.5051395893096924\n",
      "train acc:  0.9894665307509344  train loss:  0.016304240440544876 --time: 2.05167293548584\n",
      "validation:  0.8766666666666667 --max 0.9 --time: 2.4307518005371094\n",
      "train acc:  0.9911654774040095  train loss:  0.016531748859130817 --time: 2.0885469913482666\n",
      "validation:  0.8833333333333333 --max 0.9 --time: 2.4248719215393066\n",
      "train acc:  0.9887869520897044  train loss:  0.016450666336585647 --time: 2.1097543239593506\n",
      "validation:  0.89 --max 0.9 --time: 2.4325942993164062\n",
      "train acc:  0.9921848453958546  train loss:  0.013257703885598026 --time: 2.0267622470855713\n",
      "validation:  0.88 --max 0.9 --time: 2.354255199432373\n",
      "train acc:  0.9983010533469249  train loss:  0.008154633171532465 --time: 2.122868537902832\n",
      "validation:  0.8866666666666667 --max 0.9 --time: 2.4500434398651123\n",
      "train acc:  0.9836901121304791  train loss:  0.020554717488424933 --time: 1.1849701404571533\n",
      "validation:  0.8633333333333333 --max 0.9 --time: 1.3982112407684326\n",
      "train acc:  0.9616038056405029  train loss:  0.03975043385087148 --time: 1.180957317352295\n",
      "validation:  0.8533333333333334 --max 0.9 --time: 1.4129300117492676\n",
      "train acc:  0.9813115868161739  train loss:  0.02541309267120517 --time: 1.1913716793060303\n",
      "validation:  0.9133333333333333 --max 0.9133333333333333 --time: 1.4006977081298828\n",
      "train acc:  0.9935440027183147  train loss:  0.01234858229999309 --time: 1.1597025394439697\n",
      "validation:  0.92 --max 0.92 --time: 1.374358892440796\n",
      "train acc:  0.9979612640163099  train loss:  0.006601073637442744 --time: 1.2024285793304443\n",
      "validation:  0.9166666666666666 --max 0.92 --time: 1.4460735321044922\n",
      "train acc:  0.9996602106693849  train loss:  0.004494756682897391 --time: 1.174543857574463\n",
      "validation:  0.92 --max 0.92 --time: 1.385894775390625\n",
      "train acc:  0.9996602106693849  train loss:  0.0035384877625366917 --time: 1.1796746253967285\n",
      "validation:  0.92 --max 0.92 --time: 1.3922171592712402\n",
      "train acc:  0.9996602106693849  train loss:  0.0031256325158250074 --time: 1.198481798171997\n",
      "validation:  0.9266666666666666 --max 0.9266666666666666 --time: 1.4144065380096436\n",
      "train acc:  0.9996602106693849  train loss:  0.002809576588966276 --time: 1.160813808441162\n",
      "validation:  0.9233333333333333 --max 0.9266666666666666 --time: 1.37837553024292\n",
      "train acc:  0.9996602106693849  train loss:  0.002614294168660822 --time: 1.173438310623169\n",
      "validation:  0.9266666666666666 --max 0.9266666666666666 --time: 1.4000389575958252\n",
      "train acc:  1.0  train loss:  0.0024212620767724256 --time: 1.185124397277832\n",
      "validation:  0.9166666666666666 --max 0.9266666666666666 --time: 1.40749192237854\n",
      "train acc:  1.0  train loss:  0.002237600061799521 --time: 1.1661431789398193\n",
      "validation:  0.91 --max 0.9266666666666666 --time: 1.3812122344970703\n",
      "train acc:  1.0  train loss:  0.0020695344453839503 --time: 1.1710045337677002\n",
      "validation:  0.92 --max 0.9266666666666666 --time: 1.4054124355316162\n",
      "train acc:  1.0  train loss:  0.0019038167640404856 --time: 1.185692548751831\n",
      "validation:  0.9166666666666666 --max 0.9266666666666666 --time: 1.402618169784546\n",
      "train acc:  1.0  train loss:  0.001809096044820288 --time: 1.176208734512329\n",
      "validation:  0.92 --max 0.9266666666666666 --time: 1.3933300971984863\n",
      "train acc:  1.0  train loss:  0.0017015774906410472 --time: 1.1928656101226807\n",
      "validation:  0.92 --max 0.9266666666666666 --time: 1.410609483718872\n",
      "train acc:  1.0  train loss:  0.0015954881116909826 --time: 1.1950011253356934\n",
      "validation:  0.91 --max 0.9266666666666666 --time: 1.41514253616333\n",
      "train acc:  1.0  train loss:  0.0015195012446897833 --time: 1.1806972026824951\n",
      "validation:  0.91 --max 0.9266666666666666 --time: 1.4050545692443848\n",
      "train acc:  1.0  train loss:  0.0014468861349008005 --time: 1.1981747150421143\n",
      "validation:  0.91 --max 0.9266666666666666 --time: 1.412092685699463\n",
      "train acc:  0.9901461094121644  train loss:  0.015690889072847432 --time: 1.1735069751739502\n",
      "validation:  0.8833333333333333 --max 0.9266666666666666 --time: 1.3891105651855469\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
