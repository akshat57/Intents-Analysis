{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.27285083248386  train loss:  1.6984009379925935 --time: 6.546808958053589\n",
      "validation:  0.23 --max 0.23 --time: 7.347276926040649\n",
      "train acc:  0.38600067957866124  train loss:  1.5645337363947993 --time: 6.641961097717285\n",
      "validation:  0.34 --max 0.34 --time: 7.432581663131714\n",
      "train acc:  0.4858987427794767  train loss:  1.4044488927592402 --time: 7.112956762313843\n",
      "validation:  0.49666666666666665 --max 0.49666666666666665 --time: 7.556798219680786\n",
      "train acc:  0.5864763846415223  train loss:  1.1350563764572144 --time: 6.9168806076049805\n",
      "validation:  0.5166666666666667 --max 0.5166666666666667 --time: 7.661699295043945\n",
      "train acc:  0.7098199116547741  train loss:  0.8389761370161305 --time: 7.8105762004852295\n",
      "validation:  0.6566666666666666 --max 0.6566666666666666 --time: 8.312441349029541\n",
      "train acc:  0.7879714576962283  train loss:  0.6366221801094387 --time: 6.069568872451782\n",
      "validation:  0.6933333333333334 --max 0.6933333333333334 --time: 6.723601579666138\n",
      "train acc:  0.7740400951410126  train loss:  0.6386882481367692 --time: 8.312464237213135\n",
      "validation:  0.72 --max 0.72 --time: 9.016578435897827\n",
      "train acc:  0.8392796466190962  train loss:  0.49240110490633093 --time: 6.25792932510376\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 7.08680534362793\n",
      "train acc:  0.8892286782195039  train loss:  0.328876582176789 --time: 7.841414928436279\n",
      "validation:  0.7866666666666666 --max 0.7866666666666666 --time: 8.574379444122314\n",
      "train acc:  0.9340808698606864  train loss:  0.22200988297877106 --time: 3.7056665420532227\n",
      "validation:  0.7866666666666666 --max 0.7866666666666666 --time: 4.634934425354004\n",
      "train acc:  0.9548080190282026  train loss:  0.16235885153646054 --time: 7.6485817432403564\n",
      "validation:  0.81 --max 0.81 --time: 8.44668436050415\n",
      "train acc:  0.9721372748895685  train loss:  0.10316886931009915 --time: 5.526714086532593\n",
      "validation:  0.8566666666666667 --max 0.8566666666666667 --time: 6.189392328262329\n",
      "train acc:  0.9670404349303432  train loss:  0.11616368429816287 --time: 7.657365322113037\n",
      "validation:  0.8366666666666667 --max 0.8566666666666667 --time: 8.458587884902954\n",
      "train acc:  0.9673802242609582  train loss:  0.10682190161036409 --time: 7.0028464794158936\n",
      "validation:  0.83 --max 0.8566666666666667 --time: 7.552998781204224\n",
      "train acc:  0.9602446483180428  train loss:  0.13926759686159051 --time: 7.334388971328735\n",
      "validation:  0.83 --max 0.8566666666666667 --time: 8.099015235900879\n",
      "train acc:  0.9775739041794088  train loss:  0.07988992754531943 --time: 6.988078832626343\n",
      "validation:  0.8233333333333334 --max 0.8566666666666667 --time: 7.801098108291626\n",
      "train acc:  0.9921848453958546  train loss:  0.038448348965333855 --time: 7.089865446090698\n",
      "validation:  0.8433333333333334 --max 0.8566666666666667 --time: 7.94131064414978\n",
      "train acc:  0.9891267414203194  train loss:  0.04060109413188437 --time: 6.817708492279053\n",
      "validation:  0.86 --max 0.86 --time: 7.543331623077393\n",
      "train acc:  0.9881073734284743  train loss:  0.03828732681501171 --time: 7.03242564201355\n",
      "validation:  0.85 --max 0.86 --time: 7.862605094909668\n",
      "train acc:  0.9925246347264696  train loss:  0.03220553335774204 --time: 6.089611291885376\n",
      "validation:  0.8466666666666667 --max 0.86 --time: 6.63723349571228\n",
      "train acc:  0.9932042133876996  train loss:  0.02253145530172016 --time: 6.983575105667114\n",
      "validation:  0.8566666666666667 --max 0.86 --time: 7.7829344272613525\n",
      "train acc:  0.9932042133876996  train loss:  0.029655872176036886 --time: 7.042441129684448\n",
      "validation:  0.8533333333333334 --max 0.86 --time: 7.909094333648682\n",
      "train acc:  0.9952429493713897  train loss:  0.01966646323790369 --time: 7.202458381652832\n",
      "validation:  0.87 --max 0.87 --time: 7.860738039016724\n",
      "train acc:  0.9945633707101597  train loss:  0.022984100971370935 --time: 6.425526142120361\n",
      "validation:  0.8566666666666667 --max 0.87 --time: 7.236855506896973\n",
      "train acc:  0.9979612640163099  train loss:  0.017952859158749165 --time: 6.5067548751831055\n",
      "validation:  0.8833333333333333 --max 0.8833333333333333 --time: 7.2522149085998535\n",
      "train acc:  0.9996602106693849  train loss:  0.004925957136868458 --time: 6.503270626068115\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 7.274723291397095\n",
      "train acc:  1.0  train loss:  0.002496463251705079 --time: 7.177746534347534\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 7.692278861999512\n",
      "train acc:  1.0  train loss:  0.0015090049457047944 --time: 6.845228433609009\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 7.630276918411255\n",
      "train acc:  1.0  train loss:  0.001250474412581357 --time: 7.482479095458984\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 8.010030508041382\n",
      "train acc:  1.0  train loss:  0.0010923624175357754 --time: 6.074806451797485\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 6.766744136810303\n",
      "train acc:  1.0  train loss:  0.0009671567670960466 --time: 7.959165573120117\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 8.610957622528076\n",
      "train acc:  1.0  train loss:  0.0008747028768993914 --time: 5.902108669281006\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 6.8166139125823975\n",
      "train acc:  1.0  train loss:  0.0008074150233448523 --time: 7.824298858642578\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 8.634798049926758\n",
      "train acc:  1.0  train loss:  0.0007640696383769746 --time: 6.021776437759399\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 6.80344557762146\n",
      "train acc:  1.0  train loss:  0.0006943976926698309 --time: 7.720638990402222\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 8.267657041549683\n",
      "train acc:  1.0  train loss:  0.0006756395286293296 --time: 5.223705291748047\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 5.945683717727661\n",
      "train acc:  1.0  train loss:  0.0006337607312319881 --time: 7.678479194641113\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 8.502661943435669\n",
      "train acc:  1.0  train loss:  0.0005884499354895366 --time: 6.622529983520508\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 7.218929767608643\n",
      "train acc:  1.0  train loss:  0.0005659194569528589 --time: 7.735275030136108\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 8.472732305526733\n",
      "train acc:  1.0  train loss:  0.0005110310195722495 --time: 6.351480484008789\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 7.207968235015869\n",
      "train acc:  1.0  train loss:  0.0004735481520385846 --time: 6.934739112854004\n",
      "validation:  0.87 --max 0.8833333333333333 --time: 7.5245466232299805\n",
      "train acc:  1.0  train loss:  0.0004817811726673466 --time: 7.142545223236084\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 7.473433017730713\n",
      "train acc:  1.0  train loss:  0.00044307490157813805 --time: 6.904526948928833\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 7.788023471832275\n",
      "train acc:  1.0  train loss:  0.0004297163180561493 --time: 8.003209352493286\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 8.9222092628479\n",
      "train acc:  1.0  train loss:  0.00040172561167982286 --time: 5.833906888961792\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 6.62067437171936\n",
      "train acc:  1.0  train loss:  0.00038925467347523767 --time: 6.743142604827881\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 7.183870553970337\n",
      "train acc:  1.0  train loss:  0.00037218488084719235 --time: 5.260029077529907\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 5.587048292160034\n",
      "train acc:  1.0  train loss:  0.00035047779902410895 --time: 4.283538818359375\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 4.687816381454468\n",
      "train acc:  1.0  train loss:  0.0003440657924131855 --time: 2.7853317260742188\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 3.1928417682647705\n",
      "train acc:  1.0  train loss:  0.0003331886198240049 --time: 5.948622465133667\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 6.8601295948028564\n",
      "train acc:  1.0  train loss:  0.00032493462039501935 --time: 4.862303018569946\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 5.324280500411987\n",
      "train acc:  1.0  train loss:  0.00030769060148209655 --time: 2.7069380283355713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 2.970675468444824\n",
      "train acc:  1.0  train loss:  0.0002961718912600823 --time: 3.6471328735351562\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 4.064972400665283\n",
      "train acc:  1.0  train loss:  0.0002894219202141318 --time: 5.962894439697266\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 6.828343391418457\n",
      "train acc:  1.0  train loss:  0.0002728780856831809 --time: 6.823275804519653\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 7.6277899742126465\n",
      "train acc:  1.0  train loss:  0.00026549225857825547 --time: 4.977572202682495\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 5.359417915344238\n",
      "train acc:  1.0  train loss:  0.00025889420066960156 --time: 4.216412782669067\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 4.826457262039185\n",
      "train acc:  1.0  train loss:  0.00024689314380774033 --time: 2.016679286956787\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 2.397754192352295\n",
      "train acc:  1.0  train loss:  0.0002585673197324428 --time: 5.961638689041138\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 6.593914031982422\n",
      "train acc:  1.0  train loss:  0.00025234372372759265 --time: 7.003299713134766\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 7.849048852920532\n",
      "train acc:  1.0  train loss:  0.00023525923746131846 --time: 6.536237716674805\n",
      "validation:  0.8833333333333333 --max 0.8833333333333333 --time: 7.350805997848511\n",
      "train acc:  1.0  train loss:  0.00022373388966788417 --time: 6.69941520690918\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 7.605142593383789\n",
      "train acc:  1.0  train loss:  0.000217300283585918 --time: 7.154815196990967\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 7.968215703964233\n",
      "train acc:  1.0  train loss:  0.00020836691660604075 --time: 6.709857940673828\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 7.370090961456299\n",
      "train acc:  1.0  train loss:  0.00020560378375787127 --time: 6.5999391078948975\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 7.009445905685425\n",
      "train acc:  1.0  train loss:  0.00020754631259478629 --time: 6.198297739028931\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 6.849327325820923\n",
      "train acc:  1.0  train loss:  0.00019189480530179065 --time: 7.986779689788818\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 8.563886165618896\n",
      "train acc:  1.0  train loss:  0.00018672279649130675 --time: 6.127373933792114\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 7.0341880321502686\n",
      "train acc:  1.0  train loss:  0.0001817317483389912 --time: 7.997638702392578\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 8.864047288894653\n",
      "train acc:  1.0  train loss:  0.00017361262428533772 --time: 5.540013790130615\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 6.32597017288208\n",
      "train acc:  1.0  train loss:  0.00017309142656259886 --time: 7.9886744022369385\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 8.667229413986206\n",
      "train acc:  1.0  train loss:  0.00016633565626208386 --time: 6.676517724990845\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 7.663586616516113\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
