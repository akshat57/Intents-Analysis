{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=256, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 256, num_layers=2)\n",
       "  (linear): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.2599388379204893  train loss:  0.4584223809449569 --time: 3.7435364723205566\n",
      "validation:  0.20333333333333334 --max 0.20333333333333334 --time: 4.294297695159912\n",
      "train acc:  0.30105334692490654  train loss:  0.4287249329297439 --time: 3.828629970550537\n",
      "validation:  0.23333333333333334 --max 0.23333333333333334 --time: 4.278718709945679\n",
      "train acc:  0.3306150186884132  train loss:  0.42007971716963727 --time: 3.547771453857422\n",
      "validation:  0.22666666666666666 --max 0.23333333333333334 --time: 4.097537517547607\n",
      "train acc:  0.3312945973496432  train loss:  0.41525570594746136 --time: 4.087919473648071\n",
      "validation:  0.25666666666666665 --max 0.25666666666666665 --time: 4.6581971645355225\n",
      "train acc:  0.3571185864763846  train loss:  0.4079911851364633 --time: 3.144383668899536\n",
      "validation:  0.26666666666666666 --max 0.26666666666666666 --time: 3.5965676307678223\n",
      "train acc:  0.40129119945633707  train loss:  0.39424754484840063 --time: 3.089975357055664\n",
      "validation:  0.31 --max 0.31 --time: 3.530773401260376\n",
      "train acc:  0.4838600067957866  train loss:  0.36438741113828577 --time: 3.1443958282470703\n",
      "validation:  0.41 --max 0.41 --time: 3.564340353012085\n",
      "train acc:  0.5701664967720014  train loss:  0.31735772024030273 --time: 3.116971015930176\n",
      "validation:  0.37333333333333335 --max 0.41 --time: 3.5087718963623047\n",
      "train acc:  0.6170574243968739  train loss:  0.2865000606878944 --time: 3.0786468982696533\n",
      "validation:  0.5133333333333333 --max 0.5133333333333333 --time: 3.452202558517456\n",
      "train acc:  0.6608902480462113  train loss:  0.2427886160819427 --time: 3.2497622966766357\n",
      "validation:  0.52 --max 0.52 --time: 3.6670966148376465\n",
      "train acc:  0.6969079170914033  train loss:  0.21517415733441062 --time: 3.2127044200897217\n",
      "validation:  0.5933333333333334 --max 0.5933333333333334 --time: 3.5675082206726074\n",
      "train acc:  0.7220523275569147  train loss:  0.19261964118998984 --time: 3.1834330558776855\n",
      "validation:  0.6033333333333334 --max 0.6033333333333334 --time: 3.5402469635009766\n",
      "train acc:  0.7302072714916752  train loss:  0.19179086516732755 --time: 3.8531603813171387\n",
      "validation:  0.6066666666666667 --max 0.6066666666666667 --time: 4.30647873878479\n",
      "train acc:  0.7587495752633368  train loss:  0.1732342048831608 --time: 4.051557540893555\n",
      "validation:  0.5566666666666666 --max 0.6066666666666667 --time: 4.525529861450195\n",
      "train acc:  0.7876316683656133  train loss:  0.15828358284805133 --time: 4.1623375415802\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 4.589684009552002\n",
      "train acc:  0.8100577641862046  train loss:  0.14446051515962766 --time: 4.276163578033447\n",
      "validation:  0.6966666666666667 --max 0.6966666666666667 --time: 4.804262638092041\n",
      "train acc:  0.8250084947332654  train loss:  0.1427535084278687 --time: 4.221233367919922\n",
      "validation:  0.6766666666666666 --max 0.6966666666666667 --time: 4.707740306854248\n",
      "train acc:  0.8586476384641523  train loss:  0.11491953873116037 --time: 4.208874464035034\n",
      "validation:  0.7833333333333333 --max 0.7833333333333333 --time: 4.730406045913696\n",
      "train acc:  0.8888888888888888  train loss:  0.09452734596055487 --time: 4.319302558898926\n",
      "validation:  0.76 --max 0.7833333333333333 --time: 4.718450546264648\n",
      "train acc:  0.9157322460074754  train loss:  0.07772755039774853 --time: 4.241155624389648\n",
      "validation:  0.7866666666666666 --max 0.7866666666666666 --time: 4.811168432235718\n",
      "train acc:  0.9160720353380903  train loss:  0.07264962679018146 --time: 3.9632480144500732\n",
      "validation:  0.7966666666666666 --max 0.7966666666666666 --time: 4.500025749206543\n",
      "train acc:  0.945633707101597  train loss:  0.05299556125765261 --time: 3.9809436798095703\n",
      "validation:  0.8066666666666666 --max 0.8066666666666666 --time: 4.520093679428101\n",
      "train acc:  0.963642541624193  train loss:  0.041204971787722214 --time: 3.941779375076294\n",
      "validation:  0.8466666666666667 --max 0.8466666666666667 --time: 4.518079996109009\n",
      "train acc:  0.9378185524974516  train loss:  0.05894674506524335 --time: 3.992432117462158\n",
      "validation:  0.7866666666666666 --max 0.8466666666666667 --time: 4.596328258514404\n",
      "train acc:  0.946313285762827  train loss:  0.05695113264348196 --time: 3.7865705490112305\n",
      "validation:  0.82 --max 0.8466666666666667 --time: 4.332223653793335\n",
      "train acc:  0.9602446483180428  train loss:  0.04224077790327694 --time: 4.008829355239868\n",
      "validation:  0.8466666666666667 --max 0.8466666666666667 --time: 4.594739198684692\n",
      "train acc:  0.9755351681957186  train loss:  0.028583147684517113 --time: 3.9119412899017334\n",
      "validation:  0.85 --max 0.85 --time: 4.505358934402466\n",
      "train acc:  0.9714576962283384  train loss:  0.028451952840323032 --time: 4.077918291091919\n",
      "validation:  0.85 --max 0.85 --time: 4.650391578674316\n",
      "train acc:  0.9792728508324838  train loss:  0.02260952337604502 --time: 3.866934299468994\n",
      "validation:  0.85 --max 0.85 --time: 4.460114479064941\n",
      "train acc:  0.9833503227998641  train loss:  0.020299884207222774 --time: 4.029437303543091\n",
      "validation:  0.86 --max 0.86 --time: 4.6321985721588135\n",
      "train acc:  0.9847094801223242  train loss:  0.020996900194365044 --time: 4.071193218231201\n",
      "validation:  0.8466666666666667 --max 0.86 --time: 4.6609625816345215\n",
      "train acc:  0.9867482161060143  train loss:  0.018298175287149523 --time: 4.066051483154297\n",
      "validation:  0.86 --max 0.86 --time: 4.565078496932983\n",
      "train acc:  0.9908256880733946  train loss:  0.01219795534954123 --time: 4.162219285964966\n",
      "validation:  0.84 --max 0.86 --time: 4.735114812850952\n",
      "train acc:  0.9925246347264696  train loss:  0.01122801866301376 --time: 4.173174858093262\n",
      "validation:  0.8533333333333334 --max 0.86 --time: 4.582401990890503\n",
      "train acc:  0.9932042133876996  train loss:  0.010877555398189503 --time: 4.2640700340271\n",
      "validation:  0.8666666666666667 --max 0.8666666666666667 --time: 4.618159532546997\n",
      "train acc:  0.9935440027183147  train loss:  0.009086734098989678 --time: 4.3916871547698975\n",
      "validation:  0.8466666666666667 --max 0.8666666666666667 --time: 4.900223016738892\n",
      "train acc:  0.9864084267753993  train loss:  0.015858019499675087 --time: 4.615518093109131\n",
      "validation:  0.8433333333333334 --max 0.8666666666666667 --time: 4.991305828094482\n",
      "train acc:  0.9891267414203194  train loss:  0.014758234712011788 --time: 4.574426651000977\n",
      "validation:  0.8666666666666667 --max 0.8666666666666667 --time: 5.088940620422363\n",
      "train acc:  0.983010533469249  train loss:  0.01919436432502192 --time: 4.480630159378052\n",
      "validation:  0.8466666666666667 --max 0.8666666666666667 --time: 4.893752813339233\n",
      "train acc:  0.9959225280326198  train loss:  0.007275713870868734 --time: 4.3505308628082275\n",
      "validation:  0.8866666666666667 --max 0.8866666666666667 --time: 4.747515439987183\n",
      "train acc:  0.9955827387020048  train loss:  0.0064074097342951145 --time: 4.353663921356201\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 4.731499433517456\n",
      "train acc:  0.9996602106693849  train loss:  0.0026373930639870787 --time: 4.446656942367554\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 4.94919228553772\n",
      "train acc:  1.0  train loss:  0.0015959550940391162 --time: 4.317015886306763\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 4.860460519790649\n",
      "train acc:  0.9976214746856948  train loss:  0.004846911907762937 --time: 4.4695799350738525\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 4.928594589233398\n",
      "train acc:  0.9860686374447842  train loss:  0.015425618818920591 --time: 4.43330192565918\n",
      "validation:  0.8466666666666667 --max 0.8933333333333333 --time: 4.833388566970825\n",
      "train acc:  0.9932042133876996  train loss:  0.009901989575313486 --time: 4.554697751998901\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 4.938464879989624\n",
      "train acc:  0.9955827387020048  train loss:  0.007891843125791005 --time: 4.608044385910034\n",
      "validation:  0.88 --max 0.8933333333333333 --time: 5.095052242279053\n",
      "train acc:  0.9891267414203194  train loss:  0.011990593574212297 --time: 4.497737407684326\n",
      "validation:  0.8466666666666667 --max 0.8933333333333333 --time: 4.928232192993164\n",
      "train acc:  0.9884471627590894  train loss:  0.014392254691894936 --time: 4.39180064201355\n",
      "validation:  0.86 --max 0.8933333333333333 --time: 4.878194332122803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9874277947672443  train loss:  0.014717712289775196 --time: 4.359352111816406\n",
      "validation:  0.8566666666666667 --max 0.8933333333333333 --time: 4.881396532058716\n",
      "train acc:  0.9942235813795447  train loss:  0.009484508064696971 --time: 4.348232984542847\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 4.87553334236145\n",
      "train acc:  0.9955827387020048  train loss:  0.006389366007288513 --time: 4.45254373550415\n",
      "validation:  0.87 --max 0.8933333333333333 --time: 4.956547737121582\n",
      "train acc:  0.9962623173632348  train loss:  0.005615341775726689 --time: 4.498395681381226\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 4.94649600982666\n",
      "train acc:  0.9983010533469249  train loss:  0.0031303868922607407 --time: 4.57976508140564\n",
      "validation:  0.8733333333333333 --max 0.8933333333333333 --time: 4.930919408798218\n",
      "train acc:  0.9989806320081549  train loss:  0.0022855439506795096 --time: 4.554166793823242\n",
      "validation:  0.8766666666666667 --max 0.8933333333333333 --time: 5.0660240650177\n",
      "train acc:  1.0  train loss:  0.0010596313336661653 --time: 4.525731325149536\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 4.956642150878906\n",
      "train acc:  1.0  train loss:  0.0008049931569510828 --time: 4.36891770362854\n",
      "validation:  0.8833333333333333 --max 0.8933333333333333 --time: 4.824875116348267\n",
      "train acc:  1.0  train loss:  0.0006767224872221603 --time: 4.277667760848999\n",
      "validation:  0.8833333333333333 --max 0.8933333333333333 --time: 4.800514221191406\n",
      "train acc:  1.0  train loss:  0.000593875489278656 --time: 4.263361692428589\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 4.809414863586426\n",
      "train acc:  1.0  train loss:  0.0005219634862758381 --time: 3.9884650707244873\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 4.3547022342681885\n",
      "train acc:  1.0  train loss:  0.0004977647913619876 --time: 3.4880030155181885\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 3.837517738342285\n",
      "train acc:  1.0  train loss:  0.00043997314566260445 --time: 3.4507646560668945\n",
      "validation:  0.8833333333333333 --max 0.8933333333333333 --time: 3.822110652923584\n",
      "train acc:  1.0  train loss:  0.0004283759107752501 --time: 4.3921425342559814\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 4.80398154258728\n",
      "train acc:  1.0  train loss:  0.00039031047186733264 --time: 4.630545377731323\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 4.992011308670044\n",
      "train acc:  1.0  train loss:  0.0003639044450170806 --time: 3.3212592601776123\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 3.6460328102111816\n",
      "train acc:  1.0  train loss:  0.0003482536703813821 --time: 3.128304958343506\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 3.4669909477233887\n",
      "train acc:  1.0  train loss:  0.0003242126119363567 --time: 3.1878087520599365\n",
      "validation:  0.8966666666666666 --max 0.8966666666666666 --time: 3.692601203918457\n",
      "train acc:  1.0  train loss:  0.00030924424493644875 --time: 4.2821784019470215\n",
      "validation:  0.8933333333333333 --max 0.8966666666666666 --time: 4.7494425773620605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5aab3af95490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mout_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0macc_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
