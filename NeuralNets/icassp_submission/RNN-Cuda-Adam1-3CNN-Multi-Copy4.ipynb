{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati_marathi_bengali_hindi_25'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/3_lang_variations/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=70, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(70, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati_marathi_bengali_hindi_25 hindi\n",
      "train acc:  0.24566768603465852  train loss:  0.46231481432914734 --time: 4.969926357269287\n",
      "validation:  0.17333333333333334 --max 0.17333333333333334 --time: 5.360084772109985\n",
      "train acc:  0.28304451240231054  train loss:  0.43329679058945697 --time: 4.8866071701049805\n",
      "validation:  0.18 --max 0.18 --time: 5.400531053543091\n",
      "train acc:  0.31668365613319743  train loss:  0.4251184087732564 --time: 4.731136322021484\n",
      "validation:  0.21333333333333335 --max 0.21333333333333335 --time: 5.287258625030518\n",
      "train acc:  0.3421678559293238  train loss:  0.4164958764677462 --time: 4.737382888793945\n",
      "validation:  0.21 --max 0.21333333333333335 --time: 5.2473859786987305\n",
      "train acc:  0.4260958205912334  train loss:  0.3939354147600091 --time: 4.7077085971832275\n",
      "validation:  0.2866666666666667 --max 0.2866666666666667 --time: 5.063153982162476\n",
      "train acc:  0.47570506286102615  train loss:  0.3729525934094968 --time: 4.557722806930542\n",
      "validation:  0.3566666666666667 --max 0.3566666666666667 --time: 4.940714359283447\n",
      "train acc:  0.5460414542983351  train loss:  0.3412672799566518 --time: 4.538792848587036\n",
      "validation:  0.43666666666666665 --max 0.43666666666666665 --time: 5.011389970779419\n",
      "train acc:  0.6065239551478083  train loss:  0.3060093211091083 --time: 4.462695121765137\n",
      "validation:  0.4766666666666667 --max 0.4766666666666667 --time: 5.018423318862915\n",
      "train acc:  0.6520557254502208  train loss:  0.273819950611695 --time: 4.203704595565796\n",
      "validation:  0.5233333333333333 --max 0.5233333333333333 --time: 4.7999842166900635\n",
      "train acc:  0.6921508664627931  train loss:  0.24659221910912058 --time: 4.246539831161499\n",
      "validation:  0.5433333333333333 --max 0.5433333333333333 --time: 4.865762948989868\n",
      "train acc:  0.7295276928304452  train loss:  0.22200040519237518 --time: 4.1744704246521\n",
      "validation:  0.5733333333333334 --max 0.5733333333333334 --time: 4.843839406967163\n",
      "train acc:  0.7689432551817873  train loss:  0.19792217793671982 --time: 4.164537191390991\n",
      "validation:  0.59 --max 0.59 --time: 4.811002016067505\n",
      "train acc:  0.780835881753313  train loss:  0.1866180754226187 --time: 4.119884252548218\n",
      "validation:  0.65 --max 0.65 --time: 4.716678142547607\n",
      "train acc:  0.8049609242269793  train loss:  0.1692942361468854 --time: 4.102609395980835\n",
      "validation:  0.6533333333333333 --max 0.6533333333333333 --time: 4.71957540512085\n",
      "train acc:  0.8161739721372749  train loss:  0.15940030761387036 --time: 4.558317184448242\n",
      "validation:  0.66 --max 0.66 --time: 5.095057725906372\n",
      "train acc:  0.8481141692150866  train loss:  0.13876144328842993 --time: 4.3890697956085205\n",
      "validation:  0.6866666666666666 --max 0.6866666666666666 --time: 5.020586729049683\n",
      "train acc:  0.872918790349983  train loss:  0.1263951354700586 --time: 4.120640516281128\n",
      "validation:  0.66 --max 0.6866666666666666 --time: 4.743159532546997\n",
      "train acc:  0.8780156303092083  train loss:  0.11630030527063039 --time: 4.24507737159729\n",
      "validation:  0.6766666666666666 --max 0.6866666666666666 --time: 4.836437463760376\n",
      "train acc:  0.9038396194359497  train loss:  0.10125614022431166 --time: 4.0477454662323\n",
      "validation:  0.72 --max 0.72 --time: 4.68720269203186\n",
      "train acc:  0.908936459395175  train loss:  0.09425719147143156 --time: 4.26832389831543\n",
      "validation:  0.66 --max 0.72 --time: 4.915443420410156\n",
      "train acc:  0.9001019367991845  train loss:  0.09727879808000896 --time: 4.240985631942749\n",
      "validation:  0.7266666666666667 --max 0.7266666666666667 --time: 4.867185354232788\n",
      "train acc:  0.9381583418280666  train loss:  0.07406639599281808 --time: 4.1485748291015625\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 4.654132604598999\n",
      "train acc:  0.9446143391097519  train loss:  0.06539272857101067 --time: 4.124354124069214\n",
      "validation:  0.7566666666666667 --max 0.7566666666666667 --time: 4.771387338638306\n",
      "train acc:  0.9582059123343527  train loss:  0.05612536454978197 --time: 3.9842751026153564\n",
      "validation:  0.76 --max 0.76 --time: 4.632882118225098\n",
      "train acc:  0.9548080190282026  train loss:  0.0543300056749064 --time: 4.221035003662109\n",
      "validation:  0.7633333333333333 --max 0.7633333333333333 --time: 4.720486164093018\n",
      "train acc:  0.9687393815834183  train loss:  0.044679138647473374 --time: 4.407667636871338\n",
      "validation:  0.7766666666666666 --max 0.7766666666666666 --time: 5.0138750076293945\n",
      "train acc:  0.9799524294937139  train loss:  0.03518606641370317 --time: 4.593299627304077\n",
      "validation:  0.77 --max 0.7766666666666666 --time: 5.126030445098877\n",
      "train acc:  0.9864084267753993  train loss:  0.0267650677782038 --time: 4.43024754524231\n",
      "validation:  0.7866666666666666 --max 0.7866666666666666 --time: 4.938436985015869\n",
      "train acc:  0.9748555895344886  train loss:  0.033868864016688385 --time: 4.696589231491089\n",
      "validation:  0.76 --max 0.7866666666666666 --time: 5.2409679889678955\n",
      "train acc:  0.9789330615018689  train loss:  0.03152918904695822 --time: 4.582968235015869\n",
      "validation:  0.7733333333333333 --max 0.7866666666666666 --time: 5.200086355209351\n",
      "train acc:  0.9887869520897044  train loss:  0.021595551834806152 --time: 4.397595405578613\n",
      "validation:  0.7766666666666666 --max 0.7866666666666666 --time: 4.768429517745972\n",
      "train acc:  0.9918450560652395  train loss:  0.01682614178761192 --time: 4.602983236312866\n",
      "validation:  0.7766666666666666 --max 0.7866666666666666 --time: 4.939526796340942\n",
      "train acc:  0.9836901121304791  train loss:  0.02482767403125763 --time: 4.772948741912842\n",
      "validation:  0.7566666666666667 --max 0.7866666666666666 --time: 5.131649971008301\n",
      "train acc:  0.9911654774040095  train loss:  0.01588910295749488 --time: 4.6211981773376465\n",
      "validation:  0.77 --max 0.7866666666666666 --time: 5.015938997268677\n",
      "train acc:  0.9925246347264696  train loss:  0.014131505609206532 --time: 4.71234655380249\n",
      "validation:  0.7666666666666667 --max 0.7866666666666666 --time: 5.1479692459106445\n",
      "train acc:  0.9898063200815495  train loss:  0.015879497296460297 --time: 4.700733661651611\n",
      "validation:  0.7866666666666666 --max 0.7866666666666666 --time: 5.258328437805176\n",
      "train acc:  0.9925246347264696  train loss:  0.013476490407534267 --time: 4.7777392864227295\n",
      "validation:  0.75 --max 0.7866666666666666 --time: 5.283015966415405\n",
      "train acc:  0.9915052667346246  train loss:  0.012902261709551449 --time: 4.608166217803955\n",
      "validation:  0.77 --max 0.7866666666666666 --time: 5.078479290008545\n",
      "train acc:  0.9928644240570846  train loss:  0.012485410269025875 --time: 4.7194085121154785\n",
      "validation:  0.79 --max 0.79 --time: 5.206051826477051\n",
      "train acc:  0.9898063200815495  train loss:  0.01429855756704574 --time: 4.520947456359863\n",
      "validation:  0.7933333333333333 --max 0.7933333333333333 --time: 4.931054592132568\n",
      "train acc:  0.9765545361875637  train loss:  0.02480444413326357 --time: 4.530901193618774\n",
      "validation:  0.81 --max 0.81 --time: 5.001021862030029\n",
      "train acc:  0.9711179068977234  train loss:  0.03082939917626588 --time: 4.526465177536011\n",
      "validation:  0.75 --max 0.81 --time: 5.164713382720947\n",
      "train acc:  0.9935440027183147  train loss:  0.012344061862677336 --time: 4.576351881027222\n",
      "validation:  0.78 --max 0.81 --time: 5.165764093399048\n",
      "train acc:  0.9921848453958546  train loss:  0.01199259888380766 --time: 4.449432849884033\n",
      "validation:  0.7866666666666666 --max 0.81 --time: 4.95317006111145\n",
      "train acc:  0.9942235813795447  train loss:  0.009929350638033255 --time: 4.541003465652466\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.192105531692505\n",
      "train acc:  0.9976214746856948  train loss:  0.006333947961178163 --time: 4.172985076904297\n",
      "validation:  0.7833333333333333 --max 0.81 --time: 4.8945276737213135\n",
      "train acc:  0.9966021066938499  train loss:  0.006299337807475873 --time: 4.311209440231323\n",
      "validation:  0.74 --max 0.81 --time: 4.918425559997559\n",
      "train acc:  0.9979612640163099  train loss:  0.005774898292577785 --time: 4.02762770652771\n",
      "validation:  0.7666666666666667 --max 0.81 --time: 4.724657297134399\n",
      "train acc:  1.0  train loss:  0.0031718631457213473 --time: 3.906449317932129\n",
      "validation:  0.7866666666666666 --max 0.81 --time: 4.592838525772095\n",
      "train acc:  1.0  train loss:  0.0025647799281970315 --time: 3.974562883377075\n",
      "validation:  0.7866666666666666 --max 0.81 --time: 4.626938581466675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9996602106693849  train loss:  0.002329244971801729 --time: 4.074276447296143\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 4.682424545288086\n",
      "train acc:  1.0  train loss:  0.00203534133185673 --time: 4.150259971618652\n",
      "validation:  0.81 --max 0.81 --time: 4.821230411529541\n",
      "train acc:  1.0  train loss:  0.0019174764975500495 --time: 4.1147027015686035\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.8164074420928955\n",
      "train acc:  1.0  train loss:  0.001763305738163383 --time: 4.052748441696167\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.69231104850769\n",
      "train acc:  1.0  train loss:  0.0016620355283679521 --time: 3.871042490005493\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.614203453063965\n",
      "train acc:  1.0  train loss:  0.0015877300377368279 --time: 4.088138103485107\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.744243860244751\n",
      "train acc:  1.0  train loss:  0.0015139553959116988 --time: 4.090683937072754\n",
      "validation:  0.81 --max 0.81 --time: 4.52926778793335\n",
      "train acc:  1.0  train loss:  0.001451704949747933 --time: 4.64958930015564\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.213501930236816\n",
      "train acc:  1.0  train loss:  0.0013487189198317735 --time: 4.524371385574341\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.149977684020996\n",
      "train acc:  1.0  train loss:  0.0012816256266492217 --time: 4.566550016403198\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 5.150249481201172\n",
      "train acc:  1.0  train loss:  0.0012366056249922385 --time: 4.389045238494873\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.961667537689209\n",
      "train acc:  1.0  train loss:  0.001164618312664654 --time: 4.517796516418457\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.047992944717407\n",
      "train acc:  1.0  train loss:  0.0011093080823269227 --time: 4.240959644317627\n",
      "validation:  0.81 --max 0.81 --time: 4.815271615982056\n",
      "train acc:  1.0  train loss:  0.0010640559358643773 --time: 4.213587522506714\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.8132007122039795\n",
      "train acc:  1.0  train loss:  0.001011319924145937 --time: 4.270118236541748\n",
      "validation:  0.8 --max 0.81 --time: 4.866801738739014\n",
      "train acc:  1.0  train loss:  0.0009834135897741046 --time: 4.219881772994995\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 4.864930152893066\n",
      "train acc:  1.0  train loss:  0.0009395701193210224 --time: 3.9990739822387695\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 4.603100776672363\n",
      "train acc:  1.0  train loss:  0.0009137648208390759 --time: 3.8865976333618164\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 4.487908363342285\n",
      "train acc:  1.0  train loss:  0.0008753565286853067 --time: 3.6237552165985107\n",
      "validation:  0.8 --max 0.81 --time: 4.231080055236816\n",
      "train acc:  1.0  train loss:  0.0008458572505649341 --time: 3.4821794033050537\n",
      "validation:  0.8066666666666666 --max 0.81 --time: 4.1413657665252686\n",
      "train acc:  1.0  train loss:  0.0008160355627415297 --time: 3.9978585243225098\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 4.663892507553101\n",
      "train acc:  1.0  train loss:  0.0007879397396565131 --time: 4.184850215911865\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 4.621958255767822\n",
      "train acc:  1.0  train loss:  0.0007623429764467089 --time: 4.524741172790527\n",
      "validation:  0.8 --max 0.81 --time: 5.025051116943359\n",
      "train acc:  1.0  train loss:  0.0007353907618064271 --time: 4.652582883834839\n",
      "validation:  0.8 --max 0.81 --time: 5.17315936088562\n",
      "train acc:  1.0  train loss:  0.0007139614266950799 --time: 4.731314420700073\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.2707297801971436\n",
      "train acc:  1.0  train loss:  0.0006946864203833368 --time: 4.674212217330933\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.200483083724976\n",
      "train acc:  1.0  train loss:  0.0006714390094756433 --time: 4.702383041381836\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.3339478969573975\n",
      "train acc:  1.0  train loss:  0.0006481239106506109 --time: 4.5498528480529785\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 5.037537097930908\n",
      "train acc:  1.0  train loss:  0.0006262301290205315 --time: 4.481163740158081\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 5.0871922969818115\n",
      "train acc:  1.0  train loss:  0.0006100884006034745 --time: 4.538139581680298\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 5.009157657623291\n",
      "train acc:  1.0  train loss:  0.00058759323766698 --time: 4.014403820037842\n",
      "validation:  0.7866666666666666 --max 0.81 --time: 4.654861927032471\n",
      "train acc:  1.0  train loss:  0.0005710807441652793 --time: 3.6208157539367676\n",
      "validation:  0.7933333333333333 --max 0.81 --time: 4.270962953567505\n",
      "train acc:  1.0  train loss:  0.0005541534729949806 --time: 6.87851619720459\n",
      "validation:  0.7933333333333333 --max 0.81 --time: 7.910893201828003\n",
      "train acc:  0.9466530750934421  train loss:  0.05223883516863798 --time: 7.891491651535034\n",
      "validation:  0.7533333333333333 --max 0.81 --time: 8.743124008178711\n",
      "train acc:  0.9351002378525314  train loss:  0.059765773055993995 --time: 7.611429929733276\n",
      "validation:  0.76 --max 0.81 --time: 8.604225635528564\n",
      "train acc:  0.9785932721712538  train loss:  0.028205685762931473 --time: 7.432090759277344\n",
      "validation:  0.7433333333333333 --max 0.81 --time: 8.150435209274292\n",
      "train acc:  0.9877675840978594  train loss:  0.015741231021187876 --time: 9.345972537994385\n",
      "validation:  0.7566666666666667 --max 0.81 --time: 10.75620412826538\n",
      "train acc:  0.9884471627590894  train loss:  0.013827172796363417 --time: 10.62825632095337\n",
      "validation:  0.7533333333333333 --max 0.81 --time: 11.784772872924805\n",
      "train acc:  0.9925246347264696  train loss:  0.009851704209880985 --time: 8.963352918624878\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 10.039777278900146\n",
      "train acc:  0.9949031600407747  train loss:  0.0075257856743005305 --time: 8.620059490203857\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 9.562526941299438\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
