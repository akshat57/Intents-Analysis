{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=512, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[-1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 512, num_layers=2)\n",
       "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5512737338350417  train loss:  1.4118507382619447 --time: 27.86872410774231\n",
      "0 validation:  0.8056434599156118 --max 0.8056434599156118 --time: 31.728564739227295\n",
      "train acc:  0.8186497123826824  train loss:  0.5710709427601725 --time: 28.117270946502686\n",
      "1 validation:  0.8763185654008439 --max 0.8763185654008439 --time: 32.060144662857056\n",
      "train acc:  0.8787682193676745  train loss:  0.3818525254561756 --time: 28.20096755027771\n",
      "2 validation:  0.8910864978902954 --max 0.8910864978902954 --time: 32.113574743270874\n",
      "train acc:  0.911119761256001  train loss:  0.27960210075365244 --time: 28.326701879501343\n",
      "3 validation:  0.8995253164556962 --max 0.8995253164556962 --time: 32.24082398414612\n",
      "train acc:  0.9302798321871891  train loss:  0.21068566177266737 --time: 28.3791606426239\n",
      "4 validation:  0.9032172995780591 --max 0.9032172995780591 --time: 32.25114440917969\n",
      "train acc:  0.9468881103758487  train loss:  0.1592839029694789 --time: 28.320539474487305\n",
      "5 validation:  0.9142932489451476 --max 0.9142932489451476 --time: 32.138306856155396\n",
      "train acc:  0.9644046537779508  train loss:  0.11325372811098125 --time: 28.37551999092102\n",
      "6 validation:  0.9111286919831224 --max 0.9142932489451476 --time: 32.339667320251465\n",
      "train acc:  0.9713247696898923  train loss:  0.08718135057644949 --time: 28.524613857269287\n",
      "7 validation:  0.9127109704641351 --max 0.9142932489451476 --time: 32.32058024406433\n",
      "train acc:  0.9758228450326543  train loss:  0.07384225673644253 --time: 28.51314687728882\n",
      "8 validation:  0.9121835443037974 --max 0.9142932489451476 --time: 32.2339813709259\n",
      "train acc:  0.9756930928593054  train loss:  0.07466405776241866 --time: 28.351613998413086\n",
      "9 validation:  0.9119198312236287 --max 0.9142932489451476 --time: 32.09478259086609\n",
      "train acc:  0.9822672029756498  train loss:  0.0575002313113015 --time: 28.35294485092163\n",
      "10 validation:  0.9169303797468354 --max 0.9169303797468354 --time: 32.32362103462219\n",
      "train acc:  0.9828727131179448  train loss:  0.054142377352167065 --time: 28.539567708969116\n",
      "11 validation:  0.9092827004219409 --max 0.9169303797468354 --time: 32.47190761566162\n",
      "train acc:  0.9822672029756498  train loss:  0.05378908178774197 --time: 28.302239656448364\n",
      "12 validation:  0.8987341772151899 --max 0.9169303797468354 --time: 32.24978804588318\n",
      "train acc:  0.9810129319666104  train loss:  0.054958511143922806 --time: 28.29281759262085\n",
      "13 validation:  0.9164029535864979 --max 0.9169303797468354 --time: 32.196465492248535\n",
      "train acc:  0.9909173478655767  train loss:  0.028320420502546084 --time: 28.312679767608643\n",
      "14 validation:  0.9124472573839663 --max 0.9169303797468354 --time: 32.18441200256348\n",
      "train acc:  0.9955451753816876  train loss:  0.01637276338583157 --time: 28.06520986557007\n",
      "15 validation:  0.917457805907173 --max 0.917457805907173 --time: 32.086413860321045\n",
      "train acc:  0.995026166688292  train loss:  0.016324609678202612 --time: 28.382258653640747\n",
      "16 validation:  0.9140295358649789 --max 0.917457805907173 --time: 32.42433953285217\n",
      "train acc:  0.9933826391592059  train loss:  0.02121836816421221 --time: 28.197335481643677\n",
      "17 validation:  0.9106012658227848 --max 0.917457805907173 --time: 32.14248752593994\n",
      "train acc:  0.9852515029626746  train loss:  0.04566125675150867 --time: 28.55864453315735\n",
      "18 validation:  0.9000527426160337 --max 0.917457805907173 --time: 32.47894048690796\n",
      "train acc:  0.9751740841659098  train loss:  0.07295968223527814 --time: 28.404329538345337\n",
      "19 validation:  0.9066455696202531 --max 0.917457805907173 --time: 32.254889726638794\n",
      "train acc:  0.9896630768565373  train loss:  0.03298666460557908 --time: 28.31270980834961\n",
      "20 validation:  0.9158755274261603 --max 0.917457805907173 --time: 32.20330309867859\n",
      "train acc:  0.9937718956792526  train loss:  0.019191732346491876 --time: 28.39135479927063\n",
      "21 validation:  0.9127109704641351 --max 0.917457805907173 --time: 32.21261954307556\n",
      "train acc:  0.9979239652264176  train loss:  0.008181278807212707 --time: 28.24015784263611\n",
      "22 validation:  0.9177215189873418 --max 0.9177215189873418 --time: 32.196693658828735\n",
      "train acc:  0.9959344319017344  train loss:  0.011285066346217267 --time: 28.305717945098877\n",
      "23 validation:  0.9140295358649789 --max 0.9177215189873418 --time: 32.33053255081177\n",
      "train acc:  0.9928203797413606  train loss:  0.02266734268307912 --time: 28.3072988986969\n",
      "24 validation:  0.9011075949367089 --max 0.9177215189873418 --time: 32.29114079475403\n",
      "train acc:  0.9868085290428614  train loss:  0.04023420492711976 --time: 28.217042684555054\n",
      "25 validation:  0.8968881856540084 --max 0.9177215189873418 --time: 32.20182657241821\n",
      "train acc:  0.9855110073093725  train loss:  0.04283641882302518 --time: 28.13175654411316\n",
      "26 validation:  0.9013713080168776 --max 0.9177215189873418 --time: 32.12262225151062\n",
      "train acc:  0.990052333376584  train loss:  0.03027611234719294 --time: 28.193755865097046\n",
      "27 validation:  0.9095464135021097 --max 0.9177215189873418 --time: 32.29411792755127\n",
      "train acc:  0.9942909043726482  train loss:  0.019059267311835486 --time: 28.103437185287476\n",
      "28 validation:  0.9129746835443038 --max 0.9177215189873418 --time: 32.286657094955444\n",
      "train acc:  0.995026166688292  train loss:  0.014486497562656499 --time: 28.10662579536438\n",
      "29 validation:  0.9158755274261603 --max 0.9177215189873418 --time: 32.375635385513306\n",
      "train acc:  0.995977682626184  train loss:  0.01311743279111196 --time: 28.00682282447815\n",
      "30 validation:  0.9158755274261603 --max 0.9177215189873418 --time: 32.19479489326477\n",
      "train acc:  0.9974482072574715  train loss:  0.0069782456670288324 --time: 27.89030647277832\n",
      "31 validation:  0.9161392405063291 --max 0.9177215189873418 --time: 32.16172003746033\n",
      "train acc:  0.9977942130530686  train loss:  0.00667647917826341 --time: 27.98641872406006\n",
      "32 validation:  0.9193037974683544 --max 0.9193037974683544 --time: 32.163501262664795\n",
      "train acc:  0.9978807145019679  train loss:  0.00567670127986165 --time: 27.94839096069336\n",
      "33 validation:  0.9195675105485233 --max 0.9195675105485233 --time: 32.16569232940674\n",
      "train acc:  0.9980537173997664  train loss:  0.005302896661566858 --time: 28.063558340072632\n",
      "34 validation:  0.9148206751054853 --max 0.9195675105485233 --time: 32.2247052192688\n",
      "train acc:  0.9983132217464643  train loss:  0.004457916058014657 --time: 27.997987985610962\n",
      "35 validation:  0.9182489451476793 --max 0.9195675105485233 --time: 32.18182635307312\n",
      "train acc:  0.9983564724709139  train loss:  0.004165146509600284 --time: 27.84158945083618\n",
      "36 validation:  0.9190400843881856 --max 0.9195675105485233 --time: 32.10321497917175\n",
      "train acc:  0.9981402188486657  train loss:  0.004032638433112155 --time: 27.9787495136261\n",
      "37 validation:  0.9164029535864979 --max 0.9195675105485233 --time: 32.133948802948\n",
      "train acc:  0.9984862246442628  train loss:  0.0038951903356345967 --time: 28.115389585494995\n",
      "38 validation:  0.917457805907173 --max 0.9195675105485233 --time: 32.39695620536804\n",
      "train acc:  0.9983997231953635  train loss:  0.003760241688362121 --time: 28.20332431793213\n",
      "39 validation:  0.9208860759493671 --max 0.9208860759493671 --time: 32.451744556427\n",
      "train acc:  0.9985294753687124  train loss:  0.003907610124242537 --time: 28.21320080757141\n",
      "40 validation:  0.9190400843881856 --max 0.9208860759493671 --time: 32.47687554359436\n",
      "train acc:  0.9980969681242161  train loss:  0.004096803626272144 --time: 27.979110717773438\n",
      "41 validation:  0.9193037974683544 --max 0.9208860759493671 --time: 32.18598651885986\n",
      "train acc:  0.9983132217464643  train loss:  0.004152169991407616 --time: 28.219170093536377\n",
      "42 validation:  0.9187763713080169 --max 0.9208860759493671 --time: 32.4349479675293\n",
      "train acc:  0.9984429739198132  train loss:  0.0042232569344754995 --time: 27.83270764350891\n",
      "43 validation:  0.9206223628691983 --max 0.9208860759493671 --time: 32.09056901931763\n",
      "train acc:  0.9781151334284849  train loss:  0.06905075774177313 --time: 28.20638418197632\n",
      "44 validation:  0.8792194092827004 --max 0.9208860759493671 --time: 32.39905858039856\n",
      "train acc:  0.942130530686389  train loss:  0.17564966599585602 --time: 28.19450855255127\n",
      "45 validation:  0.9037447257383966 --max 0.9208860759493671 --time: 32.27763223648071\n",
      "train acc:  0.9867220275939622  train loss:  0.042983361998398 --time: 28.16355061531067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 validation:  0.915084388185654 --max 0.9208860759493671 --time: 32.01110649108887\n",
      "train acc:  0.9952424203105402  train loss:  0.015378900736091834 --time: 28.3173885345459\n",
      "47 validation:  0.9193037974683544 --max 0.9208860759493671 --time: 32.06705951690674\n",
      "train acc:  0.9979239652264176  train loss:  0.007004771088327163 --time: 28.63739562034607\n",
      "48 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.20920372009277\n",
      "train acc:  0.9981402188486657  train loss:  0.005019712262981165 --time: 28.707889080047607\n",
      "49 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.328818798065186\n",
      "train acc:  0.998226720297565  train loss:  0.0044003693293329866 --time: 28.789457321166992\n",
      "50 validation:  0.9190400843881856 --max 0.9208860759493671 --time: 32.18169903755188\n",
      "train acc:  0.9984429739198132  train loss:  0.003915457750650673 --time: 28.85030770301819\n",
      "51 validation:  0.919831223628692 --max 0.9208860759493671 --time: 32.35702466964722\n",
      "train acc:  0.998226720297565  train loss:  0.003963989404079026 --time: 28.702873706817627\n",
      "52 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.29301476478577\n",
      "train acc:  0.9980969681242161  train loss:  0.003890903833279134 --time: 28.805441856384277\n",
      "53 validation:  0.9208860759493671 --max 0.9208860759493671 --time: 32.278674364089966\n",
      "train acc:  0.9980537173997664  train loss:  0.004051133192706151 --time: 28.835688591003418\n",
      "54 validation:  0.9187763713080169 --max 0.9208860759493671 --time: 32.47146153450012\n",
      "train acc:  0.9983132217464643  train loss:  0.0038189789071181937 --time: 28.727886199951172\n",
      "55 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.51333141326904\n",
      "train acc:  0.9983132217464643  train loss:  0.0037723297021867584 --time: 28.58012104034424\n",
      "56 validation:  0.9187763713080169 --max 0.9208860759493671 --time: 32.19496536254883\n",
      "train acc:  0.9983997231953635  train loss:  0.003646549147676589 --time: 28.551920175552368\n",
      "57 validation:  0.9187763713080169 --max 0.9208860759493671 --time: 32.33077549934387\n",
      "train acc:  0.9985294753687124  train loss:  0.0036194866890522192 --time: 28.491249561309814\n",
      "58 validation:  0.9185126582278481 --max 0.9208860759493671 --time: 32.42467141151428\n",
      "train acc:  0.9983564724709139  train loss:  0.0035582622984303316 --time: 28.652191162109375\n",
      "59 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.50648212432861\n",
      "train acc:  0.9984429739198132  train loss:  0.0036330742221061933 --time: 28.554757833480835\n",
      "60 validation:  0.9195675105485233 --max 0.9208860759493671 --time: 32.40452527999878\n",
      "train acc:  0.9983564724709139  train loss:  0.0038017031723165174 --time: 28.701560258865356\n",
      "61 validation:  0.9185126582278481 --max 0.9208860759493671 --time: 32.596662521362305\n",
      "train acc:  0.9984429739198132  train loss:  0.0036206556105552413 --time: 28.510794639587402\n",
      "62 validation:  0.9203586497890295 --max 0.9208860759493671 --time: 32.5076789855957\n",
      "train acc:  0.9628043769733143  train loss:  0.10987257551415842 --time: 28.639639139175415\n",
      "63 validation:  0.8958333333333334 --max 0.9208860759493671 --time: 32.58878207206726\n",
      "train acc:  0.9666536914493318  train loss:  0.09887208079517876 --time: 28.29123020172119\n",
      "64 validation:  0.9032172995780591 --max 0.9208860759493671 --time: 32.444406509399414\n",
      "train acc:  0.9881925522252498  train loss:  0.033985450223232336 --time: 28.317375421524048\n",
      "65 validation:  0.9116561181434599 --max 0.9208860759493671 --time: 32.44695258140564\n",
      "train acc:  0.9962804376973314  train loss:  0.011520548951001684 --time: 28.09984040260315\n",
      "66 validation:  0.9132383966244726 --max 0.9208860759493671 --time: 32.233033657073975\n",
      "train acc:  0.9979239652264176  train loss:  0.005766206330269863 --time: 28.183521270751953\n",
      "67 validation:  0.9121835443037974 --max 0.9208860759493671 --time: 32.348193645477295\n",
      "train acc:  0.9980104666753168  train loss:  0.004584594818477864 --time: 28.09461259841919\n",
      "68 validation:  0.9116561181434599 --max 0.9208860759493671 --time: 32.26978635787964\n",
      "train acc:  0.9982699710220146  train loss:  0.0041934387294160075 --time: 28.391856908798218\n",
      "69 validation:  0.9132383966244726 --max 0.9208860759493671 --time: 32.54692316055298\n",
      "train acc:  0.9983132217464643  train loss:  0.0038006623127325363 --time: 28.2184100151062\n",
      "70 validation:  0.9137658227848101 --max 0.9208860759493671 --time: 32.51804971694946\n",
      "train acc:  0.9982699710220146  train loss:  0.0038992068265598185 --time: 28.296937942504883\n",
      "71 validation:  0.9129746835443038 --max 0.9208860759493671 --time: 32.43284583091736\n",
      "train acc:  0.9982699710220146  train loss:  0.0036580829578998515 --time: 28.099696159362793\n",
      "72 validation:  0.9127109704641351 --max 0.9208860759493671 --time: 32.23847508430481\n",
      "train acc:  0.9983564724709139  train loss:  0.003703726188449166 --time: 28.192005157470703\n",
      "73 validation:  0.9129746835443038 --max 0.9208860759493671 --time: 32.32701373100281\n",
      "train acc:  0.9983564724709139  train loss:  0.00376826811769015 --time: 28.29605221748352\n",
      "74 validation:  0.9127109704641351 --max 0.9208860759493671 --time: 32.35694670677185\n",
      "train acc:  0.998572726093162  train loss:  0.003733430229894719 --time: 28.489170789718628\n",
      "75 validation:  0.9106012658227848 --max 0.9208860759493671 --time: 32.47134733200073\n",
      "train acc:  0.998226720297565  train loss:  0.003760514511427397 --time: 28.408932209014893\n",
      "76 validation:  0.9140295358649789 --max 0.9208860759493671 --time: 32.47898840904236\n",
      "train acc:  0.998572726093162  train loss:  0.003494444384114355 --time: 28.404101133346558\n",
      "77 validation:  0.9129746835443038 --max 0.9208860759493671 --time: 32.33709931373596\n",
      "train acc:  0.9985294753687124  train loss:  0.003422200492312598 --time: 28.492862701416016\n",
      "78 validation:  0.9142932489451476 --max 0.9208860759493671 --time: 32.416091203689575\n",
      "train acc:  0.9982699710220146  train loss:  0.0037116457521698364 --time: 28.455657243728638\n",
      "79 validation:  0.9142932489451476 --max 0.9208860759493671 --time: 32.41248559951782\n",
      "train acc:  0.9982699710220146  train loss:  0.003690761293960576 --time: 28.507930517196655\n",
      "80 validation:  0.9156118143459916 --max 0.9208860759493671 --time: 32.19857621192932\n",
      "train acc:  0.9981834695731153  train loss:  0.0036689652149222702 --time: 28.44223165512085\n",
      "81 validation:  0.9145569620253164 --max 0.9208860759493671 --time: 32.275744676589966\n",
      "train acc:  0.9983564724709139  train loss:  0.0035789913156337376 --time: 28.458807945251465\n",
      "82 validation:  0.9156118143459916 --max 0.9208860759493671 --time: 32.19855260848999\n",
      "train acc:  0.9981834695731153  train loss:  0.003735956395778066 --time: 28.571919441223145\n",
      "83 validation:  0.9148206751054853 --max 0.9208860759493671 --time: 32.3597252368927\n",
      "train acc:  0.9984429739198132  train loss:  0.003332181477142422 --time: 28.668159008026123\n",
      "84 validation:  0.9142932489451476 --max 0.9208860759493671 --time: 32.5900821685791\n",
      "train acc:  0.9983132217464643  train loss:  0.003573676454352256 --time: 28.551687240600586\n",
      "85 validation:  0.9153481012658228 --max 0.9208860759493671 --time: 32.4272255897522\n",
      "train acc:  0.9984862246442628  train loss:  0.0035319088903573814 --time: 28.39835286140442\n",
      "86 validation:  0.9158755274261603 --max 0.9208860759493671 --time: 32.322853565216064\n",
      "train acc:  0.9984429739198132  train loss:  0.0033835844820533662 --time: 28.426629304885864\n",
      "87 validation:  0.9156118143459916 --max 0.9208860759493671 --time: 32.49321699142456\n",
      "train acc:  0.9984429739198132  train loss:  0.0033066591662625705 --time: 28.324137926101685\n",
      "88 validation:  0.9156118143459916 --max 0.9208860759493671 --time: 32.35218358039856\n",
      "train acc:  0.9983997231953635  train loss:  0.0033594048844313683 --time: 28.468956232070923\n",
      "89 validation:  0.9137658227848101 --max 0.9208860759493671 --time: 32.562949657440186\n",
      "train acc:  0.9983997231953635  train loss:  0.003515279088451129 --time: 28.27199673652649\n",
      "90 validation:  0.9145569620253164 --max 0.9208860759493671 --time: 32.26823878288269\n",
      "train acc:  0.9983132217464643  train loss:  0.003637159591296206 --time: 28.342814683914185\n",
      "91 validation:  0.915084388185654 --max 0.9208860759493671 --time: 32.3703887462616\n",
      "train acc:  0.9731845508412266  train loss:  0.08631562033920769 --time: 28.32877016067505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 validation:  0.8866033755274262 --max 0.9208860759493671 --time: 32.344133377075195\n",
      "train acc:  0.9561870161325202  train loss:  0.12687620703768993 --time: 28.548574686050415\n",
      "93 validation:  0.9106012658227848 --max 0.9208860759493671 --time: 32.43907976150513\n",
      "train acc:  0.9893170710609402  train loss:  0.03333406122041177 --time: 28.621865272521973\n",
      "94 validation:  0.9177215189873418 --max 0.9208860759493671 --time: 32.485384464263916\n",
      "train acc:  0.99645344059513  train loss:  0.010622068449149927 --time: 28.669033765792847\n",
      "95 validation:  0.9158755274261603 --max 0.9208860759493671 --time: 32.480459451675415\n",
      "train acc:  0.9978807145019679  train loss:  0.005914544758711072 --time: 28.44426965713501\n",
      "96 validation:  0.9206223628691983 --max 0.9208860759493671 --time: 32.37909150123596\n",
      "train acc:  0.9983132217464643  train loss:  0.004267751572165478 --time: 28.37317681312561\n",
      "97 validation:  0.919831223628692 --max 0.9208860759493671 --time: 32.36394667625427\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 lstm layer, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
