{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "        embeddings = input##saved for attention\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "        \n",
    "        #doing attention\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        batch_size = hn[-1].shape[0]\n",
    "        emb_size = embeddings.shape[1]\n",
    "        seq_len = embeddings.shape[2]\n",
    "\n",
    "        attention = torch.zeros([batch_size, seq_len]).to(device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            attention[:,t] = torch.sigmoid(cos(embeddings[:,:,t], hn[-1]))\n",
    "            \n",
    "        normalize = torch.sum(attention, dim=1).reshape(-1,1)\n",
    "        attention = attention/normalize\n",
    "\n",
    "        \n",
    "        output = torch.zeros([batch_size, emb_size]).to(device)\n",
    "        for t in range(seq_len):\n",
    "            weights = attention[:,t].reshape(-1,1)\n",
    "            output += weights*embeddings[:,:,t]\n",
    "        \n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.08290859667006456  train loss:  0.619538356428561 --time: 19.5309841632843\n",
      "validation:  0.16333333333333333 --max 0.16333333333333333 --time: 20.676095247268677\n",
      "train acc:  0.13761467889908258  train loss:  0.4687999383262966 --time: 20.087887287139893\n",
      "validation:  0.19666666666666666 --max 0.19666666666666666 --time: 21.170298099517822\n",
      "train acc:  0.2820251444104655  train loss:  0.4478850921858912 --time: 19.78876829147339\n",
      "validation:  0.19666666666666666 --max 0.19666666666666666 --time: 21.043764352798462\n",
      "train acc:  0.25925925925925924  train loss:  0.44488190697587054 --time: 19.73092484474182\n",
      "validation:  0.19666666666666666 --max 0.19666666666666666 --time: 20.75589609146118\n",
      "train acc:  0.3143051308188923  train loss:  0.44261407463446906 --time: 19.584691047668457\n",
      "validation:  0.19666666666666666 --max 0.19666666666666666 --time: 20.744331121444702\n",
      "train acc:  0.30479102956167176  train loss:  0.44042489710061444 --time: 20.39794087409973\n",
      "validation:  0.21333333333333335 --max 0.21333333333333335 --time: 21.46125864982605\n",
      "train acc:  0.30750934420659193  train loss:  0.43955299387807434 --time: 18.02253794670105\n",
      "validation:  0.20333333333333334 --max 0.21333333333333335 --time: 19.14379906654358\n",
      "train acc:  0.33197417601087326  train loss:  0.43678224345912103 --time: 19.571706295013428\n",
      "validation:  0.21 --max 0.21333333333333335 --time: 20.771382570266724\n",
      "train acc:  0.3302752293577982  train loss:  0.4341121989747752 --time: 18.90097403526306\n",
      "validation:  0.22333333333333333 --max 0.22333333333333333 --time: 20.083945989608765\n",
      "train acc:  0.34624532789670404  train loss:  0.42941035265507904 --time: 20.30695343017578\n",
      "validation:  0.23666666666666666 --max 0.23666666666666666 --time: 21.399765491485596\n",
      "train acc:  0.34964322120285424  train loss:  0.4264181334039439 --time: 19.123092651367188\n",
      "validation:  0.26666666666666666 --max 0.26666666666666666 --time: 20.13434147834778\n",
      "train acc:  0.36289500509684  train loss:  0.4226498111434605 --time: 19.437965869903564\n",
      "validation:  0.2633333333333333 --max 0.26666666666666666 --time: 20.456936836242676\n",
      "train acc:  0.37003058103975534  train loss:  0.41910940408706665 --time: 19.55370831489563\n",
      "validation:  0.25 --max 0.26666666666666666 --time: 20.578367471694946\n",
      "train acc:  0.36493374108053006  train loss:  0.4157907016899275 --time: 20.501892566680908\n",
      "validation:  0.25 --max 0.26666666666666666 --time: 21.612104177474976\n",
      "train acc:  0.3577981651376147  train loss:  0.41446344878362573 --time: 18.923406839370728\n",
      "validation:  0.25666666666666665 --max 0.26666666666666666 --time: 20.060749530792236\n",
      "train acc:  0.37037037037037035  train loss:  0.41147435359332873 --time: 19.335121870040894\n",
      "validation:  0.26 --max 0.26666666666666666 --time: 20.544464826583862\n",
      "train acc:  0.3758069996602107  train loss:  0.4096699382947839 --time: 20.641584873199463\n",
      "validation:  0.25333333333333335 --max 0.26666666666666666 --time: 21.64412498474121\n",
      "train acc:  0.37988447162759087  train loss:  0.4060694342074187 --time: 19.554616928100586\n",
      "validation:  0.25333333333333335 --max 0.26666666666666666 --time: 20.255388498306274\n",
      "train acc:  0.381243628950051  train loss:  0.4064275648282922 --time: 18.335147619247437\n",
      "validation:  0.2633333333333333 --max 0.26666666666666666 --time: 19.369018077850342\n",
      "train acc:  0.3893985728848114  train loss:  0.40119967901188397 --time: 17.609290838241577\n",
      "validation:  0.2633333333333333 --max 0.26666666666666666 --time: 18.46065330505371\n",
      "train acc:  0.39211688752973156  train loss:  0.39994623220485187 --time: 17.07331156730652\n",
      "validation:  0.28 --max 0.28 --time: 18.06501817703247\n",
      "train acc:  0.39789330615018687  train loss:  0.3973119155220363 --time: 17.238905668258667\n",
      "validation:  0.27 --max 0.28 --time: 18.183154106140137\n",
      "train acc:  0.399592252803262  train loss:  0.39606610329254816 --time: 17.141437768936157\n",
      "validation:  0.2733333333333333 --max 0.28 --time: 17.87739324569702\n",
      "train acc:  0.40944614339109753  train loss:  0.39125025272369385 --time: 17.023972749710083\n",
      "validation:  0.28 --max 0.28 --time: 17.76587724685669\n",
      "train acc:  0.42269792728508326  train loss:  0.3880805360234302 --time: 16.584532260894775\n",
      "validation:  0.2733333333333333 --max 0.28 --time: 17.421724796295166\n",
      "train acc:  0.4165817193340129  train loss:  0.3864623269309168 --time: 16.732705116271973\n",
      "validation:  0.2833333333333333 --max 0.2833333333333333 --time: 17.45471453666687\n",
      "train acc:  0.4257560312606184  train loss:  0.3849959477134373 --time: 17.038459300994873\n",
      "validation:  0.29 --max 0.29 --time: 17.936192512512207\n",
      "train acc:  0.4288141352361536  train loss:  0.3816762994165006 --time: 16.01844310760498\n",
      "validation:  0.29333333333333333 --max 0.29333333333333333 --time: 17.02173113822937\n",
      "train acc:  0.4345905538566089  train loss:  0.3762909497903741 --time: 17.90556502342224\n",
      "validation:  0.30666666666666664 --max 0.30666666666666664 --time: 18.7041277885437\n",
      "train acc:  0.4478423377505946  train loss:  0.3750027806862541 --time: 14.64433479309082\n",
      "validation:  0.31 --max 0.31 --time: 15.39993405342102\n",
      "train acc:  0.4475025484199796  train loss:  0.3727756025998489 --time: 15.204349279403687\n",
      "validation:  0.31 --max 0.31 --time: 15.8455650806427\n",
      "train acc:  0.4447842337750595  train loss:  0.37179180591002753 --time: 15.854961395263672\n",
      "validation:  0.30666666666666664 --max 0.31 --time: 16.60767936706543\n",
      "train acc:  0.45531770302412505  train loss:  0.37041484532148944 --time: 16.14672064781189\n",
      "validation:  0.32 --max 0.32 --time: 16.77293300628662\n",
      "train acc:  0.4655113829425756  train loss:  0.36398964731589606 --time: 16.51000213623047\n",
      "validation:  0.32666666666666666 --max 0.32666666666666666 --time: 17.450064659118652\n",
      "train acc:  0.46075433231396534  train loss:  0.36436391006345337 --time: 15.317684412002563\n",
      "validation:  0.33666666666666667 --max 0.33666666666666667 --time: 16.219258069992065\n",
      "train acc:  0.48148148148148145  train loss:  0.3597542241863582 --time: 15.727057933807373\n",
      "validation:  0.34 --max 0.34 --time: 16.638798713684082\n",
      "train acc:  0.47706422018348627  train loss:  0.3569463452567225 --time: 15.47022557258606\n",
      "validation:  0.33666666666666667 --max 0.34 --time: 16.29276990890503\n",
      "train acc:  0.4954128440366973  train loss:  0.3569524016069329 --time: 15.246485710144043\n",
      "validation:  0.33 --max 0.34 --time: 16.07302975654602\n",
      "train acc:  0.48148148148148145  train loss:  0.3539186249608579 --time: 15.144606351852417\n",
      "validation:  0.3333333333333333 --max 0.34 --time: 16.045557975769043\n",
      "train acc:  0.4892966360856269  train loss:  0.35395133106604865 --time: 15.598690509796143\n",
      "validation:  0.33 --max 0.34 --time: 16.370993852615356\n",
      "train acc:  0.5045871559633027  train loss:  0.34831088133480237 --time: 14.739118099212646\n",
      "validation:  0.3466666666666667 --max 0.3466666666666667 --time: 15.352007150650024\n",
      "train acc:  0.5032279986408427  train loss:  0.3479824221652487 --time: 15.559895277023315\n",
      "validation:  0.3566666666666667 --max 0.3566666666666667 --time: 16.286461353302002\n",
      "train acc:  0.5215766224940537  train loss:  0.34510707207348035 --time: 15.52498173713684\n",
      "validation:  0.3466666666666667 --max 0.3566666666666667 --time: 16.371575117111206\n",
      "train acc:  0.5110431532449881  train loss:  0.3410044182901797 --time: 15.200757503509521\n",
      "validation:  0.3433333333333333 --max 0.3566666666666667 --time: 16.097442150115967\n",
      "train acc:  0.528712198436969  train loss:  0.3394220937853274 --time: 15.962130784988403\n",
      "validation:  0.36333333333333334 --max 0.36333333333333334 --time: 16.694889307022095\n",
      "train acc:  0.526333673122664  train loss:  0.33473663744719134 --time: 15.906116247177124\n",
      "validation:  0.36666666666666664 --max 0.36666666666666664 --time: 16.70700478553772\n",
      "train acc:  0.5406048250084947  train loss:  0.3325612143329952 --time: 15.530669450759888\n",
      "validation:  0.36666666666666664 --max 0.36666666666666664 --time: 16.38273596763611\n",
      "train acc:  0.5389058783554196  train loss:  0.33083784191504767 --time: 15.123627662658691\n",
      "validation:  0.36666666666666664 --max 0.36666666666666664 --time: 15.900757074356079\n",
      "train acc:  0.544682296975875  train loss:  0.3287405747434367 --time: 15.411169290542603\n",
      "validation:  0.37666666666666665 --max 0.37666666666666665 --time: 16.071263790130615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.5643900781515461  train loss:  0.32813456913699274 --time: 15.098710060119629\n",
      "validation:  0.3566666666666667 --max 0.37666666666666665 --time: 16.083961248397827\n",
      "train acc:  0.5501189262657152  train loss:  0.32337859661682794 --time: 15.04345440864563\n",
      "validation:  0.38333333333333336 --max 0.38333333333333336 --time: 15.649665117263794\n",
      "train acc:  0.5783214407067618  train loss:  0.3223648745080699 --time: 15.088891506195068\n",
      "validation:  0.37333333333333335 --max 0.38333333333333336 --time: 15.88669729232788\n",
      "train acc:  0.564050288820931  train loss:  0.31905592654062354 --time: 15.790424108505249\n",
      "validation:  0.42 --max 0.42 --time: 16.588308334350586\n",
      "train acc:  0.5881753312945973  train loss:  0.31488123795260553 --time: 15.631365537643433\n",
      "validation:  0.36 --max 0.42 --time: 16.52324104309082\n",
      "train acc:  0.583078491335372  train loss:  0.3130739098009856 --time: 15.907176494598389\n",
      "validation:  0.4 --max 0.42 --time: 16.77975058555603\n",
      "train acc:  0.5905538566089025  train loss:  0.30933464480483014 --time: 16.143498420715332\n",
      "validation:  0.41 --max 0.42 --time: 16.878609657287598\n",
      "train acc:  0.6004077471967381  train loss:  0.3073660627655361 --time: 14.42391848564148\n",
      "validation:  0.38333333333333336 --max 0.42 --time: 15.182659149169922\n",
      "train acc:  0.5919130139313625  train loss:  0.30645095654155896 --time: 15.167866706848145\n",
      "validation:  0.4266666666666667 --max 0.4266666666666667 --time: 15.96088171005249\n",
      "train acc:  0.6106014271151886  train loss:  0.3044248612030693 --time: 15.79038691520691\n",
      "validation:  0.42 --max 0.4266666666666667 --time: 16.611533164978027\n",
      "train acc:  0.5983690112130479  train loss:  0.30234282042669214 --time: 15.590183973312378\n",
      "validation:  0.43666666666666665 --max 0.43666666666666665 --time: 16.04835271835327\n",
      "train acc:  0.6292898402990146  train loss:  0.2962611654530401 --time: 16.272586584091187\n",
      "validation:  0.42 --max 0.43666666666666665 --time: 16.725849151611328\n",
      "train acc:  0.6119605844376487  train loss:  0.2947717671808989 --time: 15.796152830123901\n",
      "validation:  0.43333333333333335 --max 0.43666666666666665 --time: 16.28108811378479\n",
      "train acc:  0.6333673122663949  train loss:  0.29048047895016876 --time: 15.404387950897217\n",
      "validation:  0.4266666666666667 --max 0.43666666666666665 --time: 16.073850393295288\n",
      "train acc:  0.6235134216785593  train loss:  0.2900853999283003 --time: 14.210737228393555\n",
      "validation:  0.46 --max 0.46 --time: 14.685347557067871\n",
      "train acc:  0.6435609921848454  train loss:  0.28938034954278363 --time: 15.0727698802948\n",
      "validation:  0.44666666666666666 --max 0.46 --time: 15.895691871643066\n",
      "train acc:  0.6207951070336392  train loss:  0.2861520013083582 --time: 15.971488237380981\n",
      "validation:  0.44 --max 0.46 --time: 16.952235221862793\n",
      "train acc:  0.6615698267074414  train loss:  0.2843357052492059 --time: 17.98192524909973\n",
      "validation:  0.43666666666666665 --max 0.46 --time: 18.609564304351807\n",
      "train acc:  0.6401630988786952  train loss:  0.2826843209888624 --time: 18.584195852279663\n",
      "validation:  0.44333333333333336 --max 0.46 --time: 19.179916381835938\n",
      "train acc:  0.6663268773360517  train loss:  0.27895765330480493 --time: 18.11665940284729\n",
      "validation:  0.44 --max 0.46 --time: 18.6570827960968\n",
      "train acc:  0.6615698267074414  train loss:  0.2796936229519222 --time: 17.601746559143066\n",
      "validation:  0.48333333333333334 --max 0.48333333333333334 --time: 18.434093952178955\n",
      "train acc:  0.6534148827726809  train loss:  0.27787087922510895 --time: 16.58336091041565\n",
      "validation:  0.4633333333333333 --max 0.48333333333333334 --time: 17.58560347557068\n",
      "train acc:  0.6822969758749575  train loss:  0.2731852019610612 --time: 17.65526819229126\n",
      "validation:  0.5066666666666667 --max 0.5066666666666667 --time: 18.515157461166382\n",
      "train acc:  0.6836561331974176  train loss:  0.2709724527338277 --time: 16.480304718017578\n",
      "validation:  0.4633333333333333 --max 0.5066666666666667 --time: 17.444463968276978\n",
      "train acc:  0.6782195039075773  train loss:  0.26868827317071997 --time: 17.107249975204468\n",
      "validation:  0.49333333333333335 --max 0.5066666666666667 --time: 18.250074863433838\n",
      "train acc:  0.7016649677200136  train loss:  0.26487858917402185 --time: 16.193854570388794\n",
      "validation:  0.4766666666666667 --max 0.5066666666666667 --time: 17.07056736946106\n",
      "train acc:  0.6999660210669385  train loss:  0.2598810506903607 --time: 16.55205774307251\n",
      "validation:  0.5 --max 0.5066666666666667 --time: 17.52023720741272\n",
      "train acc:  0.7196738022426096  train loss:  0.2572526685569597 --time: 16.71936535835266\n",
      "validation:  0.52 --max 0.52 --time: 17.920670747756958\n",
      "train acc:  0.7339449541284404  train loss:  0.2528293476156566 --time: 17.063071489334106\n",
      "validation:  0.51 --max 0.52 --time: 18.068867444992065\n",
      "train acc:  0.7390417940876657  train loss:  0.2512597972932069 --time: 16.921640634536743\n",
      "validation:  0.51 --max 0.52 --time: 18.049153804779053\n",
      "train acc:  0.7468569486918111  train loss:  0.2455681141303933 --time: 17.394335508346558\n",
      "validation:  0.5133333333333333 --max 0.52 --time: 18.402209043502808\n",
      "train acc:  0.7526333673122664  train loss:  0.24313167515008346 --time: 16.63110613822937\n",
      "validation:  0.53 --max 0.53 --time: 17.606878995895386\n",
      "train acc:  0.7652055725450221  train loss:  0.24121300357839334 --time: 17.488649129867554\n",
      "validation:  0.5433333333333333 --max 0.5433333333333333 --time: 18.32525134086609\n",
      "train acc:  0.7611281005776419  train loss:  0.23845411059649094 --time: 16.783794403076172\n",
      "validation:  0.5366666666666666 --max 0.5433333333333333 --time: 17.913230657577515\n",
      "train acc:  0.7567108392796467  train loss:  0.23847516917664072 --time: 17.227779626846313\n",
      "validation:  0.5466666666666666 --max 0.5466666666666666 --time: 18.192692041397095\n",
      "train acc:  0.7563710499490316  train loss:  0.24263452252616052 --time: 17.351323127746582\n",
      "validation:  0.5333333333333333 --max 0.5466666666666666 --time: 18.25620460510254\n",
      "train acc:  0.7577302072714917  train loss:  0.23747061516927637 --time: 17.15092444419861\n",
      "validation:  0.5533333333333333 --max 0.5533333333333333 --time: 18.165513277053833\n",
      "train acc:  0.7743798844716276  train loss:  0.23040842621222787 --time: 17.322208881378174\n",
      "validation:  0.5533333333333333 --max 0.5533333333333333 --time: 18.326164484024048\n",
      "train acc:  0.783214407067618  train loss:  0.22780019304026727 --time: 17.123302698135376\n",
      "validation:  0.56 --max 0.56 --time: 18.126795530319214\n",
      "train acc:  0.781855249745158  train loss:  0.22348196156646893 --time: 16.35059881210327\n",
      "validation:  0.5666666666666667 --max 0.5666666666666667 --time: 17.079453706741333\n",
      "train acc:  0.7852531430513082  train loss:  0.22340336830719656 --time: 17.32395339012146\n",
      "validation:  0.5866666666666667 --max 0.5866666666666667 --time: 18.205974340438843\n",
      "train acc:  0.7964661909616038  train loss:  0.2177022838074228 --time: 16.150904178619385\n",
      "validation:  0.56 --max 0.5866666666666667 --time: 17.070820569992065\n",
      "train acc:  0.7896704043493035  train loss:  0.21676005617431973 --time: 17.289772033691406\n",
      "validation:  0.57 --max 0.5866666666666667 --time: 18.31566619873047\n",
      "train acc:  0.7937478763166836  train loss:  0.21340722169565118 --time: 17.485381603240967\n",
      "validation:  0.57 --max 0.5866666666666667 --time: 18.4703426361084\n",
      "train acc:  0.8042813455657493  train loss:  0.21039282627727673 --time: 17.090952157974243\n",
      "validation:  0.58 --max 0.5866666666666667 --time: 18.003882884979248\n",
      "train acc:  0.8042813455657493  train loss:  0.21041704001634018 --time: 17.564749717712402\n",
      "validation:  0.5833333333333334 --max 0.5866666666666667 --time: 18.566204071044922\n",
      "train acc:  0.8083588175331294  train loss:  0.20553335093933603 --time: 16.86623501777649\n",
      "validation:  0.5833333333333334 --max 0.5866666666666667 --time: 17.65373682975769\n",
      "train acc:  0.8107373428474346  train loss:  0.20293207855328269 --time: 16.62896203994751\n",
      "validation:  0.5833333333333334 --max 0.5866666666666667 --time: 17.5047504901886\n",
      "train acc:  0.8148148148148148  train loss:  0.1994281203850456 --time: 17.206504106521606\n",
      "validation:  0.5933333333333334 --max 0.5933333333333334 --time: 18.121466398239136\n",
      "train acc:  0.8107373428474346  train loss:  0.19995833674202795 --time: 16.408924341201782\n",
      "validation:  0.5933333333333334 --max 0.5933333333333334 --time: 17.386877298355103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8154943934760448  train loss:  0.1967972218990326 --time: 16.758275985717773\n",
      "validation:  0.5866666666666667 --max 0.5933333333333334 --time: 17.58957815170288\n",
      "train acc:  0.8134556574923547  train loss:  0.1968961792147678 --time: 17.717207431793213\n",
      "validation:  0.5933333333333334 --max 0.5933333333333334 --time: 18.656149864196777\n",
      "train acc:  0.8120965001698947  train loss:  0.19456509403560474 --time: 16.842267751693726\n",
      "validation:  0.5833333333333334 --max 0.5933333333333334 --time: 17.67947769165039\n",
      "train acc:  0.8158341828066599  train loss:  0.19142293735690738 --time: 17.4012668132782\n",
      "validation:  0.6033333333333334 --max 0.6033333333333334 --time: 18.35224151611328\n",
      "train acc:  0.8233095480801903  train loss:  0.18947659238525058 --time: 17.171788454055786\n",
      "validation:  0.59 --max 0.6033333333333334 --time: 17.70242667198181\n",
      "train acc:  0.8154943934760448  train loss:  0.1888878786045572 --time: 17.660732984542847\n",
      "validation:  0.6133333333333333 --max 0.6133333333333333 --time: 18.356923580169678\n",
      "train acc:  0.8131158681617398  train loss:  0.1935382200324017 --time: 17.730417013168335\n",
      "validation:  0.59 --max 0.6133333333333333 --time: 18.33870244026184\n",
      "train acc:  0.8100577641862046  train loss:  0.1935830550349277 --time: 17.192201137542725\n",
      "validation:  0.5866666666666667 --max 0.6133333333333333 --time: 17.78278875350952\n",
      "train acc:  0.8114169215086646  train loss:  0.19114109096319779 --time: 18.243658542633057\n",
      "validation:  0.6366666666666667 --max 0.6366666666666667 --time: 18.84304118156433\n",
      "train acc:  0.8219503907577302  train loss:  0.18706454012704932 --time: 18.062129259109497\n",
      "validation:  0.5966666666666667 --max 0.6366666666666667 --time: 18.708121299743652\n",
      "train acc:  0.81923207611281  train loss:  0.1865113228559494 --time: 17.619091749191284\n",
      "validation:  0.6 --max 0.6366666666666667 --time: 18.282921075820923\n",
      "train acc:  0.835202174651716  train loss:  0.18065307580906412 --time: 18.07148551940918\n",
      "validation:  0.6233333333333333 --max 0.6366666666666667 --time: 18.675763845443726\n",
      "train acc:  0.8389398572884812  train loss:  0.17630580521148184 --time: 17.704375505447388\n",
      "validation:  0.6033333333333334 --max 0.6366666666666667 --time: 18.36187505722046\n",
      "train acc:  0.836901121304791  train loss:  0.17496909784234088 --time: 17.77524423599243\n",
      "validation:  0.6166666666666667 --max 0.6366666666666667 --time: 18.48183798789978\n",
      "train acc:  0.8389398572884812  train loss:  0.1735196946107823 --time: 17.72092342376709\n",
      "validation:  0.6066666666666667 --max 0.6366666666666667 --time: 18.338579177856445\n",
      "train acc:  0.8409785932721713  train loss:  0.17135776255441748 --time: 18.05292010307312\n",
      "validation:  0.63 --max 0.6366666666666667 --time: 18.672338008880615\n",
      "train acc:  0.8423377505946313  train loss:  0.1677144461351892 --time: 17.563079357147217\n",
      "validation:  0.62 --max 0.6366666666666667 --time: 18.086573839187622\n",
      "train acc:  0.8430173292558614  train loss:  0.16763110070124917 --time: 16.01171851158142\n",
      "validation:  0.6366666666666667 --max 0.6366666666666667 --time: 16.50404667854309\n",
      "train acc:  0.8447162759089365  train loss:  0.1665126603582631 --time: 11.49432110786438\n",
      "validation:  0.65 --max 0.65 --time: 12.10490608215332\n",
      "train acc:  0.8525314305130819  train loss:  0.16455311360566513 --time: 11.443080186843872\n",
      "validation:  0.6466666666666666 --max 0.65 --time: 12.057685613632202\n",
      "train acc:  0.8484539585457017  train loss:  0.16290955180707184 --time: 15.157214879989624\n",
      "validation:  0.6233333333333333 --max 0.65 --time: 16.044899702072144\n",
      "train acc:  0.8498131158681618  train loss:  0.1591256610725237 --time: 16.56821036338806\n",
      "validation:  0.64 --max 0.65 --time: 17.40451145172119\n",
      "train acc:  0.855249745158002  train loss:  0.15912255709585937 --time: 17.283605098724365\n",
      "validation:  0.6366666666666667 --max 0.65 --time: 18.13808584213257\n",
      "train acc:  0.8467550118926266  train loss:  0.16002868698990863 --time: 17.987044095993042\n",
      "validation:  0.6433333333333333 --max 0.65 --time: 18.905383825302124\n",
      "train acc:  0.8579680598029222  train loss:  0.15673799683218417 --time: 16.85854697227478\n",
      "validation:  0.6433333333333333 --max 0.65 --time: 17.65540647506714\n",
      "train acc:  0.8504926945293918  train loss:  0.15768145996591318 --time: 17.469538688659668\n",
      "validation:  0.64 --max 0.65 --time: 18.287813186645508\n",
      "train acc:  0.8644240570846076  train loss:  0.1570437569981036 --time: 17.749675035476685\n",
      "validation:  0.6333333333333333 --max 0.65 --time: 18.863905906677246\n",
      "train acc:  0.8521916411824668  train loss:  0.15438209636055905 --time: 18.02426528930664\n",
      "validation:  0.65 --max 0.65 --time: 19.034248113632202\n",
      "train acc:  0.8627251104315324  train loss:  0.15053639204605765 --time: 16.724093914031982\n",
      "validation:  0.63 --max 0.65 --time: 17.848294734954834\n",
      "train acc:  0.8620455317703024  train loss:  0.15076858712279279 --time: 17.49117636680603\n",
      "validation:  0.6433333333333333 --max 0.65 --time: 18.424459218978882\n",
      "train acc:  0.8661230037376826  train loss:  0.14896824826364932 --time: 16.918444395065308\n",
      "validation:  0.6466666666666666 --max 0.65 --time: 17.832340955734253\n",
      "train acc:  0.8596670064559973  train loss:  0.14853816479444504 --time: 17.180216073989868\n",
      "validation:  0.6233333333333333 --max 0.65 --time: 18.18881130218506\n",
      "train acc:  0.8158341828066599  train loss:  0.18067217974559122 --time: 17.279804229736328\n",
      "validation:  0.5933333333333334 --max 0.65 --time: 18.217130422592163\n",
      "train acc:  0.7743798844716276  train loss:  0.2053913467604181 --time: 16.83684539794922\n",
      "validation:  0.63 --max 0.65 --time: 17.669931411743164\n",
      "train acc:  0.800883452259599  train loss:  0.1849636169879333 --time: 16.968202829360962\n",
      "validation:  0.67 --max 0.67 --time: 17.938711881637573\n",
      "train acc:  0.8311247026843357  train loss:  0.17210742312928903 --time: 17.87294054031372\n",
      "validation:  0.6566666666666666 --max 0.67 --time: 18.819852828979492\n",
      "train acc:  0.8396194359497112  train loss:  0.16266067909157794 --time: 16.374340295791626\n",
      "validation:  0.6733333333333333 --max 0.6733333333333333 --time: 17.41969656944275\n",
      "train acc:  0.8464152225620115  train loss:  0.15433601516744364 --time: 16.605762004852295\n",
      "validation:  0.6833333333333333 --max 0.6833333333333333 --time: 17.41481900215149\n",
      "train acc:  0.8634046890927625  train loss:  0.14946009477843408 --time: 17.549289226531982\n",
      "validation:  0.6833333333333333 --max 0.6833333333333333 --time: 18.807374477386475\n",
      "train acc:  0.8654434250764526  train loss:  0.14555841133646344 --time: 17.1934654712677\n",
      "validation:  0.68 --max 0.6833333333333333 --time: 18.406407594680786\n",
      "train acc:  0.8702004757050629  train loss:  0.1443187996097233 --time: 17.190210342407227\n",
      "validation:  0.6766666666666666 --max 0.6833333333333333 --time: 18.24474024772644\n",
      "train acc:  0.872239211688753  train loss:  0.14121197585178458 --time: 16.30379581451416\n",
      "validation:  0.67 --max 0.6833333333333333 --time: 17.464576482772827\n",
      "train acc:  0.872239211688753  train loss:  0.14056405165921088 --time: 17.323421478271484\n",
      "validation:  0.6766666666666666 --max 0.6833333333333333 --time: 18.331087112426758\n",
      "train acc:  0.872579001019368  train loss:  0.13904429291901382 --time: 17.110782146453857\n",
      "validation:  0.6766666666666666 --max 0.6833333333333333 --time: 18.181220769882202\n",
      "train acc:  0.8776758409785933  train loss:  0.13718693023142609 --time: 18.19223189353943\n",
      "validation:  0.6833333333333333 --max 0.6833333333333333 --time: 19.11483645439148\n",
      "train acc:  0.8769962623173633  train loss:  0.1350419693019079 --time: 16.98671317100525\n",
      "validation:  0.6933333333333334 --max 0.6933333333333334 --time: 17.9889235496521\n",
      "train acc:  0.8841318382602786  train loss:  0.1322843788758568 --time: 16.570289611816406\n",
      "validation:  0.6933333333333334 --max 0.6933333333333334 --time: 17.41606831550598\n",
      "train acc:  0.8797145769622834  train loss:  0.13171279430389404 --time: 16.61731505393982\n",
      "validation:  0.6966666666666667 --max 0.6966666666666667 --time: 17.537089347839355\n",
      "train acc:  0.8871899422358138  train loss:  0.12782668743444525 --time: 16.070934295654297\n",
      "validation:  0.69 --max 0.6966666666666667 --time: 17.049543619155884\n",
      "train acc:  0.8793747876316683  train loss:  0.13046728819608688 --time: 17.033690214157104\n",
      "validation:  0.71 --max 0.71 --time: 17.937971115112305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8953448861705743  train loss:  0.12852308445650598 --time: 17.068642616271973\n",
      "validation:  0.6966666666666667 --max 0.71 --time: 18.040283203125\n",
      "train acc:  0.8831124702684335  train loss:  0.12670135951560477 --time: 16.522379398345947\n",
      "validation:  0.6933333333333334 --max 0.71 --time: 17.596497774124146\n",
      "train acc:  0.890587835541964  train loss:  0.13010820260514383 --time: 18.197951316833496\n",
      "validation:  0.69 --max 0.71 --time: 19.09064483642578\n",
      "train acc:  0.8865103635745838  train loss:  0.12552511562471805 --time: 16.810335397720337\n",
      "validation:  0.7 --max 0.71 --time: 17.6221182346344\n",
      "train acc:  0.891267414203194  train loss:  0.12562847169845 --time: 17.18888783454895\n",
      "validation:  0.7066666666666667 --max 0.71 --time: 17.985478162765503\n",
      "train acc:  0.8970438328236493  train loss:  0.12419332045575847 --time: 17.347966194152832\n",
      "validation:  0.7066666666666667 --max 0.71 --time: 18.31604290008545\n",
      "train acc:  0.889568467550119  train loss:  0.12252492554809737 --time: 17.019399642944336\n",
      "validation:  0.71 --max 0.71 --time: 17.822004556655884\n",
      "train acc:  0.8916072035338091  train loss:  0.12043549444364465 --time: 16.403347730636597\n",
      "validation:  0.7033333333333334 --max 0.71 --time: 17.356983184814453\n",
      "train acc:  0.9055385660890248  train loss:  0.1210497630679089 --time: 17.18971347808838\n",
      "validation:  0.7033333333333334 --max 0.71 --time: 17.86158585548401\n",
      "train acc:  0.8967040434930343  train loss:  0.11986673202203668 --time: 17.668097257614136\n",
      "validation:  0.6966666666666667 --max 0.71 --time: 18.333258867263794\n",
      "train acc:  0.9028202514441046  train loss:  0.12009169290895047 --time: 18.015702724456787\n",
      "validation:  0.7033333333333334 --max 0.71 --time: 19.033583879470825\n",
      "train acc:  0.9004417261297996  train loss:  0.1200017767108005 --time: 18.28079342842102\n",
      "validation:  0.71 --max 0.71 --time: 18.982085466384888\n",
      "train acc:  0.9051987767584098  train loss:  0.11860301183617633 --time: 18.365352869033813\n",
      "validation:  0.7133333333333334 --max 0.7133333333333334 --time: 19.39566707611084\n",
      "train acc:  0.9034998301053347  train loss:  0.11537612002828847 --time: 16.157318592071533\n",
      "validation:  0.71 --max 0.7133333333333334 --time: 17.01707649230957\n",
      "train acc:  0.90791709140333  train loss:  0.11537350714206696 --time: 17.281940460205078\n",
      "validation:  0.71 --max 0.7133333333333334 --time: 17.92866802215576\n",
      "train acc:  0.90791709140333  train loss:  0.11374008169640666 --time: 17.645142078399658\n",
      "validation:  0.7 --max 0.7133333333333334 --time: 18.310808897018433\n",
      "train acc:  0.9113149847094801  train loss:  0.11397067425043686 --time: 17.951765060424805\n",
      "validation:  0.7166666666666667 --max 0.7166666666666667 --time: 18.802862882614136\n",
      "train acc:  0.909616038056405  train loss:  0.11194865859073141 --time: 17.61819314956665\n",
      "validation:  0.7133333333333334 --max 0.7166666666666667 --time: 18.223473072052002\n",
      "train acc:  0.9174311926605505  train loss:  0.11187480098527411 --time: 18.098541736602783\n",
      "validation:  0.7233333333333334 --max 0.7233333333333334 --time: 18.81243324279785\n",
      "train acc:  0.90791709140333  train loss:  0.113611682601597 --time: 18.188087224960327\n",
      "validation:  0.7 --max 0.7233333333333334 --time: 18.867416620254517\n",
      "train acc:  0.9133537206931702  train loss:  0.1094068279084952 --time: 17.75018548965454\n",
      "validation:  0.7166666666666667 --max 0.7233333333333334 --time: 18.374796390533447\n",
      "train acc:  0.9130139313625553  train loss:  0.10959290842647137 --time: 17.90389657020569\n",
      "validation:  0.7133333333333334 --max 0.7233333333333334 --time: 18.501681089401245\n",
      "train acc:  0.9109751953788651  train loss:  0.10904117220121881 --time: 17.428056955337524\n",
      "validation:  0.7066666666666667 --max 0.7233333333333334 --time: 18.018457174301147\n",
      "train acc:  0.9177709819911655  train loss:  0.10668835950934369 --time: 17.083807468414307\n",
      "validation:  0.7166666666666667 --max 0.7233333333333334 --time: 18.108323574066162\n",
      "train acc:  0.9153924566768603  train loss:  0.1060795809911645 --time: 17.30873465538025\n",
      "validation:  0.7133333333333334 --max 0.7233333333333334 --time: 18.48555636405945\n",
      "train acc:  0.9174311926605505  train loss:  0.10451120850832565 --time: 17.50521969795227\n",
      "validation:  0.7066666666666667 --max 0.7233333333333334 --time: 18.482911586761475\n",
      "train acc:  0.9204892966360856  train loss:  0.10665026922588763 --time: 17.18742871284485\n",
      "validation:  0.7166666666666667 --max 0.7233333333333334 --time: 18.140262603759766\n",
      "train acc:  0.9136935100237853  train loss:  0.1039194850170094 --time: 17.4066960811615\n",
      "validation:  0.7133333333333334 --max 0.7233333333333334 --time: 18.36908268928528\n",
      "train acc:  0.9181107713217805  train loss:  0.10278488144926402 --time: 17.446709632873535\n",
      "validation:  0.7166666666666667 --max 0.7233333333333334 --time: 18.11953854560852\n",
      "train acc:  0.9164118246687054  train loss:  0.10449381334626157 --time: 17.99525809288025\n",
      "validation:  0.72 --max 0.7233333333333334 --time: 19.037965774536133\n",
      "train acc:  0.9170914033299354  train loss:  0.10467569886342339 --time: 17.924539804458618\n",
      "validation:  0.7133333333333334 --max 0.7233333333333334 --time: 18.73398518562317\n",
      "train acc:  0.9238871899422358  train loss:  0.10153093707302342 --time: 17.706934690475464\n",
      "validation:  0.7066666666666667 --max 0.7233333333333334 --time: 18.557124376296997\n",
      "train acc:  0.9242269792728508  train loss:  0.10131389226602472 --time: 17.670992136001587\n",
      "validation:  0.73 --max 0.73 --time: 18.436251878738403\n",
      "train acc:  0.9198097179748556  train loss:  0.10031384002903233 --time: 17.572436571121216\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.448034048080444\n",
      "train acc:  0.9249065579340808  train loss:  0.0993880208419717 --time: 17.826854944229126\n",
      "validation:  0.71 --max 0.73 --time: 18.736963748931885\n",
      "train acc:  0.9296636085626911  train loss:  0.09835953816123631 --time: 18.24837851524353\n",
      "validation:  0.72 --max 0.73 --time: 18.924328804016113\n",
      "train acc:  0.9211688752973156  train loss:  0.10148852966401888 --time: 17.641090393066406\n",
      "validation:  0.6933333333333334 --max 0.73 --time: 18.35399580001831\n",
      "train acc:  0.9164118246687054  train loss:  0.10389760428148767 --time: 17.090139389038086\n",
      "validation:  0.7 --max 0.73 --time: 17.848830699920654\n",
      "train acc:  0.9126741420319402  train loss:  0.1081018904628961 --time: 16.714608669281006\n",
      "validation:  0.72 --max 0.73 --time: 17.644128561019897\n",
      "train acc:  0.9058783554196398  train loss:  0.10900488819765008 --time: 17.347098112106323\n",
      "validation:  0.7233333333333334 --max 0.73 --time: 18.306905508041382\n",
      "train acc:  0.9130139313625553  train loss:  0.11035457221062286 --time: 17.302654504776\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.275607109069824\n",
      "train acc:  0.9041794087665648  train loss:  0.11101858253064363 --time: 17.147128343582153\n",
      "validation:  0.7166666666666667 --max 0.73 --time: 18.004858016967773\n",
      "train acc:  0.9133537206931702  train loss:  0.10566363613242688 --time: 17.39514994621277\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 18.353259563446045\n",
      "train acc:  0.9133537206931702  train loss:  0.10201974439880122 --time: 17.504895210266113\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 18.484886169433594\n",
      "train acc:  0.9150526673462454  train loss:  0.09895638991957126 --time: 16.246681451797485\n",
      "validation:  0.7 --max 0.73 --time: 17.208509922027588\n",
      "train acc:  0.9218484539585458  train loss:  0.09592138360375943 --time: 17.137389183044434\n",
      "validation:  0.71 --max 0.73 --time: 18.168224096298218\n",
      "train acc:  0.9221882432891607  train loss:  0.09310666342144427 --time: 16.784979104995728\n",
      "validation:  0.72 --max 0.73 --time: 17.688491582870483\n",
      "train acc:  0.9320421338769963  train loss:  0.09287941909354666 --time: 17.034979820251465\n",
      "validation:  0.7033333333333334 --max 0.73 --time: 17.98288083076477\n",
      "train acc:  0.9283044512402311  train loss:  0.09092999411665875 --time: 16.64252209663391\n",
      "validation:  0.71 --max 0.73 --time: 17.56280016899109\n",
      "train acc:  0.9317023445463812  train loss:  0.08927588893667511 --time: 16.27669382095337\n",
      "validation:  0.71 --max 0.73 --time: 17.256158590316772\n",
      "train acc:  0.9327217125382263  train loss:  0.0914131066073542 --time: 17.759579181671143\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.703003644943237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9330615018688413  train loss:  0.0894127169057079 --time: 17.011809825897217\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 17.988672494888306\n",
      "train acc:  0.927964661909616  train loss:  0.09046839048033176 --time: 16.83220076560974\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 17.576088190078735\n",
      "train acc:  0.9395174991505266  train loss:  0.08867763987053996 --time: 17.579853534698486\n",
      "validation:  0.71 --max 0.73 --time: 18.523365259170532\n",
      "train acc:  0.9310227658851512  train loss:  0.0888652969961581 --time: 17.28476119041443\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.150908946990967\n",
      "train acc:  0.9384981311586816  train loss:  0.0869600798772729 --time: 17.256192207336426\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.308889389038086\n",
      "train acc:  0.9354400271831464  train loss:  0.08787852995421576 --time: 17.22477698326111\n",
      "validation:  0.7166666666666667 --max 0.73 --time: 18.272212266921997\n",
      "train acc:  0.9351002378525314  train loss:  0.08643294903247253 --time: 17.3052921295166\n",
      "validation:  0.7166666666666667 --max 0.73 --time: 18.320740461349487\n",
      "train acc:  0.9425756031260618  train loss:  0.08495449938851854 --time: 17.88860535621643\n",
      "validation:  0.7133333333333334 --max 0.73 --time: 18.904481649398804\n",
      "train acc:  0.9405368671423717  train loss:  0.08346965066764665 --time: 16.956328630447388\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 17.836721658706665\n",
      "train acc:  0.9337410805300713  train loss:  0.0854586724029935 --time: 15.810870885848999\n",
      "validation:  0.7 --max 0.73 --time: 16.395410299301147\n",
      "train acc:  0.9395174991505266  train loss:  0.0856733066232308 --time: 14.27116847038269\n",
      "validation:  0.7233333333333334 --max 0.73 --time: 14.999425411224365\n",
      "train acc:  0.9374787631668365  train loss:  0.08477915175583052 --time: 13.968329906463623\n",
      "validation:  0.7066666666666667 --max 0.73 --time: 14.570744276046753\n",
      "train acc:  0.9327217125382263  train loss:  0.08391968484805978 --time: 14.140606880187988\n",
      "validation:  0.71 --max 0.73 --time: 14.688852071762085\n",
      "train acc:  0.9425756031260618  train loss:  0.08188278309029082 --time: 12.900387048721313\n",
      "validation:  0.7033333333333334 --max 0.73 --time: 13.476954698562622\n",
      "train acc:  0.9361196058443765  train loss:  0.08285750027583993 --time: 13.433398962020874\n",
      "validation:  0.72 --max 0.73 --time: 14.155996561050415\n",
      "train acc:  0.9374787631668365  train loss:  0.08194255602100621 --time: 13.670310974121094\n",
      "validation:  0.72 --max 0.73 --time: 14.325988292694092\n",
      "train acc:  0.9364593951749915  train loss:  0.08159933206827744 --time: 13.558020114898682\n",
      "validation:  0.71 --max 0.73 --time: 14.31451153755188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5aab3af95490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([ [1,2,3], [10,20,30],[100,200,300], [1000,2000,3000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3],\n",
       "        [  10,   20,   30],\n",
       "        [ 100,  200,  300],\n",
       "        [1000, 2000, 3000]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     2,     3],\n",
       "        [   20,    40,    60],\n",
       "        [  300,   600,   900],\n",
       "        [ 4000,  8000, 12000]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0000,   2.0000,   3.0000],\n",
       "        [  5.0000,  10.0000,  15.0000],\n",
       "        [ 33.3333,  66.6667, 100.0000],\n",
       "        [250.0000, 500.0000, 750.0000]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.float()/a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
