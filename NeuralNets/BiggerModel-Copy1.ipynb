{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=384, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 384, num_layers=2)\n",
       "  (linear): Linear(in_features=384, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5656762250767701  train loss:  1.3870387353949785 --time: 27.709045886993408\n",
      "0 validation:  0.8072257383966245 --max 0.8072257383966245 --time: 31.27848243713379\n",
      "train acc:  0.8185199602093335  train loss:  0.5749109726584418 --time: 27.42573571205139\n",
      "1 validation:  0.8847573839662447 --max 0.8847573839662447 --time: 31.353657245635986\n",
      "train acc:  0.8782924613987284  train loss:  0.38989038246771246 --time: 27.913216590881348\n",
      "2 validation:  0.897415611814346 --max 0.897415611814346 --time: 31.967920780181885\n",
      "train acc:  0.9080489598200769  train loss:  0.29097310729448306 --time: 27.90494656562805\n",
      "3 validation:  0.9013713080168776 --max 0.9013713080168776 --time: 31.91893482208252\n",
      "train acc:  0.9300635785649409  train loss:  0.21519309422258515 --time: 27.770965576171875\n",
      "4 validation:  0.9071729957805907 --max 0.9071729957805907 --time: 31.782634496688843\n",
      "train acc:  0.946152848060205  train loss:  0.1704964364431181 --time: 27.881584644317627\n",
      "5 validation:  0.9021624472573839 --max 0.9071729957805907 --time: 31.895761489868164\n",
      "train acc:  0.9577872929371567  train loss:  0.13489213548806492 --time: 27.72683835029602\n",
      "6 validation:  0.9103375527426161 --max 0.9103375527426161 --time: 31.863534927368164\n",
      "train acc:  0.9727520435967303  train loss:  0.09271138237573165 --time: 27.89100170135498\n",
      "7 validation:  0.9179852320675106 --max 0.9179852320675106 --time: 32.00229263305664\n",
      "train acc:  0.9823537044245491  train loss:  0.06600172076765345 --time: 27.793508291244507\n",
      "8 validation:  0.9169303797468354 --max 0.9179852320675106 --time: 31.90479040145874\n",
      "train acc:  0.9892738203364906  train loss:  0.043548327381851264 --time: 27.82677984237671\n",
      "9 validation:  0.9116561181434599 --max 0.9179852320675106 --time: 31.996965408325195\n",
      "train acc:  0.9916093594567709  train loss:  0.03576481137784887 --time: 27.81215190887451\n",
      "10 validation:  0.9148206751054853 --max 0.9179852320675106 --time: 31.978774070739746\n",
      "train acc:  0.9934258898836555  train loss:  0.029701846433052206 --time: 27.68059539794922\n",
      "11 validation:  0.9137658227848101 --max 0.9179852320675106 --time: 31.779832124710083\n",
      "train acc:  0.978807145019679  train loss:  0.07030114538720629 --time: 27.856189489364624\n",
      "12 validation:  0.9077004219409283 --max 0.9179852320675106 --time: 32.03320670127869\n",
      "train acc:  0.9796721595086718  train loss:  0.0652969593840821 --time: 27.836036205291748\n",
      "13 validation:  0.9047995780590717 --max 0.9179852320675106 --time: 31.99201273918152\n",
      "train acc:  0.9880628000519008  train loss:  0.045217192412781125 --time: 27.735607624053955\n",
      "14 validation:  0.9127109704641351 --max 0.9179852320675106 --time: 31.903740406036377\n",
      "train acc:  0.9935556420570044  train loss:  0.02430714963220101 --time: 27.82067847251892\n",
      "15 validation:  0.9161392405063291 --max 0.9179852320675106 --time: 31.967334747314453\n",
      "train acc:  0.9960641840750832  train loss:  0.016782427004677403 --time: 27.79525089263916\n",
      "16 validation:  0.9100738396624473 --max 0.9179852320675106 --time: 31.974013805389404\n",
      "train acc:  0.997275204359673  train loss:  0.010768740426990827 --time: 27.800220727920532\n",
      "17 validation:  0.9171940928270043 --max 0.9179852320675106 --time: 31.989439964294434\n",
      "train acc:  0.9980537173997664  train loss:  0.007608280831353626 --time: 27.766947746276855\n",
      "18 validation:  0.9177215189873418 --max 0.9179852320675106 --time: 31.983673810958862\n",
      "train acc:  0.9978807145019679  train loss:  0.006978843741579424 --time: 27.90183448791504\n",
      "19 validation:  0.9214135021097046 --max 0.9214135021097046 --time: 32.15606379508972\n",
      "train acc:  0.9981402188486657  train loss:  0.005958153924599897 --time: 27.53386425971985\n",
      "20 validation:  0.9166666666666666 --max 0.9214135021097046 --time: 31.78039312362671\n",
      "train acc:  0.9980969681242161  train loss:  0.006076911940928857 --time: 27.522034883499146\n",
      "21 validation:  0.9206223628691983 --max 0.9214135021097046 --time: 31.790467739105225\n",
      "train acc:  0.9980969681242161  train loss:  0.0052598680069513145 --time: 27.557239294052124\n",
      "22 validation:  0.9182489451476793 --max 0.9214135021097046 --time: 31.88375496864319\n",
      "train acc:  0.9980104666753168  train loss:  0.005882562961087186 --time: 27.30831527709961\n",
      "23 validation:  0.9179852320675106 --max 0.9214135021097046 --time: 31.679460525512695\n",
      "train acc:  0.9952856710349899  train loss:  0.014306943398612186 --time: 27.687947988510132\n",
      "24 validation:  0.8871308016877637 --max 0.9214135021097046 --time: 32.048073291778564\n",
      "train acc:  0.9294148176981964  train loss:  0.21179774942812998 --time: 27.693962335586548\n",
      "25 validation:  0.9040084388185654 --max 0.9214135021097046 --time: 32.0284628868103\n",
      "train acc:  0.9839972319536352  train loss:  0.053868302272745916 --time: 27.50742197036743\n",
      "26 validation:  0.9079641350210971 --max 0.9214135021097046 --time: 31.85486388206482\n",
      "train acc:  0.9934691406081052  train loss:  0.0238003161895341 --time: 27.236846923828125\n",
      "27 validation:  0.9103375527426161 --max 0.9214135021097046 --time: 31.658047914505005\n",
      "train acc:  0.9970589507374249  train loss:  0.011405906350562833 --time: 27.467308282852173\n",
      "28 validation:  0.917457805907173 --max 0.9214135021097046 --time: 31.83687114715576\n",
      "train acc:  0.9980537173997664  train loss:  0.007374174567132427 --time: 27.47713875770569\n",
      "29 validation:  0.9200949367088608 --max 0.9214135021097046 --time: 31.92458200454712\n",
      "train acc:  0.9974914579819212  train loss:  0.007693455591206104 --time: 27.511221170425415\n",
      "30 validation:  0.9190400843881856 --max 0.9214135021097046 --time: 31.894944429397583\n",
      "train acc:  0.9974914579819212  train loss:  0.007517158690079769 --time: 27.464847803115845\n",
      "31 validation:  0.9171940928270043 --max 0.9214135021097046 --time: 31.773711681365967\n",
      "train acc:  0.9971887029107738  train loss:  0.009112424632657875 --time: 27.365459203720093\n",
      "32 validation:  0.9132383966244726 --max 0.9214135021097046 --time: 31.793651580810547\n",
      "train acc:  0.997404956533022  train loss:  0.00801839884203339 --time: 27.59421443939209\n",
      "33 validation:  0.9182489451476793 --max 0.9214135021097046 --time: 31.863515615463257\n",
      "train acc:  0.9974482072574715  train loss:  0.008580113522507744 --time: 27.716121196746826\n",
      "34 validation:  0.917457805907173 --max 0.9214135021097046 --time: 31.91450572013855\n",
      "train acc:  0.9844729899225813  train loss:  0.04642097339116407 --time: 27.665127277374268\n",
      "35 validation:  0.8976793248945147 --max 0.9214135021097046 --time: 31.826568841934204\n",
      "train acc:  0.9739198131568704  train loss:  0.07566089963139092 --time: 27.81519055366516\n",
      "36 validation:  0.9124472573839663 --max 0.9214135021097046 --time: 31.86589241027832\n",
      "train acc:  0.994204402923749  train loss:  0.021851289411492194 --time: 27.82758641242981\n",
      "37 validation:  0.9092827004219409 --max 0.9214135021097046 --time: 32.006739139556885\n",
      "train acc:  0.9968426971151767  train loss:  0.010985179873085607 --time: 27.44100522994995\n",
      "38 validation:  0.9171940928270043 --max 0.9214135021097046 --time: 31.46589756011963\n",
      "train acc:  0.9974482072574715  train loss:  0.008568366031157295 --time: 27.859019994735718\n",
      "39 validation:  0.9169303797468354 --max 0.9214135021097046 --time: 31.881626844406128\n",
      "train acc:  0.9980104666753168  train loss:  0.0064516290575291615 --time: 27.63054585456848\n",
      "40 validation:  0.9214135021097046 --max 0.9214135021097046 --time: 31.756921768188477\n",
      "train acc:  0.9982699710220146  train loss:  0.004554918208879866 --time: 27.92383861541748\n",
      "41 validation:  0.9203586497890295 --max 0.9214135021097046 --time: 31.899678230285645\n",
      "train acc:  0.9984429739198132  train loss:  0.004407715413350231 --time: 27.97066330909729\n",
      "42 validation:  0.9208860759493671 --max 0.9214135021097046 --time: 31.911396026611328\n",
      "train acc:  0.9981834695731153  train loss:  0.004642135477242142 --time: 28.069260120391846\n",
      "43 validation:  0.9208860759493671 --max 0.9214135021097046 --time: 32.07286834716797\n",
      "train acc:  0.9980969681242161  train loss:  0.004663235154478857 --time: 27.94508934020996\n",
      "44 validation:  0.9224683544303798 --max 0.9224683544303798 --time: 31.95129680633545\n",
      "train acc:  0.9983997231953635  train loss:  0.0044981318920560035 --time: 28.081068515777588\n",
      "45 validation:  0.9240506329113924 --max 0.9240506329113924 --time: 31.77420473098755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9983564724709139  train loss:  0.003920862024004548 --time: 28.306659698486328\n",
      "46 validation:  0.9195675105485233 --max 0.9240506329113924 --time: 31.9681715965271\n",
      "train acc:  0.9982699710220146  train loss:  0.0043623430048371535 --time: 28.346237897872925\n",
      "47 validation:  0.9190400843881856 --max 0.9240506329113924 --time: 32.03111672401428\n",
      "train acc:  0.9984862246442628  train loss:  0.004595090844395097 --time: 28.21490240097046\n",
      "48 validation:  0.9200949367088608 --max 0.9240506329113924 --time: 31.919862747192383\n",
      "train acc:  0.9981834695731153  train loss:  0.004767956597024951 --time: 28.165061473846436\n",
      "49 validation:  0.9179852320675106 --max 0.9240506329113924 --time: 31.91321635246277\n",
      "train acc:  0.9592578175684443  train loss:  0.12270525450952022 --time: 28.247945070266724\n",
      "50 validation:  0.9037447257383966 --max 0.9240506329113924 --time: 31.975516319274902\n",
      "train acc:  0.9801046667531681  train loss:  0.06271103906409187 --time: 28.419225931167603\n",
      "51 validation:  0.9145569620253164 --max 0.9240506329113924 --time: 32.29467964172363\n",
      "train acc:  0.9949396652393928  train loss:  0.017197352096963242 --time: 28.255767107009888\n",
      "52 validation:  0.9179852320675106 --max 0.9240506329113924 --time: 32.28543305397034\n",
      "train acc:  0.997750962328619  train loss:  0.008501215911861407 --time: 27.977150440216064\n",
      "53 validation:  0.9164029535864979 --max 0.9240506329113924 --time: 31.722790718078613\n",
      "train acc:  0.9978374637775183  train loss:  0.006745265847591693 --time: 28.116809368133545\n",
      "54 validation:  0.9208860759493671 --max 0.9240506329113924 --time: 31.9497971534729\n",
      "train acc:  0.9981402188486657  train loss:  0.004786266102095376 --time: 28.173374891281128\n",
      "55 validation:  0.9182489451476793 --max 0.9240506329113924 --time: 31.945640087127686\n",
      "train acc:  0.9983132217464643  train loss:  0.004331489219664704 --time: 28.307948112487793\n",
      "56 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 32.08631992340088\n",
      "train acc:  0.998226720297565  train loss:  0.004341200548587547 --time: 28.319040536880493\n",
      "57 validation:  0.9195675105485233 --max 0.9240506329113924 --time: 32.0915424823761\n",
      "train acc:  0.9984429739198132  train loss:  0.004234223176007422 --time: 28.199673414230347\n",
      "58 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 32.03282117843628\n",
      "train acc:  0.9980537173997664  train loss:  0.004411245460602074 --time: 28.281086683273315\n",
      "59 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 32.168927907943726\n",
      "train acc:  0.9983564724709139  train loss:  0.004450261930533742 --time: 28.410122632980347\n",
      "60 validation:  0.919831223628692 --max 0.9240506329113924 --time: 32.22880792617798\n",
      "train acc:  0.9980969681242161  train loss:  0.004399187289318584 --time: 28.09288716316223\n",
      "61 validation:  0.919831223628692 --max 0.9240506329113924 --time: 31.88078784942627\n",
      "train acc:  0.998226720297565  train loss:  0.004190488950637535 --time: 28.320362329483032\n",
      "62 validation:  0.9219409282700421 --max 0.9240506329113924 --time: 32.22512173652649\n",
      "train acc:  0.9981834695731153  train loss:  0.004148880535934779 --time: 28.243760585784912\n",
      "63 validation:  0.922204641350211 --max 0.9240506329113924 --time: 32.15309023857117\n",
      "train acc:  0.9983997231953635  train loss:  0.004044545389856893 --time: 28.043327569961548\n",
      "64 validation:  0.9219409282700421 --max 0.9240506329113924 --time: 32.08822464942932\n",
      "train acc:  0.998226720297565  train loss:  0.0039500632181905795 --time: 27.971882820129395\n",
      "65 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 32.02691340446472\n",
      "train acc:  0.9983564724709139  train loss:  0.004199591579337962 --time: 27.95506525039673\n",
      "66 validation:  0.9224683544303798 --max 0.9240506329113924 --time: 32.07941699028015\n",
      "train acc:  0.9984429739198132  train loss:  0.0038290493874389353 --time: 27.957397937774658\n",
      "67 validation:  0.9179852320675106 --max 0.9240506329113924 --time: 32.091333627700806\n",
      "train acc:  0.9984862246442628  train loss:  0.003936290136700527 --time: 27.763917922973633\n",
      "68 validation:  0.9190400843881856 --max 0.9240506329113924 --time: 31.84666609764099\n",
      "train acc:  0.9982699710220146  train loss:  0.003898837639847629 --time: 27.826502084732056\n",
      "69 validation:  0.919831223628692 --max 0.9240506329113924 --time: 32.02283191680908\n",
      "train acc:  0.998226720297565  train loss:  0.004255266316641555 --time: 28.190368175506592\n",
      "70 validation:  0.9195675105485233 --max 0.9240506329113924 --time: 32.47739005088806\n",
      "train acc:  0.9714545218632412  train loss:  0.08619636327133756 --time: 27.802156448364258\n",
      "71 validation:  0.8858122362869199 --max 0.9240506329113924 --time: 32.17749762535095\n",
      "train acc:  0.9651831668180442  train loss:  0.10285945158472377 --time: 28.037561416625977\n",
      "72 validation:  0.9095464135021097 --max 0.9240506329113924 --time: 32.446584939956665\n",
      "train acc:  0.9917391116301199  train loss:  0.025745544298320433 --time: 27.795600414276123\n",
      "73 validation:  0.9135021097046413 --max 0.9240506329113924 --time: 32.156662940979004\n",
      "train acc:  0.9975347087063708  train loss:  0.009380057759940604 --time: 27.627327919006348\n",
      "74 validation:  0.9177215189873418 --max 0.9240506329113924 --time: 31.940763235092163\n",
      "train acc:  0.9984429739198132  train loss:  0.005129364141949982 --time: 27.65605878829956\n",
      "75 validation:  0.9185126582278481 --max 0.9240506329113924 --time: 32.056511640548706\n",
      "train acc:  0.9983997231953635  train loss:  0.00417267417506886 --time: 27.655831813812256\n",
      "76 validation:  0.9203586497890295 --max 0.9240506329113924 --time: 32.0265371799469\n",
      "train acc:  0.998572726093162  train loss:  0.0039932912976792315 --time: 27.628925323486328\n",
      "77 validation:  0.9208860759493671 --max 0.9240506329113924 --time: 32.04971122741699\n",
      "train acc:  0.9985294753687124  train loss:  0.0038963431297564747 --time: 27.79474425315857\n",
      "78 validation:  0.9214135021097046 --max 0.9240506329113924 --time: 32.07278275489807\n",
      "train acc:  0.998226720297565  train loss:  0.003996594180127874 --time: 27.729806900024414\n",
      "79 validation:  0.9193037974683544 --max 0.9240506329113924 --time: 32.140657901763916\n",
      "train acc:  0.9981402188486657  train loss:  0.004289947581369409 --time: 27.58565616607666\n",
      "80 validation:  0.9190400843881856 --max 0.9240506329113924 --time: 31.955174684524536\n",
      "train acc:  0.9982699710220146  train loss:  0.004384395501123566 --time: 27.24434518814087\n",
      "81 validation:  0.9193037974683544 --max 0.9240506329113924 --time: 31.619140625\n",
      "train acc:  0.9983997231953635  train loss:  0.003855913008311738 --time: 27.410471200942993\n",
      "82 validation:  0.9187763713080169 --max 0.9240506329113924 --time: 31.779536724090576\n",
      "train acc:  0.9984862246442628  train loss:  0.0037969283925051653 --time: 27.627450942993164\n",
      "83 validation:  0.919831223628692 --max 0.9240506329113924 --time: 31.935560703277588\n",
      "train acc:  0.9983997231953635  train loss:  0.0038168317061004443 --time: 27.68254518508911\n",
      "84 validation:  0.9208860759493671 --max 0.9240506329113924 --time: 32.07462739944458\n",
      "train acc:  0.9981402188486657  train loss:  0.0038766237447319363 --time: 27.788606643676758\n",
      "85 validation:  0.9203586497890295 --max 0.9240506329113924 --time: 32.17983913421631\n",
      "train acc:  0.998226720297565  train loss:  0.004077968017790834 --time: 27.594703435897827\n",
      "86 validation:  0.9224683544303798 --max 0.9240506329113924 --time: 31.94906973838806\n",
      "train acc:  0.9983997231953635  train loss:  0.003742692965519864 --time: 27.58770227432251\n",
      "87 validation:  0.922204641350211 --max 0.9240506329113924 --time: 32.00239586830139\n",
      "train acc:  0.9984429739198132  train loss:  0.003733961010158502 --time: 27.544947862625122\n",
      "88 validation:  0.9216772151898734 --max 0.9240506329113924 --time: 31.94719433784485\n",
      "train acc:  0.9981834695731153  train loss:  0.003978058440853245 --time: 27.729633808135986\n",
      "89 validation:  0.9203586497890295 --max 0.9240506329113924 --time: 32.11126947402954\n",
      "train acc:  0.9983564724709139  train loss:  0.0038378124484034688 --time: 27.596564292907715\n",
      "90 validation:  0.9211497890295358 --max 0.9240506329113924 --time: 32.0233108997345\n",
      "train acc:  0.9983132217464643  train loss:  0.003974710369321199 --time: 27.58434796333313\n",
      "91 validation:  0.9219409282700421 --max 0.9240506329113924 --time: 31.884052991867065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9985294753687124  train loss:  0.003564983304808346 --time: 27.632081985473633\n",
      "92 validation:  0.9208860759493671 --max 0.9240506329113924 --time: 32.00350332260132\n",
      "train acc:  0.998572726093162  train loss:  0.0033651491508631674 --time: 27.812506437301636\n",
      "93 validation:  0.9216772151898734 --max 0.9240506329113924 --time: 32.11572766304016\n",
      "train acc:  0.9982699710220146  train loss:  0.0037728226723539185 --time: 27.739736557006836\n",
      "94 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 32.096752643585205\n",
      "train acc:  0.9983132217464643  train loss:  0.0036565493952688114 --time: 27.590723991394043\n",
      "95 validation:  0.9219409282700421 --max 0.9240506329113924 --time: 31.94054079055786\n",
      "train acc:  0.9983997231953635  train loss:  0.0038754207729733386 --time: 27.807960033416748\n",
      "96 validation:  0.9203586497890295 --max 0.9240506329113924 --time: 32.05123257637024\n",
      "train acc:  0.9982699710220146  train loss:  0.004194337664633052 --time: 27.661723613739014\n",
      "97 validation:  0.9206223628691983 --max 0.9240506329113924 --time: 31.81954312324524\n",
      "train acc:  0.9983564724709139  train loss:  0.0038421348058539817 --time: 27.957369089126587\n",
      "98 validation:  0.9208860759493671 --max 0.9240506329113924 --time: 32.112831830978394\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 LSTM layer, size 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
