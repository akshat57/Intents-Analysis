{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati_marathi_bengali_hindi_15'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/3_lang_variations/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=69, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(69, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati_marathi_bengali_hindi_15 hindi\n",
      "train acc:  0.24872579001019368  train loss:  0.4611376770164656 --time: 3.5377166271209717\n",
      "validation:  0.18666666666666668 --max 0.18666666666666668 --time: 4.207354784011841\n",
      "train acc:  0.30139313625552155  train loss:  0.4328637291555819 --time: 3.1559157371520996\n",
      "validation:  0.15333333333333332 --max 0.18666666666666668 --time: 3.8269591331481934\n",
      "train acc:  0.3143051308188923  train loss:  0.42676447526268335 --time: 4.210057258605957\n",
      "validation:  0.19333333333333333 --max 0.19333333333333333 --time: 4.713374614715576\n",
      "train acc:  0.3384301732925586  train loss:  0.4203515597011732 --time: 4.15291166305542\n",
      "validation:  0.19333333333333333 --max 0.19333333333333333 --time: 4.537609577178955\n",
      "train acc:  0.3696907917091403  train loss:  0.4110347576763319 --time: 4.836566686630249\n",
      "validation:  0.21666666666666667 --max 0.21666666666666667 --time: 5.256062269210815\n",
      "train acc:  0.4148827726809378  train loss:  0.3944229807542718 --time: 4.865340709686279\n",
      "validation:  0.23666666666666666 --max 0.23666666666666666 --time: 5.366828441619873\n",
      "train acc:  0.4712878015630309  train loss:  0.3734556345835976 --time: 4.6688220500946045\n",
      "validation:  0.26666666666666666 --max 0.26666666666666666 --time: 5.249131917953491\n",
      "train acc:  0.507985049269453  train loss:  0.35437447610108747 --time: 4.740152359008789\n",
      "validation:  0.35333333333333333 --max 0.35333333333333333 --time: 5.270806074142456\n",
      "train acc:  0.54570166496772  train loss:  0.33344267243924347 --time: 4.714547395706177\n",
      "validation:  0.4066666666666667 --max 0.4066666666666667 --time: 5.1732470989227295\n",
      "train acc:  0.6163778457356439  train loss:  0.30068859198819037 --time: 4.599041223526001\n",
      "validation:  0.45666666666666667 --max 0.45666666666666667 --time: 5.326344966888428\n",
      "train acc:  0.6653075093442066  train loss:  0.2717065428910048 --time: 4.527309894561768\n",
      "validation:  0.4666666666666667 --max 0.4666666666666667 --time: 5.0741894245147705\n",
      "train acc:  0.7135575942915392  train loss:  0.2424336263666982 --time: 4.483090162277222\n",
      "validation:  0.5333333333333333 --max 0.5333333333333333 --time: 5.1909685134887695\n",
      "train acc:  0.7471967380224261  train loss:  0.2130141037961711 --time: 4.465967178344727\n",
      "validation:  0.5633333333333334 --max 0.5633333333333334 --time: 4.984622955322266\n",
      "train acc:  0.7859327217125383  train loss:  0.18354724736317343 --time: 4.455621719360352\n",
      "validation:  0.57 --max 0.57 --time: 5.114153861999512\n",
      "train acc:  0.7770981991165478  train loss:  0.1861073718122814 --time: 4.410757064819336\n",
      "validation:  0.59 --max 0.59 --time: 5.070239067077637\n",
      "train acc:  0.799184505606524  train loss:  0.1688893094010975 --time: 4.329818248748779\n",
      "validation:  0.6166666666666667 --max 0.6166666666666667 --time: 4.989208698272705\n",
      "train acc:  0.8273870200475705  train loss:  0.15140319259270377 --time: 4.468334913253784\n",
      "validation:  0.6166666666666667 --max 0.6166666666666667 --time: 5.148359060287476\n",
      "train acc:  0.8572884811416922  train loss:  0.131163589656353 --time: 4.29007625579834\n",
      "validation:  0.6233333333333333 --max 0.6233333333333333 --time: 4.985410451889038\n",
      "train acc:  0.874277947672443  train loss:  0.11719082846589711 --time: 4.063404321670532\n",
      "validation:  0.6733333333333333 --max 0.6733333333333333 --time: 4.717480421066284\n",
      "train acc:  0.8793747876316683  train loss:  0.1150248154349949 --time: 4.398267030715942\n",
      "validation:  0.6566666666666666 --max 0.6733333333333333 --time: 5.044630527496338\n",
      "train acc:  0.8926265715256541  train loss:  0.10062983372937077 --time: 4.280019760131836\n",
      "validation:  0.6333333333333333 --max 0.6733333333333333 --time: 4.98232102394104\n",
      "train acc:  0.8970438328236493  train loss:  0.09784294357118399 --time: 4.369850397109985\n",
      "validation:  0.63 --max 0.6733333333333333 --time: 5.095602512359619\n",
      "train acc:  0.9211688752973156  train loss:  0.08290214512659155 --time: 4.481273651123047\n",
      "validation:  0.6533333333333333 --max 0.6733333333333333 --time: 5.066551685333252\n",
      "train acc:  0.9204892966360856  train loss:  0.08164460373961407 --time: 4.19930362701416\n",
      "validation:  0.65 --max 0.6733333333333333 --time: 4.7825212478637695\n",
      "train acc:  0.9398572884811417  train loss:  0.06526851087160733 --time: 4.506504774093628\n",
      "validation:  0.66 --max 0.6733333333333333 --time: 5.139161825180054\n",
      "train acc:  0.9582059123343527  train loss:  0.05440338360874549 --time: 4.425424098968506\n",
      "validation:  0.7 --max 0.7 --time: 4.972315073013306\n",
      "train acc:  0.9578661230037376  train loss:  0.05306336283683777 --time: 4.36828088760376\n",
      "validation:  0.6833333333333333 --max 0.7 --time: 4.9743335247039795\n",
      "train acc:  0.9673802242609582  train loss:  0.04374678837864295 --time: 4.410924434661865\n",
      "validation:  0.6566666666666666 --max 0.7 --time: 5.06583046913147\n",
      "train acc:  0.9704383282364933  train loss:  0.040471759017394936 --time: 4.588585615158081\n",
      "validation:  0.6666666666666666 --max 0.7 --time: 5.235742092132568\n",
      "train acc:  0.9616038056405029  train loss:  0.04177733178695907 --time: 4.412916660308838\n",
      "validation:  0.6933333333333334 --max 0.7 --time: 5.17124342918396\n",
      "train acc:  0.9483520217465171  train loss:  0.053845077105190445 --time: 4.358293294906616\n",
      "validation:  0.67 --max 0.7 --time: 5.063347816467285\n",
      "train acc:  0.9677200135915732  train loss:  0.040012733281954475 --time: 4.522012233734131\n",
      "validation:  0.6833333333333333 --max 0.7 --time: 5.150258541107178\n",
      "train acc:  0.9782534828406388  train loss:  0.028469714012158955 --time: 4.387999534606934\n",
      "validation:  0.69 --max 0.7 --time: 5.015177488327026\n",
      "train acc:  0.983010533469249  train loss:  0.024446040189460568 --time: 4.616055965423584\n",
      "validation:  0.71 --max 0.71 --time: 5.18978214263916\n",
      "train acc:  0.9789330615018689  train loss:  0.025850890204310417 --time: 4.826051473617554\n",
      "validation:  0.72 --max 0.72 --time: 5.322093725204468\n",
      "train acc:  0.9707781175671084  train loss:  0.029362063247548496 --time: 4.854032754898071\n",
      "validation:  0.71 --max 0.72 --time: 5.434189796447754\n",
      "train acc:  0.9802922188243289  train loss:  0.024855517903747765 --time: 4.490187406539917\n",
      "validation:  0.69 --max 0.72 --time: 5.115039825439453\n",
      "train acc:  0.9860686374447842  train loss:  0.01812737167853376 --time: 4.326090574264526\n",
      "validation:  0.7 --max 0.72 --time: 4.844708442687988\n",
      "train acc:  0.9874277947672443  train loss:  0.01716158893364279 --time: 4.516953229904175\n",
      "validation:  0.7233333333333334 --max 0.7233333333333334 --time: 5.070789337158203\n",
      "train acc:  0.9765545361875637  train loss:  0.027684339139934466 --time: 4.775269269943237\n",
      "validation:  0.6533333333333333 --max 0.7233333333333334 --time: 5.324299335479736\n",
      "train acc:  0.9870880054366293  train loss:  0.0195495564814495 --time: 4.649630069732666\n",
      "validation:  0.6866666666666666 --max 0.7233333333333334 --time: 5.166610479354858\n",
      "train acc:  0.9928644240570846  train loss:  0.012161826917334743 --time: 4.722820281982422\n",
      "validation:  0.7 --max 0.7233333333333334 --time: 5.2430970668792725\n",
      "train acc:  0.9949031600407747  train loss:  0.008807299354963976 --time: 4.583937168121338\n",
      "validation:  0.6866666666666666 --max 0.7233333333333334 --time: 5.132386207580566\n",
      "train acc:  0.9870880054366293  train loss:  0.016360173427054415 --time: 4.544694900512695\n",
      "validation:  0.6866666666666666 --max 0.7233333333333334 --time: 5.1167871952056885\n",
      "train acc:  0.9959225280326198  train loss:  0.00834004187187099 --time: 4.63484001159668\n",
      "validation:  0.7033333333333334 --max 0.7233333333333334 --time: 5.389936923980713\n",
      "train acc:  0.9972816853550799  train loss:  0.007090194454497617 --time: 4.409142255783081\n",
      "validation:  0.71 --max 0.7233333333333334 --time: 5.120031833648682\n",
      "train acc:  0.9969418960244648  train loss:  0.005389638093259671 --time: 4.371438980102539\n",
      "validation:  0.72 --max 0.7233333333333334 --time: 5.085428714752197\n",
      "train acc:  0.9976214746856948  train loss:  0.005645387390952395 --time: 4.365458250045776\n",
      "validation:  0.7166666666666667 --max 0.7233333333333334 --time: 5.058784246444702\n",
      "train acc:  0.99932042133877  train loss:  0.003914634373200976 --time: 4.369703531265259\n",
      "validation:  0.72 --max 0.7233333333333334 --time: 4.958760976791382\n",
      "train acc:  0.99932042133877  train loss:  0.002952259610933454 --time: 4.438509941101074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:  0.7366666666666667 --max 0.7366666666666667 --time: 5.072945833206177\n",
      "train acc:  1.0  train loss:  0.002691733625555492 --time: 4.436884164810181\n",
      "validation:  0.7433333333333333 --max 0.7433333333333333 --time: 5.0824432373046875\n",
      "train acc:  1.0  train loss:  0.001960788144851508 --time: 4.562573671340942\n",
      "validation:  0.7366666666666667 --max 0.7433333333333333 --time: 5.216519832611084\n",
      "train acc:  1.0  train loss:  0.0017082726825838504 --time: 4.494392156600952\n",
      "validation:  0.7333333333333333 --max 0.7433333333333333 --time: 5.110377073287964\n",
      "train acc:  1.0  train loss:  0.0016286379515962756 --time: 4.498527765274048\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 5.125490665435791\n",
      "train acc:  1.0  train loss:  0.0014541093513126607 --time: 4.424524307250977\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 4.951566219329834\n",
      "train acc:  1.0  train loss:  0.001379696318231847 --time: 4.44624137878418\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 4.969449758529663\n",
      "train acc:  1.0  train loss:  0.0013137576119650316 --time: 4.389886379241943\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 5.010720252990723\n",
      "train acc:  1.0  train loss:  0.001237147234623199 --time: 4.585000991821289\n",
      "validation:  0.7433333333333333 --max 0.7466666666666667 --time: 5.031005620956421\n",
      "train acc:  1.0  train loss:  0.001174420123393445 --time: 4.6158058643341064\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 5.170819997787476\n",
      "train acc:  1.0  train loss:  0.001113482125852581 --time: 4.695842027664185\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 5.169183254241943\n",
      "train acc:  1.0  train loss:  0.0010617000288734941 --time: 4.68660306930542\n",
      "validation:  0.74 --max 0.7466666666666667 --time: 5.366542100906372\n",
      "train acc:  1.0  train loss:  0.001021127031771871 --time: 4.67744779586792\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 5.303437948226929\n",
      "train acc:  1.0  train loss:  0.000946167633747277 --time: 4.494643449783325\n",
      "validation:  0.7333333333333333 --max 0.7466666666666667 --time: 5.109355926513672\n",
      "train acc:  1.0  train loss:  0.0009390669520539434 --time: 4.495426416397095\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 5.078528881072998\n",
      "train acc:  1.0  train loss:  0.0009170851553790271 --time: 4.554623365402222\n",
      "validation:  0.74 --max 0.7466666666666667 --time: 5.130195140838623\n",
      "train acc:  1.0  train loss:  0.0008624661966915364 --time: 4.433811187744141\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 4.988714694976807\n",
      "train acc:  1.0  train loss:  0.0008363529366603041 --time: 4.309384107589722\n",
      "validation:  0.74 --max 0.7466666666666667 --time: 4.862140655517578\n",
      "train acc:  1.0  train loss:  0.00079140012435939 --time: 4.523956298828125\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 5.010377407073975\n",
      "train acc:  0.9813115868161739  train loss:  0.01848774522026677 --time: 4.473876476287842\n",
      "validation:  0.69 --max 0.7466666666666667 --time: 5.018090486526489\n",
      "train acc:  0.9619435949711179  train loss:  0.03917561965468137 --time: 4.604980945587158\n",
      "validation:  0.7233333333333334 --max 0.7466666666666667 --time: 5.2683680057525635\n",
      "train acc:  0.9670404349303432  train loss:  0.03203402807855088 --time: 4.585231065750122\n",
      "validation:  0.6933333333333334 --max 0.7466666666666667 --time: 5.154668092727661\n",
      "train acc:  0.9796126401630989  train loss:  0.021437981894806675 --time: 4.508984088897705\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 5.012032747268677\n",
      "train acc:  0.9867482161060143  train loss:  0.017703929369378348 --time: 4.721038341522217\n",
      "validation:  0.71 --max 0.7466666666666667 --time: 5.161987066268921\n",
      "train acc:  0.9884471627590894  train loss:  0.013739372461872257 --time: 4.791221857070923\n",
      "validation:  0.71 --max 0.7466666666666667 --time: 5.3141725063323975\n",
      "train acc:  0.9860686374447842  train loss:  0.01608840072446543 --time: 4.736093044281006\n",
      "validation:  0.7066666666666667 --max 0.7466666666666667 --time: 5.2120115756988525\n",
      "train acc:  0.9952429493713897  train loss:  0.007852621153806862 --time: 4.681109666824341\n",
      "validation:  0.7233333333333334 --max 0.7466666666666667 --time: 5.2176618576049805\n",
      "train acc:  0.9901461094121644  train loss:  0.01267107540194917 --time: 4.681356906890869\n",
      "validation:  0.69 --max 0.7466666666666667 --time: 5.277684211730957\n",
      "train acc:  0.9792728508324838  train loss:  0.019305919479254797 --time: 4.624862909317017\n",
      "validation:  0.6966666666666667 --max 0.7466666666666667 --time: 5.244645357131958\n",
      "train acc:  0.9826707441386341  train loss:  0.020055717505190685 --time: 4.695379257202148\n",
      "validation:  0.73 --max 0.7466666666666667 --time: 5.190562009811401\n",
      "train acc:  0.9952429493713897  train loss:  0.008188163041663558 --time: 4.481255054473877\n",
      "validation:  0.6966666666666667 --max 0.7466666666666667 --time: 5.051710844039917\n",
      "train acc:  0.9969418960244648  train loss:  0.00588430058332565 --time: 4.7343573570251465\n",
      "validation:  0.7233333333333334 --max 0.7466666666666667 --time: 5.300180196762085\n",
      "train acc:  1.0  train loss:  0.0019192601675572603 --time: 4.6602208614349365\n",
      "validation:  0.73 --max 0.7466666666666667 --time: 5.329463958740234\n",
      "train acc:  1.0  train loss:  0.001295132635910388 --time: 4.634414196014404\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 5.566817045211792\n",
      "train acc:  1.0  train loss:  0.0010267845658666413 --time: 8.110323667526245\n",
      "validation:  0.74 --max 0.7466666666666667 --time: 8.82949423789978\n",
      "train acc:  1.0  train loss:  0.0008869845895906506 --time: 7.052934646606445\n",
      "validation:  0.7433333333333333 --max 0.7466666666666667 --time: 8.02332091331482\n",
      "train acc:  1.0  train loss:  0.0008208217762370149 --time: 8.13603138923645\n",
      "validation:  0.7466666666666667 --max 0.7466666666666667 --time: 9.024651288986206\n",
      "train acc:  1.0  train loss:  0.0007764669843827901 --time: 7.255402326583862\n",
      "validation:  0.7366666666666667 --max 0.7466666666666667 --time: 8.056287050247192\n",
      "train acc:  1.0  train loss:  0.0007235958954602804 --time: 10.087076902389526\n",
      "validation:  0.7433333333333333 --max 0.7466666666666667 --time: 11.330506324768066\n",
      "train acc:  1.0  train loss:  0.0006831183184302696 --time: 10.350782871246338\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 11.650171995162964\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5aab3af95490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mout_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0macc_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
