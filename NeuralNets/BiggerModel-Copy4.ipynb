{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=512, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        #cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "        \n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "        \n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 512, num_layers=2)\n",
       "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5767051598114269  train loss:  1.3503262515884737 --time: 27.802088499069214\n",
      "0 validation:  0.8267405063291139 --max 0.8267405063291139 --time: 31.805152654647827\n",
      "train acc:  0.8242290558366853  train loss:  0.5568097095463157 --time: 28.063539266586304\n",
      "1 validation:  0.8850210970464135 --max 0.8850210970464135 --time: 32.156490325927734\n",
      "train acc:  0.8759136715539986  train loss:  0.3887044584882852 --time: 27.90113663673401\n",
      "2 validation:  0.8995253164556962 --max 0.8995253164556962 --time: 31.950369119644165\n",
      "train acc:  0.9125902858872886  train loss:  0.2793455912592661 --time: 27.847119092941284\n",
      "3 validation:  0.9000527426160337 --max 0.9000527426160337 --time: 31.90141987800598\n",
      "train acc:  0.9292850655248476  train loss:  0.22086571827942494 --time: 28.05527353286743\n",
      "4 validation:  0.9087552742616034 --max 0.9087552742616034 --time: 32.21358776092529\n",
      "train acc:  0.9476666234159422  train loss:  0.1669569470564634 --time: 28.019537687301636\n",
      "5 validation:  0.9090189873417721 --max 0.9090189873417721 --time: 31.873271942138672\n",
      "train acc:  0.9617663595865231  train loss:  0.12330731259689805 --time: 28.046908617019653\n",
      "6 validation:  0.9108649789029536 --max 0.9108649789029536 --time: 32.18421244621277\n",
      "train acc:  0.9717572769343886  train loss:  0.09443576111154661 --time: 27.99241042137146\n",
      "7 validation:  0.9111286919831224 --max 0.9111286919831224 --time: 32.161280393600464\n",
      "train acc:  0.9772501189394922  train loss:  0.07437982165270089 --time: 27.844380617141724\n",
      "8 validation:  0.9029535864978903 --max 0.9111286919831224 --time: 31.96755599975586\n",
      "train acc:  0.9831754681890922  train loss:  0.06122487545713206 --time: 27.944780111312866\n",
      "9 validation:  0.9116561181434599 --max 0.9116561181434599 --time: 32.11265993118286\n",
      "train acc:  0.9867220275939622  train loss:  0.04715689299988318 --time: 27.920888900756836\n",
      "10 validation:  0.9100738396624473 --max 0.9116561181434599 --time: 32.112839460372925\n",
      "train acc:  0.9855975087582717  train loss:  0.05104058992004444 --time: 27.918935537338257\n",
      "11 validation:  0.9106012658227848 --max 0.9116561181434599 --time: 32.14533090591431\n",
      "train acc:  0.9849919986159769  train loss:  0.049663788407464715 --time: 27.871867418289185\n",
      "12 validation:  0.9084915611814346 --max 0.9116561181434599 --time: 32.06117796897888\n",
      "train acc:  0.9885818087452964  train loss:  0.04144361047640196 --time: 27.671454191207886\n",
      "13 validation:  0.9113924050632911 --max 0.9116561181434599 --time: 31.7830491065979\n",
      "train acc:  0.9925608753946629  train loss:  0.028860124967765906 --time: 27.88245391845703\n",
      "14 validation:  0.9113924050632911 --max 0.9116561181434599 --time: 32.0957236289978\n",
      "train acc:  0.9951559188616409  train loss:  0.01976856496828243 --time: 27.857946634292603\n",
      "15 validation:  0.9116561181434599 --max 0.9116561181434599 --time: 32.02536940574646\n",
      "train acc:  0.9972319536352234  train loss:  0.013841538652726217 --time: 27.974119901657104\n",
      "16 validation:  0.9092827004219409 --max 0.9116561181434599 --time: 32.171470403671265\n",
      "train acc:  0.9892738203364906  train loss:  0.03707681068872714 --time: 27.87305212020874\n",
      "17 validation:  0.9055907172995781 --max 0.9116561181434599 --time: 32.15408802032471\n",
      "train acc:  0.9799749145798192  train loss:  0.06136252452717137 --time: 27.90580725669861\n",
      "18 validation:  0.9042721518987342 --max 0.9116561181434599 --time: 32.19401717185974\n",
      "train acc:  0.9864625232472644  train loss:  0.043584492124452444 --time: 27.99382996559143\n",
      "19 validation:  0.9082278481012658 --max 0.9116561181434599 --time: 32.30518388748169\n",
      "train acc:  0.993728644954803  train loss:  0.02296654788715487 --time: 27.640108108520508\n",
      "20 validation:  0.9142932489451476 --max 0.9142932489451476 --time: 31.813968658447266\n",
      "train acc:  0.9975347087063708  train loss:  0.011624003674330715 --time: 28.047396183013916\n",
      "21 validation:  0.9135021097046413 --max 0.9142932489451476 --time: 32.17044162750244\n",
      "train acc:  0.9978374637775183  train loss:  0.008111812762507026 --time: 28.098475217819214\n",
      "22 validation:  0.915084388185654 --max 0.915084388185654 --time: 32.06859540939331\n",
      "train acc:  0.9974914579819212  train loss:  0.009388438910271623 --time: 28.15762758255005\n",
      "23 validation:  0.9113924050632911 --max 0.915084388185654 --time: 32.056429862976074\n",
      "train acc:  0.9961939362484321  train loss:  0.014130312303024556 --time: 28.131422758102417\n",
      "24 validation:  0.9071729957805907 --max 0.915084388185654 --time: 31.90790367126465\n",
      "train acc:  0.9785476406729813  train loss:  0.06569215944242658 --time: 28.26798415184021\n",
      "25 validation:  0.9005801687763713 --max 0.915084388185654 --time: 32.01306748390198\n",
      "train acc:  0.9785476406729813  train loss:  0.0640170175623334 --time: 28.42039918899536\n",
      "26 validation:  0.9100738396624473 --max 0.915084388185654 --time: 32.13542079925537\n",
      "train acc:  0.9941179014748497  train loss:  0.020728844383896137 --time: 28.46526789665222\n",
      "27 validation:  0.9116561181434599 --max 0.915084388185654 --time: 32.205331563949585\n",
      "train acc:  0.9966696942173782  train loss:  0.011148969710351724 --time: 28.280761003494263\n",
      "28 validation:  0.9121835443037974 --max 0.915084388185654 --time: 32.08036732673645\n",
      "train acc:  0.9973617058085723  train loss:  0.009137032570276224 --time: 28.068569660186768\n",
      "29 validation:  0.9127109704641351 --max 0.915084388185654 --time: 31.902353048324585\n",
      "train acc:  0.9978374637775183  train loss:  0.00647192746072812 --time: 28.006593704223633\n",
      "30 validation:  0.9171940928270043 --max 0.9171940928270043 --time: 32.02700638771057\n",
      "train acc:  0.9979672159508671  train loss:  0.006293082367937538 --time: 27.975231409072876\n",
      "31 validation:  0.9169303797468354 --max 0.9171940928270043 --time: 32.009584188461304\n",
      "train acc:  0.9984862246442628  train loss:  0.004986651025188943 --time: 27.93018102645874\n",
      "32 validation:  0.9156118143459916 --max 0.9171940928270043 --time: 31.938926935195923\n",
      "train acc:  0.9980969681242161  train loss:  0.005484711056178338 --time: 28.005348682403564\n",
      "33 validation:  0.9177215189873418 --max 0.9177215189873418 --time: 32.14932084083557\n",
      "train acc:  0.998226720297565  train loss:  0.005155716306296465 --time: 27.975757360458374\n",
      "34 validation:  0.9171940928270043 --max 0.9177215189873418 --time: 32.083547592163086\n",
      "train acc:  0.9981834695731153  train loss:  0.004877909869955755 --time: 27.729778051376343\n",
      "35 validation:  0.9182489451476793 --max 0.9182489451476793 --time: 31.962474584579468\n",
      "train acc:  0.9983997231953635  train loss:  0.004716263259104994 --time: 27.940415859222412\n",
      "36 validation:  0.9185126582278481 --max 0.9185126582278481 --time: 32.337870359420776\n",
      "train acc:  0.9981834695731153  train loss:  0.004894708040727043 --time: 27.7168550491333\n",
      "37 validation:  0.917457805907173 --max 0.9185126582278481 --time: 32.024954080581665\n",
      "train acc:  0.9983564724709139  train loss:  0.0042326395077153635 --time: 27.720020532608032\n",
      "38 validation:  0.9169303797468354 --max 0.9185126582278481 --time: 32.03652000427246\n",
      "train acc:  0.9981402188486657  train loss:  0.005024640490226184 --time: 27.76486611366272\n",
      "39 validation:  0.9171940928270043 --max 0.9185126582278481 --time: 32.077518463134766\n",
      "train acc:  0.9967129449418278  train loss:  0.010106218340890276 --time: 28.009937047958374\n",
      "40 validation:  0.8960970464135021 --max 0.9185126582278481 --time: 32.32767605781555\n",
      "train acc:  0.9348644089788504  train loss:  0.19428465177835022 --time: 27.78740429878235\n",
      "41 validation:  0.9029535864978903 --max 0.9185126582278481 --time: 32.15733337402344\n",
      "train acc:  0.9830457160157433  train loss:  0.049943186054572215 --time: 27.878973722457886\n",
      "42 validation:  0.9121835443037974 --max 0.9185126582278481 --time: 32.11918330192566\n",
      "train acc:  0.9943774058215475  train loss:  0.019616202751009917 --time: 27.926210641860962\n",
      "43 validation:  0.9140295358649789 --max 0.9185126582278481 --time: 32.159724712371826\n",
      "train acc:  0.9975779594308205  train loss:  0.008207008654884582 --time: 27.820876598358154\n",
      "44 validation:  0.9153481012658228 --max 0.9185126582278481 --time: 32.14701008796692\n",
      "train acc:  0.998572726093162  train loss:  0.0049564390917931226 --time: 27.730159282684326\n",
      "45 validation:  0.9187763713080169 --max 0.9187763713080169 --time: 31.950693130493164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9981402188486657  train loss:  0.005419900119549065 --time: 27.85844087600708\n",
      "46 validation:  0.9190400843881856 --max 0.9190400843881856 --time: 32.12855648994446\n",
      "train acc:  0.9980969681242161  train loss:  0.004857082000305845 --time: 27.706533908843994\n",
      "47 validation:  0.9185126582278481 --max 0.9190400843881856 --time: 31.95323657989502\n",
      "train acc:  0.9982699710220146  train loss:  0.004704034146151726 --time: 27.749624490737915\n",
      "48 validation:  0.9203586497890295 --max 0.9203586497890295 --time: 32.10962414741516\n",
      "train acc:  0.9983564724709139  train loss:  0.004446415509128151 --time: 27.93761396408081\n",
      "49 validation:  0.9182489451476793 --max 0.9203586497890295 --time: 32.20478010177612\n",
      "train acc:  0.9982699710220146  train loss:  0.004643171800648439 --time: 27.903313159942627\n",
      "50 validation:  0.9208860759493671 --max 0.9208860759493671 --time: 32.13633060455322\n",
      "train acc:  0.9983997231953635  train loss:  0.004293776907008115 --time: 28.04420566558838\n",
      "51 validation:  0.9203586497890295 --max 0.9208860759493671 --time: 32.29238486289978\n",
      "train acc:  0.9983564724709139  train loss:  0.004501529575247724 --time: 27.953763008117676\n",
      "52 validation:  0.9211497890295358 --max 0.9211497890295358 --time: 32.067326068878174\n",
      "train acc:  0.9984429739198132  train loss:  0.0045100731066221025 --time: 28.128365516662598\n",
      "53 validation:  0.9185126582278481 --max 0.9211497890295358 --time: 32.22238206863403\n",
      "train acc:  0.9980104666753168  train loss:  0.005333077609030921 --time: 28.066363096237183\n",
      "54 validation:  0.9187763713080169 --max 0.9211497890295358 --time: 32.15774083137512\n",
      "train acc:  0.9974914579819212  train loss:  0.007175862942552143 --time: 28.05768132209778\n",
      "55 validation:  0.9092827004219409 --max 0.9211497890295358 --time: 32.05861306190491\n",
      "train acc:  0.9639288958090048  train loss:  0.10709339501181854 --time: 28.14028263092041\n",
      "56 validation:  0.9005801687763713 --max 0.9211497890295358 --time: 32.17950916290283\n",
      "train acc:  0.9796289087842222  train loss:  0.06084496046040845 --time: 28.186813592910767\n",
      "57 validation:  0.9095464135021097 --max 0.9211497890295358 --time: 32.20206689834595\n",
      "train acc:  0.9927771290169111  train loss:  0.022516717843567305 --time: 27.91233491897583\n",
      "58 validation:  0.9084915611814346 --max 0.9211497890295358 --time: 31.969040632247925\n",
      "train acc:  0.997750962328619  train loss:  0.007998263299689005 --time: 28.302011489868164\n",
      "59 validation:  0.9195675105485233 --max 0.9211497890295358 --time: 32.41602921485901\n",
      "train acc:  0.9983132217464643  train loss:  0.00497741557082717 --time: 28.303298711776733\n",
      "60 validation:  0.9211497890295358 --max 0.9211497890295358 --time: 32.2760751247406\n",
      "train acc:  0.998226720297565  train loss:  0.004520006678102523 --time: 28.13390874862671\n",
      "61 validation:  0.9214135021097046 --max 0.9214135021097046 --time: 32.20404243469238\n",
      "train acc:  0.998226720297565  train loss:  0.004430214927106638 --time: 28.37202215194702\n",
      "62 validation:  0.9216772151898734 --max 0.9216772151898734 --time: 32.46389365196228\n",
      "train acc:  0.9982699710220146  train loss:  0.004566644005334642 --time: 28.058217763900757\n",
      "63 validation:  0.9195675105485233 --max 0.9216772151898734 --time: 32.16855597496033\n",
      "train acc:  0.9982699710220146  train loss:  0.004249548396814064 --time: 28.23444414138794\n",
      "64 validation:  0.9208860759493671 --max 0.9216772151898734 --time: 32.38859415054321\n",
      "train acc:  0.9984862246442628  train loss:  0.004401209371200459 --time: 27.841378688812256\n",
      "65 validation:  0.9208860759493671 --max 0.9216772151898734 --time: 32.02759790420532\n",
      "train acc:  0.9981834695731153  train loss:  0.00466020070590421 --time: 28.059709787368774\n",
      "66 validation:  0.9219409282700421 --max 0.9219409282700421 --time: 32.24915361404419\n",
      "train acc:  0.9982699710220146  train loss:  0.004293130536339225 --time: 28.001315116882324\n",
      "67 validation:  0.9235232067510548 --max 0.9235232067510548 --time: 32.245617151260376\n",
      "train acc:  0.9980969681242161  train loss:  0.004286560095452385 --time: 27.968684196472168\n",
      "68 validation:  0.9219409282700421 --max 0.9235232067510548 --time: 32.161473512649536\n",
      "train acc:  0.9982699710220146  train loss:  0.004379505521678289 --time: 27.972851991653442\n",
      "69 validation:  0.9219409282700421 --max 0.9235232067510548 --time: 32.27814435958862\n",
      "train acc:  0.9980537173997664  train loss:  0.004877245719660361 --time: 27.87209391593933\n",
      "70 validation:  0.9219409282700421 --max 0.9235232067510548 --time: 31.985368967056274\n",
      "train acc:  0.9981834695731153  train loss:  0.004542947240712157 --time: 28.08209228515625\n",
      "71 validation:  0.9224683544303798 --max 0.9235232067510548 --time: 32.34962749481201\n",
      "train acc:  0.9983132217464643  train loss:  0.00405309865015137 --time: 27.958121299743652\n",
      "72 validation:  0.9211497890295358 --max 0.9235232067510548 --time: 32.14201474189758\n",
      "train acc:  0.998226720297565  train loss:  0.004024276580389471 --time: 28.118065118789673\n",
      "73 validation:  0.9216772151898734 --max 0.9235232067510548 --time: 32.20723056793213\n",
      "train acc:  0.9982699710220146  train loss:  0.004202644683749322 --time: 28.135798692703247\n",
      "74 validation:  0.9227320675105485 --max 0.9235232067510548 --time: 32.21817064285278\n",
      "train acc:  0.9981402188486657  train loss:  0.004743536568890925 --time: 28.064635038375854\n",
      "75 validation:  0.9211497890295358 --max 0.9235232067510548 --time: 32.08430600166321\n",
      "train acc:  0.9627611262488647  train loss:  0.11360735317675438 --time: 28.22772455215454\n",
      "76 validation:  0.8987341772151899 --max 0.9235232067510548 --time: 32.337289571762085\n",
      "train acc:  0.9794559058864236  train loss:  0.05976587830327492 --time: 27.944846868515015\n",
      "77 validation:  0.9108649789029536 --max 0.9235232067510548 --time: 32.13010025024414\n",
      "train acc:  0.995026166688292  train loss:  0.01811756580574078 --time: 28.01385760307312\n",
      "78 validation:  0.9116561181434599 --max 0.9235232067510548 --time: 32.09120774269104\n",
      "train acc:  0.9974914579819212  train loss:  0.007755611431128886 --time: 27.859238862991333\n",
      "79 validation:  0.917457805907173 --max 0.9235232067510548 --time: 31.999033451080322\n",
      "train acc:  0.9980969681242161  train loss:  0.005205152332634814 --time: 28.109387159347534\n",
      "80 validation:  0.9177215189873418 --max 0.9235232067510548 --time: 32.15825176239014\n",
      "train acc:  0.998226720297565  train loss:  0.004655047316613571 --time: 28.14457654953003\n",
      "81 validation:  0.9187763713080169 --max 0.9235232067510548 --time: 32.24203014373779\n",
      "train acc:  0.9982699710220146  train loss:  0.004367737010930464 --time: 27.957114219665527\n",
      "82 validation:  0.9190400843881856 --max 0.9235232067510548 --time: 32.02047610282898\n",
      "train acc:  0.9983997231953635  train loss:  0.004051767363362864 --time: 28.16526198387146\n",
      "83 validation:  0.9171940928270043 --max 0.9235232067510548 --time: 32.22415566444397\n",
      "train acc:  0.9982699710220146  train loss:  0.004190780838259408 --time: 28.065967798233032\n",
      "84 validation:  0.917457805907173 --max 0.9235232067510548 --time: 32.04458999633789\n",
      "train acc:  0.9984862246442628  train loss:  0.003963934429837597 --time: 28.223062992095947\n",
      "85 validation:  0.9185126582278481 --max 0.9235232067510548 --time: 32.242443323135376\n",
      "train acc:  0.998226720297565  train loss:  0.003952235684750051 --time: 28.302561044692993\n",
      "86 validation:  0.9164029535864979 --max 0.9235232067510548 --time: 32.33012533187866\n",
      "train acc:  0.998226720297565  train loss:  0.004346381315899735 --time: 28.14987826347351\n",
      "87 validation:  0.9177215189873418 --max 0.9235232067510548 --time: 32.19824552536011\n",
      "train acc:  0.9983132217464643  train loss:  0.00420247936764094 --time: 28.134413957595825\n",
      "88 validation:  0.9185126582278481 --max 0.9235232067510548 --time: 32.27686047554016\n",
      "train acc:  0.9981402188486657  train loss:  0.004382480211084508 --time: 28.128904819488525\n",
      "89 validation:  0.9193037974683544 --max 0.9235232067510548 --time: 32.22956323623657\n",
      "train acc:  0.9984862246442628  train loss:  0.003694553367098284 --time: 27.962377548217773\n",
      "90 validation:  0.9185126582278481 --max 0.9235232067510548 --time: 32.05187797546387\n",
      "train acc:  0.9981834695731153  train loss:  0.004415289715898174 --time: 28.00038504600525\n",
      "91 validation:  0.9193037974683544 --max 0.9235232067510548 --time: 32.02284908294678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9982699710220146  train loss:  0.00422287223411522 --time: 28.107422590255737\n",
      "92 validation:  0.9208860759493671 --max 0.9235232067510548 --time: 32.43273878097534\n",
      "train acc:  0.9983997231953635  train loss:  0.003959220845171235 --time: 27.89655303955078\n",
      "93 validation:  0.9195675105485233 --max 0.9235232067510548 --time: 32.143205404281616\n",
      "train acc:  0.9983997231953635  train loss:  0.0040623122041621145 --time: 27.96421480178833\n",
      "94 validation:  0.9187763713080169 --max 0.9235232067510548 --time: 32.15187454223633\n",
      "train acc:  0.9984429739198132  train loss:  0.003822343873062032 --time: 28.197158575057983\n",
      "95 validation:  0.9214135021097046 --max 0.9235232067510548 --time: 32.38037467002869\n",
      "train acc:  0.9985294753687124  train loss:  0.0036630684719549606 --time: 27.909326553344727\n",
      "96 validation:  0.917457805907173 --max 0.9235232067510548 --time: 32.11670541763306\n",
      "train acc:  0.9980969681242161  train loss:  0.00406158097217053 --time: 28.196134567260742\n",
      "97 validation:  0.9195675105485233 --max 0.9235232067510548 --time: 32.418906927108765\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 lstm layer, 512, 2CNN context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
