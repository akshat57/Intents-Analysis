{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=256, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 256)\n",
       "  (linear): Linear(in_features=256, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5485056874702652  train loss:  1.488811171845178 --time: 2.7095887660980225\n",
      "validation:  0.8022151898734177 --max 0.8022151898734177 --time: 3.0723462104797363\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
