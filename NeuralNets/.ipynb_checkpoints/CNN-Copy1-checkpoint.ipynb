{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents/Intents-Analysis/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining constants and labels\n",
    "max_sent_len = {'english': 247, 'hindi': 265, 'gujarati': 283, 'bengali': 295, 'marathi': 307}\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "language = 'english'\n",
    "\n",
    "#Loading data\n",
    "train_file = '../Analysis/Labels/TaskMaster/taskmaster_training_' + language + '.pkl'\n",
    "test_file = '../Analysis/Labels/TaskMaster/taskmaster_testing_' + language + '.pkl'\n",
    "feature_file = '../Analysis/Labels/TaskMaster/panphon_features_' + language + '.pkl'\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "test_data = load_data(test_file)\n",
    "feature_vectors = load_data(feature_file)\n",
    "\n",
    "\n",
    "#Add vector for padding, converting feature vectors to float tensors. \n",
    "size_of_feature_vector = 22\n",
    "feature_vectors['unk'] = np.zeros(size_of_feature_vector)\n",
    "for ipa in feature_vectors:\n",
    "    feature_vectors[ipa] = torch.from_numpy(feature_vectors[ipa]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vector(utterance, feature_vectors, max_len):\n",
    "    '''\n",
    "    Pad sentence at the end with maximum length with 'unk' \n",
    "    '''\n",
    "    input_vector = feature_vectors[utterance[0]].reshape(-1,1)\n",
    "    for ipa in utterance[1:]:\n",
    "        input_vector = torch.cat((input_vector, feature_vectors[ipa].reshape(-1,1)), dim = 1)\n",
    "    \n",
    "    for i in range(max_len - len(utterance)):\n",
    "        input_vector = torch.cat((input_vector, feature_vectors['unk'].reshape(-1,1)), dim = 1)\n",
    "        \n",
    "    return input_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, feature_vectors, intent_labels, max_len, train = True):\n",
    "        self.all_data = []\n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                input_vector = create_input_vector(utterance,feature_vectors, max_len)\n",
    "                self.all_data.append([input_vector, intent_labels[intent]])\n",
    "        \n",
    "        if train:\n",
    "            random.shuffle(self.all_data)\n",
    "\n",
    "        self.dim1 = self.all_data[0][0].shape[0]\n",
    "        self.dim2 = self.all_data[0][0].shape[1]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        \n",
    "        return self.all_data[index][0].reshape(1, self.dim1, self.dim2 ), self.all_data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data, feature_vectors, intent_labels, max_sent_len[language], train=True)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "test_dataset = MyDataset(test_data, feature_vectors, intent_labels, max_sent_len[language], train=False)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, **test_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([128, 1, 22, 247])\n",
      "torch.Size([127, 1, 22, 247])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (1,3)) # (22,245) 265\n",
    "        self.pool1 = nn.MaxPool2d( kernel_size = (1,3), stride = (1,2)) #(22, 122) 132\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,7)) # (20,116) 126\n",
    "        self.pool2 = nn.MaxPool2d( kernel_size = (3,4), stride = (1,2)) #(18, 57) 63\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 32, (5,11)) # (14, 47) 53\n",
    "        self.pool3 = nn.MaxPool2d( kernel_size = (3,5), stride = (1,2)) #(12, 22) 25\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 12 * 22, 512)\n",
    "        self.fc1_bn = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc2_bn = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 32 *12 * 22)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN_Model(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(1, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 3), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 7), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 4), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 32, kernel_size=(5, 11), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 5), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=8448, out_features=512, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (fc2_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNN_Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "    \n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')  \n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  1.7069410189338352 Time:  2.5927205085754395 s\n",
      "Training Accuracy:  30.479102956167175 %\n",
      "Testing Loss:  2.0727489391962686\n",
      "Testing Accuracy:  27.666666666666668 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.5419570466746455 Time:  2.5049312114715576 s\n",
      "Training Accuracy:  37.58069996602107 %\n",
      "Testing Loss:  1.778596043586731\n",
      "Testing Accuracy:  29.666666666666668 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.3211995518725852 Time:  2.50978422164917 s\n",
      "Training Accuracy:  46.279306829765545 %\n",
      "Testing Loss:  1.679516037305196\n",
      "Testing Accuracy:  36.666666666666664 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.3015844614609429 Time:  2.509868860244751 s\n",
      "Training Accuracy:  46.958885490995584 %\n",
      "Testing Loss:  1.5706644852956135\n",
      "Testing Accuracy:  45.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.1163942088251528 Time:  2.5245471000671387 s\n",
      "Training Accuracy:  57.118586476384635 %\n",
      "Testing Loss:  1.4225377639134724\n",
      "Testing Accuracy:  51.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.9764296049657075 Time:  2.5175387859344482 s\n",
      "Training Accuracy:  62.24940536867143 %\n",
      "Testing Loss:  1.3451929489771526\n",
      "Testing Accuracy:  52.666666666666664 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.8748737236727839 Time:  2.5250184535980225 s\n",
      "Training Accuracy:  65.91913013931364 %\n",
      "Testing Loss:  1.2210729718208313\n",
      "Testing Accuracy:  55.333333333333336 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.8003667307936627 Time:  2.533393621444702 s\n",
      "Training Accuracy:  68.36561331974175 %\n",
      "Testing Loss:  1.0953891674677532\n",
      "Testing Accuracy:  59.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.7515478445136029 Time:  2.5478875637054443 s\n",
      "Training Accuracy:  70.43832823649338 %\n",
      "Testing Loss:  1.1060173312822978\n",
      "Testing Accuracy:  59.333333333333336 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.7057098435318988 Time:  2.540553569793701 s\n",
      "Training Accuracy:  71.35575942915392 %\n",
      "Testing Loss:  1.0413705706596375\n",
      "Testing Accuracy:  60.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.6237732120182203 Time:  2.5666468143463135 s\n",
      "Training Accuracy:  75.05946313285763 %\n",
      "Testing Loss:  1.0397141575813293\n",
      "Testing Accuracy:  64.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.5880808130554531 Time:  2.543501853942871 s\n",
      "Training Accuracy:  76.3506625891947 %\n",
      "Testing Loss:  1.164972146352132\n",
      "Testing Accuracy:  61.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.5492100819297459 Time:  2.554452657699585 s\n",
      "Training Accuracy:  76.99626231736325 %\n",
      "Testing Loss:  1.0186558763186138\n",
      "Testing Accuracy:  67.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.5044464585573777 Time:  2.554706573486328 s\n",
      "Training Accuracy:  79.7825348284064 %\n",
      "Testing Loss:  1.2759948472181957\n",
      "Testing Accuracy:  63.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.4549761155377264 Time:  2.5509800910949707 s\n",
      "Training Accuracy:  81.8212708120965 %\n",
      "Testing Loss:  1.0452574094136555\n",
      "Testing Accuracy:  68.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.40967829460683075 Time:  2.551352024078369 s\n",
      "Training Accuracy:  84.23377505946313 %\n",
      "Testing Loss:  1.051918625831604\n",
      "Testing Accuracy:  69.33333333333334 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.39558017966539966 Time:  2.570990562438965 s\n",
      "Training Accuracy:  84.74345905538566 %\n",
      "Testing Loss:  0.8544443845748901\n",
      "Testing Accuracy:  69.66666666666667 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.32469876823217975 Time:  2.5607361793518066 s\n",
      "Training Accuracy:  87.32585796805981 %\n",
      "Testing Loss:  1.1054930488268535\n",
      "Testing Accuracy:  73.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.27826229968796606 Time:  2.565232515335083 s\n",
      "Training Accuracy:  89.70438328236493 %\n",
      "Testing Loss:  1.072106420993805\n",
      "Testing Accuracy:  71.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.24433439646078192 Time:  2.5788354873657227 s\n",
      "Training Accuracy:  90.41794087665647 %\n",
      "Testing Loss:  1.2600476145744324\n",
      "Testing Accuracy:  71.33333333333334 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.21221136074999106 Time:  2.5694994926452637 s\n",
      "Training Accuracy:  91.91301393136256 %\n",
      "Testing Loss:  1.189354399840037\n",
      "Testing Accuracy:  71.66666666666667 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.1888459739477738 Time:  2.570026397705078 s\n",
      "Training Accuracy:  92.7624872579001 %\n",
      "Testing Loss:  1.5992970665295918\n",
      "Testing Accuracy:  70.33333333333334 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.17226328959931497 Time:  2.594454288482666 s\n",
      "Training Accuracy:  93.64593951749916 %\n",
      "Testing Loss:  1.8393173813819885\n",
      "Testing Accuracy:  69.66666666666667 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.14167445314967114 Time:  2.567946672439575 s\n",
      "Training Accuracy:  94.97111790689773 %\n",
      "Testing Loss:  1.441708544890086\n",
      "Testing Accuracy:  73.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.1277400825334632 Time:  2.5670082569122314 s\n",
      "Training Accuracy:  95.37886510363575 %\n",
      "Testing Loss:  1.5166808366775513\n",
      "Testing Accuracy:  74.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.11923035898286363 Time:  2.5776355266571045 s\n",
      "Training Accuracy:  96.1264016309888 %\n",
      "Testing Loss:  1.5428278644879658\n",
      "Testing Accuracy:  73.66666666666667 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.10821793212190918 Time:  2.58345627784729 s\n",
      "Training Accuracy:  96.2623173632348 %\n",
      "Testing Loss:  1.9232691725095112\n",
      "Testing Accuracy:  73.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  0.08055081882554552 Time:  2.58798885345459 s\n",
      "Training Accuracy:  97.31566428814136 %\n",
      "Testing Loss:  1.6765807072321575\n",
      "Testing Accuracy:  75.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, cooldown=5)\n",
    "\n",
    "for i in range(50):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "    Train_loss.append(train_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "\n",
    "    #scheduler.step(test_acc)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('Learning rate:', param_group['lr'])\n",
    "    \n",
    "\n",
    "    print('='*20)\n",
    "    \n",
    "print(max(Test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
