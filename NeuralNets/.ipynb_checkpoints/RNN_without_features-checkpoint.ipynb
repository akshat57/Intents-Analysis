{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents/Intents-Analysis/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=16481628160, available=15663706112, percent=5.0, used=524664832, free=14307622912, active=1249918976, inactive=679854080, buffers=129814528, cached=1519525888, shared=819200, slab=141631488)\n"
     ]
    }
   ],
   "source": [
    "print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining constants and labels\n",
    "max_sent_len = {'english': 247, 'hindi': 265, 'gujarati': 283, 'bengali': 295, 'marathi': 307}\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "language = 'english'\n",
    "\n",
    "#Loading data\n",
    "data_file = '../Analysis/Labels/TaskMaster/data_taskmaster_' + language + '.pkl'\n",
    "train_file = '../Analysis/Labels/TaskMaster/taskmaster_training_' + language + '.pkl'\n",
    "test_file = '../Analysis/Labels/TaskMaster/taskmaster_testing_' + language + '.pkl'\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "test_data = load_data(test_file)\n",
    "vocab, _ = get_vocab(1, data_file)\n",
    "\n",
    "#creating vocabulary dictionary\n",
    "my_vocab = {}\n",
    "for i, phone in enumerate(vocab):\n",
    "    my_vocab[phone] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_pad_end(utterance, my_vocab, max_len):\n",
    "    '''\n",
    "    Pad sentence at the end with maximum length with index max_len \n",
    "    '''\n",
    "    input_vector = []\n",
    "    for ipa in utterance:\n",
    "        input_vector.append(my_vocab[ipa])\n",
    "    \n",
    "    for i in range(max_len - len(utterance)):\n",
    "        input_vector.append(len(my_vocab))\n",
    "        \n",
    "    return input_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, my_vocab, intent_labels, max_len, train = True):\n",
    "        self.all_data = []\n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                input_vector = create_input_pad_end(utterance,my_vocab, max_len)\n",
    "                self.all_data.append([torch.from_numpy(np.array(input_vector)).float(), intent_labels[intent]])\n",
    "        \n",
    "        if train:\n",
    "            random.shuffle(self.all_data)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        \n",
    "        return self.all_data[index][0], self.all_data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data, my_vocab, intent_labels, max_sent_len[language], train=True)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "test_dataset = MyDataset(test_data, my_vocab, intent_labels, max_sent_len[language], train=False)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, **test_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader),len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 247])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN_Model(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.word2wemb = nn.Embedding(voca_size, rnn_in_dim)\n",
    "        self.rnn = nn.LSTM(rnn_in_dim, rnn_hid_dim, num_layers = 2, bidirectional = True)\n",
    "        self.rnn2logit = nn.Linear(2* rnn_hid_dim, 3)\n",
    "\n",
    "    def init_rnn_hid(self):\n",
    "        \"\"\"Initial hidden state.\"\"\"\n",
    "        return torch.zeros(1, 1, self.rnn_hid_dim)\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"Feeds the words into the neural network and returns the value\n",
    "        of the output layer.\"\"\"\n",
    "        wembs = self.word2wemb(words) # (seq_len, rnn_in_dim)\n",
    "        rnn_outs, _ = self.rnn(wembs.unsqueeze(1))\n",
    "                                      # (seq_len, 1, rnn_hid_dim)\n",
    "        logit = self.rnn2logit(rnn_outs[-1]) # (1 x 3)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN_Model(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(1, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 3), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 7), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(3, 4), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 64, kernel_size=(5, 11), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 5), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=16896, out_features=512, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (fc2_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNN_Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "    \n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')  \n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  1.7394491071286409 Time:  6.356580972671509 s\n",
      "Training Accuracy:  27.149167516139993 %\n",
      "Testing Loss:  1.9255032142003377\n",
      "Testing Accuracy:  27.0 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.6468719451323799 Time:  6.402014255523682 s\n",
      "Training Accuracy:  34.4206591913014 %\n",
      "Testing Loss:  1.9241166512171428\n",
      "Testing Accuracy:  26.666666666666668 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.6134489360062971 Time:  6.512433290481567 s\n",
      "Training Accuracy:  35.23615358477743 %\n",
      "Testing Loss:  1.774964173634847\n",
      "Testing Accuracy:  28.000000000000004 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.616716628489287 Time:  6.579664707183838 s\n",
      "Training Accuracy:  35.91573224600747 %\n",
      "Testing Loss:  1.7613558371861775\n",
      "Testing Accuracy:  27.666666666666668 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.565118318018706 Time:  6.610743522644043 s\n",
      "Training Accuracy:  38.70200475705063 %\n",
      "Testing Loss:  1.8216347694396973\n",
      "Testing Accuracy:  32.33333333333333 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.3078618256942085 Time:  6.592539310455322 s\n",
      "Training Accuracy:  47.09480122324159 %\n",
      "Testing Loss:  1.5197852452596028\n",
      "Testing Accuracy:  38.666666666666664 %\n",
      "Learning rate: 0.001\n",
      "====================\n",
      "Training Loss:  1.1753989198933477 Time:  6.567767858505249 s\n",
      "Training Accuracy:  52.83724091063541 %\n",
      "Testing Loss:  1.4462201595306396\n",
      "Testing Accuracy:  47.66666666666667 %\n",
      "Learning rate: 0.001\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, cooldown=3)\n",
    " \n",
    "for i in range(30):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "    Train_loss.append(train_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "\n",
    "    #scheduler.step(test_acc)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('Learning rate:', param_group['lr'])\n",
    "    \n",
    "\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.array([1,2,3,])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
