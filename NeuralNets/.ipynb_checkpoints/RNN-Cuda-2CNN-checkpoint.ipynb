{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.29323819232076115  train loss:  1.6929825181546418 --time: 1.196711540222168\n",
      "validation:  0.25333333333333335 --max 0.25333333333333335 --time: 1.3875484466552734\n",
      "train acc:  0.3944954128440367  train loss:  1.5717243111651877 --time: 0.9537580013275146\n",
      "validation:  0.36 --max 0.36 --time: 1.1417338848114014\n",
      "train acc:  0.5321100917431193  train loss:  1.3043683715488599 --time: 0.9610955715179443\n",
      "validation:  0.4666666666666667 --max 0.4666666666666667 --time: 1.1508233547210693\n",
      "train acc:  0.6377845735643901  train loss:  1.0180131482041401 --time: 0.9666135311126709\n",
      "validation:  0.57 --max 0.57 --time: 1.1554412841796875\n",
      "train acc:  0.7193340129119946  train loss:  0.8062165172203727 --time: 0.9638862609863281\n",
      "validation:  0.6533333333333333 --max 0.6533333333333333 --time: 1.1578176021575928\n",
      "train acc:  0.7886510363574584  train loss:  0.6198414240194403 --time: 0.9561073780059814\n",
      "validation:  0.6833333333333333 --max 0.6833333333333333 --time: 1.143336534500122\n",
      "train acc:  0.8260278627251104  train loss:  0.49643340965975885 --time: 0.9592466354370117\n",
      "validation:  0.7766666666666666 --max 0.7766666666666666 --time: 1.146017074584961\n",
      "train acc:  0.8769962623173633  train loss:  0.36050348242987756 --time: 0.9557487964630127\n",
      "validation:  0.7933333333333333 --max 0.7933333333333333 --time: 1.1476624011993408\n",
      "train acc:  0.9075773020727149  train loss:  0.2744504874167235 --time: 0.9541041851043701\n",
      "validation:  0.82 --max 0.82 --time: 1.1414563655853271\n",
      "train acc:  0.909616038056405  train loss:  0.24724166354407434 --time: 0.9622786045074463\n",
      "validation:  0.8366666666666667 --max 0.8366666666666667 --time: 1.1489732265472412\n",
      "train acc:  0.9235474006116208  train loss:  0.22908427987409674 --time: 0.9618322849273682\n",
      "validation:  0.82 --max 0.8366666666666667 --time: 1.1503570079803467\n",
      "train acc:  0.9503907577302073  train loss:  0.15925647415544675 --time: 0.9662230014801025\n",
      "validation:  0.85 --max 0.85 --time: 1.1573781967163086\n",
      "train acc:  0.963982330954808  train loss:  0.11920472736591878 --time: 0.966064453125\n",
      "validation:  0.87 --max 0.87 --time: 1.166694164276123\n",
      "train acc:  0.9734964322120285  train loss:  0.09456600864296374 --time: 0.9780046939849854\n",
      "validation:  0.85 --max 0.87 --time: 1.1702632904052734\n",
      "train acc:  0.9772341148487937  train loss:  0.07765303163424782 --time: 0.9584054946899414\n",
      "validation:  0.8766666666666667 --max 0.8766666666666667 --time: 1.1511363983154297\n",
      "train acc:  0.9867482161060143  train loss:  0.05537567603523317 --time: 0.9656562805175781\n",
      "validation:  0.87 --max 0.8766666666666667 --time: 1.1622405052185059\n",
      "train acc:  0.9894665307509344  train loss:  0.039469380903503166 --time: 0.9525401592254639\n",
      "validation:  0.8566666666666667 --max 0.8766666666666667 --time: 1.1530828475952148\n",
      "train acc:  0.9898063200815495  train loss:  0.039018731645267944 --time: 0.958798885345459\n",
      "validation:  0.8833333333333333 --max 0.8833333333333333 --time: 1.1468243598937988\n",
      "train acc:  0.9785932721712538  train loss:  0.07372116775292417 --time: 0.9664294719696045\n",
      "validation:  0.8533333333333334 --max 0.8833333333333333 --time: 1.1594789028167725\n",
      "train acc:  0.9721372748895685  train loss:  0.0916197873328043 --time: 0.9538898468017578\n",
      "validation:  0.8533333333333334 --max 0.8833333333333333 --time: 1.1391468048095703\n",
      "train acc:  0.9891267414203194  train loss:  0.043080969754120575 --time: 0.9561030864715576\n",
      "validation:  0.8633333333333333 --max 0.8833333333333333 --time: 1.1444575786590576\n",
      "train acc:  0.9966021066938499  train loss:  0.020835938620502533 --time: 0.9417567253112793\n",
      "validation:  0.88 --max 0.8833333333333333 --time: 1.1347217559814453\n",
      "train acc:  0.9762147468569486  train loss:  0.08973982661922017 --time: 0.9605667591094971\n",
      "validation:  0.8566666666666667 --max 0.8833333333333333 --time: 1.1486742496490479\n",
      "train acc:  0.9819911654774041  train loss:  0.060507489449304085 --time: 0.95768141746521\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 1.1448493003845215\n",
      "train acc:  0.9925246347264696  train loss:  0.027162001326518213 --time: 0.9559476375579834\n",
      "validation:  0.8766666666666667 --max 0.8833333333333333 --time: 1.1494815349578857\n",
      "train acc:  0.99932042133877  train loss:  0.010580167397046867 --time: 0.9606606960296631\n",
      "validation:  0.8733333333333333 --max 0.8833333333333333 --time: 1.1502211093902588\n",
      "train acc:  1.0  train loss:  0.004829480748056718 --time: 0.9499657154083252\n",
      "validation:  0.8933333333333333 --max 0.8933333333333333 --time: 1.1376621723175049\n",
      "train acc:  1.0  train loss:  0.002767434976387607 --time: 0.950005292892456\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 1.1387059688568115\n",
      "train acc:  1.0  train loss:  0.002092950299139256 --time: 0.9699196815490723\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 1.1639764308929443\n",
      "train acc:  1.0  train loss:  0.0017160441139308007 --time: 0.965895414352417\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 1.1522982120513916\n",
      "train acc:  1.0  train loss:  0.001457360927420466 --time: 0.9629902839660645\n",
      "validation:  0.8866666666666667 --max 0.8933333333333333 --time: 1.153411626815796\n",
      "train acc:  1.0  train loss:  0.0013338374404196181 --time: 0.9716763496398926\n",
      "validation:  0.89 --max 0.8933333333333333 --time: 1.163456916809082\n",
      "train acc:  1.0  train loss:  0.0011590761436230462 --time: 0.9626510143280029\n",
      "validation:  0.8966666666666666 --max 0.8966666666666666 --time: 1.153989315032959\n",
      "train acc:  1.0  train loss:  0.0010485476054980056 --time: 0.9633533954620361\n",
      "validation:  0.9033333333333333 --max 0.9033333333333333 --time: 1.149949073791504\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
