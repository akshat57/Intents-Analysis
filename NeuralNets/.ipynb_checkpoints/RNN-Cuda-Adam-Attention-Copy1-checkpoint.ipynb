{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "        embeddings = input.clone()##saved for attention\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        _, (hn, cn) = self.lstm(pack_tensor)\n",
    "        \n",
    "        #doing attention\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        batch_size = hn[-1].shape[0]\n",
    "        emb_size = embeddings.shape[1]\n",
    "        seq_len = embeddings.shape[2]\n",
    "\n",
    "        attention = torch.zeros([batch_size, seq_len]).to(device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            attention[:,t] = torch.sigmoid(cos(embeddings[:,:,t], hn[-1]))\n",
    "            \n",
    "        normalize = torch.sum(attention, dim=1).reshape(-1,1)\n",
    "        attention = attention/normalize\n",
    "\n",
    "        \n",
    "        output = torch.zeros([batch_size, emb_size]).to(device)\n",
    "        for t in range(seq_len):\n",
    "            weights = attention[:,t].reshape(-1,1)\n",
    "            output += weights*embeddings[:,:,t]\n",
    "        \n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.38328236493374107  train loss:  0.4049745927686277 --time: 19.1665780544281\n",
      "validation:  0.28 --max 0.28 --time: 20.31694221496582\n",
      "train acc:  0.40095141012572205  train loss:  0.40102275558139966 --time: 19.793357372283936\n",
      "validation:  0.2866666666666667 --max 0.2866666666666667 --time: 21.03965973854065\n",
      "train acc:  0.4016309887869521  train loss:  0.39636705621429114 --time: 18.5506591796875\n",
      "validation:  0.2833333333333333 --max 0.2866666666666667 --time: 19.757171630859375\n",
      "train acc:  0.4074074074074074  train loss:  0.39093967624332593 --time: 19.440757274627686\n",
      "validation:  0.2866666666666667 --max 0.2866666666666667 --time: 20.56821608543396\n",
      "train acc:  0.42201834862385323  train loss:  0.38684572732966876 --time: 19.207708597183228\n",
      "validation:  0.29 --max 0.29 --time: 20.49865436553955\n",
      "train acc:  0.4233775059463133  train loss:  0.3843989268593166 --time: 19.773959398269653\n",
      "validation:  0.30333333333333334 --max 0.30333333333333334 --time: 20.86291742324829\n",
      "train acc:  0.4274549779136935  train loss:  0.3809216514877651 --time: 18.93120288848877\n",
      "validation:  0.30333333333333334 --max 0.30333333333333334 --time: 20.148013830184937\n",
      "train acc:  0.4291539245667686  train loss:  0.3792384046575297 --time: 19.824559450149536\n",
      "validation:  0.30666666666666664 --max 0.30666666666666664 --time: 20.853001594543457\n",
      "train acc:  0.42779476724430854  train loss:  0.37660085118335224 --time: 19.487984657287598\n",
      "validation:  0.31333333333333335 --max 0.31333333333333335 --time: 20.529515266418457\n",
      "train acc:  0.44036697247706424  train loss:  0.37264834538750025 --time: 19.489602088928223\n",
      "validation:  0.30666666666666664 --max 0.31333333333333335 --time: 20.66612696647644\n",
      "train acc:  0.4492014950730547  train loss:  0.3692597086014955 --time: 19.25359320640564\n",
      "validation:  0.3233333333333333 --max 0.3233333333333333 --time: 20.406992435455322\n",
      "train acc:  0.4617737003058104  train loss:  0.3699317794779073 --time: 19.185879945755005\n",
      "validation:  0.3433333333333333 --max 0.3433333333333333 --time: 20.24091410636902\n",
      "train acc:  0.4685694869181108  train loss:  0.3628886082898016 --time: 19.241353273391724\n",
      "validation:  0.3433333333333333 --max 0.3433333333333333 --time: 20.444913148880005\n",
      "train acc:  0.4763846415222562  train loss:  0.35980081946953485 --time: 19.17196011543274\n",
      "validation:  0.37 --max 0.37 --time: 20.12011480331421\n",
      "train acc:  0.49949031600407745  train loss:  0.35570195576419 --time: 20.23280668258667\n",
      "validation:  0.37666666666666665 --max 0.37666666666666665 --time: 21.452372312545776\n",
      "train acc:  0.5025484199796126  train loss:  0.3524028542249099 --time: 19.5254864692688\n",
      "validation:  0.41333333333333333 --max 0.41333333333333333 --time: 20.840070009231567\n",
      "train acc:  0.527353041114509  train loss:  0.3488489350546961 --time: 19.44017219543457\n",
      "validation:  0.39 --max 0.41333333333333333 --time: 20.620755672454834\n",
      "train acc:  0.5198776758409785  train loss:  0.34717364674029144 --time: 19.47809410095215\n",
      "validation:  0.42333333333333334 --max 0.42333333333333334 --time: 20.63904047012329\n",
      "train acc:  0.5327896704043493  train loss:  0.3435413319131602 --time: 19.029606103897095\n",
      "validation:  0.43 --max 0.43 --time: 20.4450523853302\n",
      "train acc:  0.544682296975875  train loss:  0.33894771078358527 --time: 19.793383598327637\n",
      "validation:  0.43666666666666665 --max 0.43666666666666665 --time: 21.38606548309326\n",
      "train acc:  0.5477404009514101  train loss:  0.3385973715263864 --time: 19.607774257659912\n",
      "validation:  0.42 --max 0.43666666666666665 --time: 20.582006692886353\n",
      "train acc:  0.5650696568127761  train loss:  0.33461086387219635 --time: 20.21417999267578\n",
      "validation:  0.44333333333333336 --max 0.44333333333333336 --time: 21.297527313232422\n",
      "train acc:  0.563370710159701  train loss:  0.3318659572497658 --time: 19.318394899368286\n",
      "validation:  0.42333333333333334 --max 0.44333333333333336 --time: 20.393932342529297\n",
      "train acc:  0.5857968059802923  train loss:  0.3283636557019275 --time: 20.223236799240112\n",
      "validation:  0.4533333333333333 --max 0.4533333333333333 --time: 21.3071928024292\n",
      "train acc:  0.5779816513761468  train loss:  0.3263479484164197 --time: 19.50666618347168\n",
      "validation:  0.4666666666666667 --max 0.4666666666666667 --time: 20.774577140808105\n",
      "train acc:  0.5963302752293578  train loss:  0.3245305393053138 --time: 18.84697914123535\n",
      "validation:  0.47 --max 0.47 --time: 20.012123823165894\n",
      "train acc:  0.6119605844376487  train loss:  0.31806766857271607 --time: 19.617557525634766\n",
      "validation:  0.49333333333333335 --max 0.49333333333333335 --time: 20.77579140663147\n",
      "train acc:  0.6055045871559633  train loss:  0.3171055718608525 --time: 18.81579828262329\n",
      "validation:  0.47 --max 0.49333333333333335 --time: 20.008765697479248\n",
      "train acc:  0.617737003058104  train loss:  0.3165007777836012 --time: 18.274543046951294\n",
      "validation:  0.4766666666666667 --max 0.49333333333333335 --time: 19.3757266998291\n",
      "train acc:  0.6194359497111791  train loss:  0.3117597090161365 --time: 19.08513069152832\n",
      "validation:  0.4866666666666667 --max 0.49333333333333335 --time: 20.460761070251465\n",
      "train acc:  0.6204553177030241  train loss:  0.3099030308101488 --time: 18.67616295814514\n",
      "validation:  0.49 --max 0.49333333333333335 --time: 19.73423719406128\n",
      "train acc:  0.6316683656133197  train loss:  0.3045093481955321 --time: 18.712226152420044\n",
      "validation:  0.48333333333333334 --max 0.49333333333333335 --time: 19.820175886154175\n",
      "train acc:  0.6425416241930003  train loss:  0.30340767295464227 --time: 20.00135374069214\n",
      "validation:  0.49 --max 0.49333333333333335 --time: 21.043583631515503\n",
      "train acc:  0.6292898402990146  train loss:  0.3024371007214422 --time: 18.44193983078003\n",
      "validation:  0.52 --max 0.52 --time: 19.631587982177734\n",
      "train acc:  0.6398233095480802  train loss:  0.3010353290516397 --time: 18.85649085044861\n",
      "validation:  0.5133333333333333 --max 0.52 --time: 19.974942684173584\n",
      "train acc:  0.6401630988786952  train loss:  0.2977080021215522 --time: 19.308526515960693\n",
      "validation:  0.53 --max 0.53 --time: 20.5364031791687\n",
      "train acc:  0.6527353041114509  train loss:  0.2937563813250998 --time: 19.56748938560486\n",
      "validation:  0.5366666666666666 --max 0.5366666666666666 --time: 20.66911029815674\n",
      "train acc:  0.6534148827726809  train loss:  0.29086296065993933 --time: 18.886817693710327\n",
      "validation:  0.5233333333333333 --max 0.5366666666666666 --time: 19.982520580291748\n",
      "train acc:  0.6591913013931363  train loss:  0.28910439299500507 --time: 19.092140913009644\n",
      "validation:  0.52 --max 0.5366666666666666 --time: 20.282033681869507\n",
      "train acc:  0.6636085626911316  train loss:  0.28579961346543353 --time: 18.25184178352356\n",
      "validation:  0.48333333333333334 --max 0.5366666666666666 --time: 19.37739109992981\n",
      "train acc:  0.6741420319401971  train loss:  0.28241822641828784 --time: 19.29085111618042\n",
      "validation:  0.52 --max 0.5366666666666666 --time: 20.40754270553589\n",
      "train acc:  0.6642881413523616  train loss:  0.28346493257128674 --time: 18.890655279159546\n",
      "validation:  0.5466666666666666 --max 0.5466666666666666 --time: 20.096311807632446\n",
      "train acc:  0.6700645599728169  train loss:  0.28225367613460706 --time: 19.423616409301758\n",
      "validation:  0.51 --max 0.5466666666666666 --time: 20.49274492263794\n",
      "train acc:  0.6778797145769623  train loss:  0.27563286410725635 --time: 19.257586240768433\n",
      "validation:  0.5533333333333333 --max 0.5533333333333333 --time: 20.18610429763794\n",
      "train acc:  0.6890927624872579  train loss:  0.2721561724724977 --time: 18.70655107498169\n",
      "validation:  0.5366666666666666 --max 0.5533333333333333 --time: 19.83848547935486\n",
      "train acc:  0.6877336051647979  train loss:  0.272915402184362 --time: 18.449352502822876\n",
      "validation:  0.5566666666666666 --max 0.5566666666666666 --time: 19.80178165435791\n",
      "train acc:  0.7043832823649337  train loss:  0.26643083147380664 --time: 18.481059312820435\n",
      "validation:  0.55 --max 0.5566666666666666 --time: 19.565550327301025\n",
      "train acc:  0.6952089704383282  train loss:  0.26764814944370935 --time: 18.68980312347412\n",
      "validation:  0.5566666666666666 --max 0.5566666666666666 --time: 19.778815031051636\n",
      "train acc:  0.7135575942915392  train loss:  0.2640585387530534 --time: 19.340617895126343\n",
      "validation:  0.5533333333333333 --max 0.5566666666666666 --time: 20.459245204925537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.7138973836221543  train loss:  0.26250529354033264 --time: 19.70974612236023\n",
      "validation:  0.56 --max 0.56 --time: 20.702330350875854\n",
      "train acc:  0.6986068637444784  train loss:  0.26534340303877124 --time: 19.669549703598022\n",
      "validation:  0.57 --max 0.57 --time: 20.684131622314453\n",
      "train acc:  0.7138973836221543  train loss:  0.2584718290878379 --time: 16.35853338241577\n",
      "validation:  0.56 --max 0.57 --time: 17.57904028892517\n",
      "train acc:  0.72579001019368  train loss:  0.25475888278173364 --time: 19.584773063659668\n",
      "validation:  0.56 --max 0.57 --time: 20.708574056625366\n",
      "train acc:  0.7336051647978253  train loss:  0.2573270434918611 --time: 19.490800142288208\n",
      "validation:  0.58 --max 0.58 --time: 20.647418975830078\n",
      "train acc:  0.7332653754672104  train loss:  0.2524961278490398 --time: 19.471823692321777\n",
      "validation:  0.5833333333333334 --max 0.5833333333333334 --time: 20.502278089523315\n",
      "train acc:  0.7349643221202854  train loss:  0.24973946550618048 --time: 17.564955949783325\n",
      "validation:  0.5766666666666667 --max 0.5833333333333334 --time: 18.58553433418274\n",
      "train acc:  0.7427794767244309  train loss:  0.24713185105634772 --time: 18.702444553375244\n",
      "validation:  0.6033333333333334 --max 0.6033333333333334 --time: 19.85956621170044\n",
      "train acc:  0.744818212708121  train loss:  0.2429926071477973 --time: 17.945449352264404\n",
      "validation:  0.5866666666666667 --max 0.6033333333333334 --time: 19.043359756469727\n",
      "train acc:  0.7125382262996942  train loss:  0.2577540757863418 --time: 18.398980379104614\n",
      "validation:  0.5433333333333333 --max 0.6033333333333334 --time: 19.495824098587036\n",
      "train acc:  0.7047230716955487  train loss:  0.25584438572759216 --time: 18.645833015441895\n",
      "validation:  0.6033333333333334 --max 0.6033333333333334 --time: 19.84357213973999\n",
      "train acc:  0.7125382262996942  train loss:  0.25174488256806915 --time: 19.685455560684204\n",
      "validation:  0.61 --max 0.61 --time: 20.840882062911987\n",
      "train acc:  0.7240910635406048  train loss:  0.24685247695964316 --time: 19.276610612869263\n",
      "validation:  0.5966666666666667 --max 0.61 --time: 20.279937744140625\n",
      "train acc:  0.7478763166836562  train loss:  0.23932978381281314 --time: 18.96603536605835\n",
      "validation:  0.6133333333333333 --max 0.6133333333333333 --time: 19.936973810195923\n",
      "train acc:  0.7485558953448862  train loss:  0.24084612662377564 --time: 18.92095184326172\n",
      "validation:  0.6033333333333334 --max 0.6133333333333333 --time: 19.92402744293213\n",
      "train acc:  0.7550118926265715  train loss:  0.23651237008364304 --time: 17.660783052444458\n",
      "validation:  0.58 --max 0.6133333333333333 --time: 18.838860273361206\n",
      "train acc:  0.7594291539245668  train loss:  0.23347373241963593 --time: 19.34859848022461\n",
      "validation:  0.62 --max 0.62 --time: 20.520501852035522\n",
      "train acc:  0.7740400951410126  train loss:  0.22939786574114923 --time: 19.78784203529358\n",
      "validation:  0.62 --max 0.62 --time: 20.99313712120056\n",
      "train acc:  0.782534828406388  train loss:  0.2267196573640989 --time: 19.434115648269653\n",
      "validation:  0.6233333333333333 --max 0.6233333333333333 --time: 20.62287449836731\n",
      "train acc:  0.7757390417940877  train loss:  0.2252424754526304 --time: 18.048720598220825\n",
      "validation:  0.6166666666666667 --max 0.6233333333333333 --time: 19.50221562385559\n",
      "train acc:  0.7811756710839279  train loss:  0.22512334390826846 --time: 19.02217745780945\n",
      "validation:  0.6166666666666667 --max 0.6233333333333333 --time: 20.19492506980896\n",
      "train acc:  0.7923887189942236  train loss:  0.22151007535664932 --time: 19.744545221328735\n",
      "validation:  0.6333333333333333 --max 0.6333333333333333 --time: 21.075664281845093\n",
      "train acc:  0.780156303092083  train loss:  0.22312939620536307 --time: 18.918379068374634\n",
      "validation:  0.6133333333333333 --max 0.6333333333333333 --time: 20.148634910583496\n",
      "train acc:  0.7737003058103975  train loss:  0.22477934736272562 --time: 18.52703309059143\n",
      "validation:  0.6033333333333334 --max 0.6333333333333333 --time: 19.792202949523926\n",
      "train acc:  0.7733605164797825  train loss:  0.222025818798853 --time: 19.473731517791748\n",
      "validation:  0.6166666666666667 --max 0.6333333333333333 --time: 20.708844423294067\n",
      "train acc:  0.7954468229697588  train loss:  0.21665510535240173 --time: 19.829118251800537\n",
      "validation:  0.6166666666666667 --max 0.6333333333333333 --time: 21.014657974243164\n",
      "train acc:  0.799864084267754  train loss:  0.21325020038563272 --time: 18.480392694473267\n",
      "validation:  0.6233333333333333 --max 0.6333333333333333 --time: 19.75474238395691\n",
      "train acc:  0.7930682976554536  train loss:  0.2123733771883923 --time: 18.459388494491577\n",
      "validation:  0.6333333333333333 --max 0.6333333333333333 --time: 19.740692377090454\n",
      "train acc:  0.8042813455657493  train loss:  0.2097899667594744 --time: 19.99170422554016\n",
      "validation:  0.62 --max 0.6333333333333333 --time: 21.21757173538208\n",
      "train acc:  0.8090383961943595  train loss:  0.20769000895645306 --time: 19.10653042793274\n",
      "validation:  0.6233333333333333 --max 0.6333333333333333 --time: 20.372333765029907\n",
      "train acc:  0.8090383961943595  train loss:  0.20652602872122888 --time: 18.88706398010254\n",
      "validation:  0.6433333333333333 --max 0.6433333333333333 --time: 20.02013397216797\n",
      "train acc:  0.8086986068637445  train loss:  0.2028297062801278 --time: 18.932424068450928\n",
      "validation:  0.6433333333333333 --max 0.6433333333333333 --time: 20.021327018737793\n",
      "train acc:  0.8086986068637445  train loss:  0.20190516049447266 --time: 18.903448820114136\n",
      "validation:  0.6366666666666667 --max 0.6433333333333333 --time: 20.185708045959473\n",
      "train acc:  0.8107373428474346  train loss:  0.2038171420926633 --time: 20.27639603614807\n",
      "validation:  0.6466666666666666 --max 0.6466666666666666 --time: 21.407564163208008\n",
      "train acc:  0.8158341828066599  train loss:  0.2019064108962598 --time: 19.18945598602295\n",
      "validation:  0.63 --max 0.6466666666666666 --time: 20.425673484802246\n",
      "train acc:  0.8141352361535847  train loss:  0.19668024130489514 --time: 18.899759531021118\n",
      "validation:  0.6433333333333333 --max 0.6466666666666666 --time: 20.164348125457764\n",
      "train acc:  0.81991165477404  train loss:  0.19344101392704507 --time: 19.063660621643066\n",
      "validation:  0.65 --max 0.65 --time: 20.253369331359863\n",
      "train acc:  0.8253482840638804  train loss:  0.19198055824507837 --time: 18.965031147003174\n",
      "validation:  0.6466666666666666 --max 0.65 --time: 20.03843331336975\n",
      "train acc:  0.8195718654434251  train loss:  0.19118322691191797 --time: 18.9268696308136\n",
      "validation:  0.65 --max 0.65 --time: 19.945547103881836\n",
      "train acc:  0.8341828066598709  train loss:  0.18968022387960684 --time: 19.47806143760681\n",
      "validation:  0.65 --max 0.65 --time: 20.620128631591797\n",
      "train acc:  0.8301053346924907  train loss:  0.18948520979155664 --time: 19.377758502960205\n",
      "validation:  0.6366666666666667 --max 0.65 --time: 20.655598163604736\n",
      "train acc:  0.8386000679578661  train loss:  0.18548459397709888 --time: 18.54025149345398\n",
      "validation:  0.6566666666666666 --max 0.6566666666666666 --time: 19.787639617919922\n",
      "train acc:  0.837580699966021  train loss:  0.18769646986671115 --time: 19.20767831802368\n",
      "validation:  0.6433333333333333 --max 0.6566666666666666 --time: 20.497963666915894\n",
      "train acc:  0.8348623853211009  train loss:  0.18778920562370963 --time: 19.564703226089478\n",
      "validation:  0.65 --max 0.6566666666666666 --time: 20.835577487945557\n",
      "train acc:  0.8440366972477065  train loss:  0.18244983838952106 --time: 19.56448245048523\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 20.795684337615967\n",
      "train acc:  0.8484539585457017  train loss:  0.17968501085820404 --time: 19.619611024856567\n",
      "validation:  0.6466666666666666 --max 0.6666666666666666 --time: 20.898288249969482\n",
      "train acc:  0.8467550118926266  train loss:  0.17911866890347522 --time: 18.82586169242859\n",
      "validation:  0.6633333333333333 --max 0.6666666666666666 --time: 20.074578285217285\n",
      "train acc:  0.8498131158681618  train loss:  0.175626574974993 --time: 18.684997081756592\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 19.93192458152771\n",
      "train acc:  0.8328236493374108  train loss:  0.18781172775703928 --time: 19.375259399414062\n",
      "validation:  0.66 --max 0.6666666666666666 --time: 20.412240743637085\n",
      "train acc:  0.8419979612640163  train loss:  0.17582927255526834 --time: 19.19030499458313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:  0.6466666666666666 --max 0.6666666666666666 --time: 20.410544872283936\n",
      "train acc:  0.8460754332313966  train loss:  0.17613010691559833 --time: 18.966920375823975\n",
      "validation:  0.6466666666666666 --max 0.6666666666666666 --time: 20.058122396469116\n",
      "train acc:  0.8464152225620115  train loss:  0.1738181762073351 --time: 18.65601420402527\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 19.8183376789093\n",
      "train acc:  0.8521916411824668  train loss:  0.1704526090103647 --time: 18.84866189956665\n",
      "validation:  0.6566666666666666 --max 0.6666666666666666 --time: 19.94084095954895\n",
      "train acc:  0.8525314305130819  train loss:  0.1706941289746243 --time: 20.377695560455322\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 21.551726579666138\n",
      "train acc:  0.8579680598029222  train loss:  0.16569236892720926 --time: 20.163663625717163\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 21.351536750793457\n",
      "train acc:  0.8606863744478424  train loss:  0.16398269391578177 --time: 18.92273998260498\n",
      "validation:  0.6566666666666666 --max 0.6666666666666666 --time: 20.364479064941406\n",
      "train acc:  0.8610261637784573  train loss:  0.16220963908278424 --time: 19.269004583358765\n",
      "validation:  0.6666666666666666 --max 0.6666666666666666 --time: 20.45494318008423\n",
      "train acc:  0.8606863744478424  train loss:  0.16125267355338388 --time: 19.07674789428711\n",
      "validation:  0.6633333333333333 --max 0.6666666666666666 --time: 20.22978639602661\n",
      "train acc:  0.8664627930682977  train loss:  0.16255721773790277 --time: 18.868749856948853\n",
      "validation:  0.67 --max 0.67 --time: 20.102293729782104\n",
      "train acc:  0.8623853211009175  train loss:  0.16058091754498688 --time: 18.99427819252014\n",
      "validation:  0.6666666666666666 --max 0.67 --time: 20.144750356674194\n",
      "train acc:  0.8746177370030581  train loss:  0.15610914904138315 --time: 19.284362316131592\n",
      "validation:  0.6766666666666666 --max 0.6766666666666666 --time: 20.56821084022522\n",
      "train acc:  0.8623853211009175  train loss:  0.15915423242942148 --time: 18.158715963363647\n",
      "validation:  0.66 --max 0.6766666666666666 --time: 19.180108547210693\n",
      "train acc:  0.8661230037376826  train loss:  0.1549148695624393 --time: 18.834855318069458\n",
      "validation:  0.67 --max 0.6766666666666666 --time: 20.063334941864014\n",
      "train acc:  0.8664627930682977  train loss:  0.15667735299338464 --time: 19.794501543045044\n",
      "validation:  0.6766666666666666 --max 0.6766666666666666 --time: 21.103498220443726\n",
      "train acc:  0.8685015290519877  train loss:  0.15411189263281616 --time: 19.31673526763916\n",
      "validation:  0.6466666666666666 --max 0.6766666666666666 --time: 20.503472089767456\n",
      "train acc:  0.8579680598029222  train loss:  0.16213321426640387 --time: 20.063360929489136\n",
      "validation:  0.69 --max 0.69 --time: 21.39660358428955\n",
      "train acc:  0.8596670064559973  train loss:  0.1616537253493848 --time: 19.23301386833191\n",
      "validation:  0.67 --max 0.69 --time: 20.391403436660767\n",
      "train acc:  0.8589874277947672  train loss:  0.15802577915398971 --time: 19.782630681991577\n",
      "validation:  0.6833333333333333 --max 0.69 --time: 21.16316843032837\n",
      "train acc:  0.8695208970438328  train loss:  0.15552340771840967 --time: 19.400869131088257\n",
      "validation:  0.6733333333333333 --max 0.69 --time: 20.472849369049072\n",
      "train acc:  0.8797145769622834  train loss:  0.14980687557355218 --time: 18.815951585769653\n",
      "validation:  0.6766666666666666 --max 0.69 --time: 19.940895557403564\n",
      "train acc:  0.8766564729867482  train loss:  0.14903558855471405 --time: 19.958739042282104\n",
      "validation:  0.68 --max 0.69 --time: 21.238665103912354\n",
      "train acc:  0.8780156303092083  train loss:  0.14817119681316873 --time: 19.397189378738403\n",
      "validation:  0.6866666666666666 --max 0.69 --time: 20.580703735351562\n",
      "train acc:  0.8773360516479782  train loss:  0.1469127445117287 --time: 19.54554533958435\n",
      "validation:  0.6933333333333334 --max 0.6933333333333334 --time: 20.586731433868408\n",
      "train acc:  0.8807339449541285  train loss:  0.1440455252709596 --time: 19.60552477836609\n",
      "validation:  0.6833333333333333 --max 0.6933333333333334 --time: 20.800217628479004\n",
      "train acc:  0.8861705742439687  train loss:  0.13953662955242654 --time: 17.912198781967163\n",
      "validation:  0.6933333333333334 --max 0.6933333333333334 --time: 19.111934423446655\n",
      "train acc:  0.8916072035338091  train loss:  0.1402858748384144 --time: 17.599751472473145\n",
      "validation:  0.69 --max 0.6933333333333334 --time: 18.515336751937866\n",
      "train acc:  0.8793747876316683  train loss:  0.13938530696474988 --time: 18.072795867919922\n",
      "validation:  0.6866666666666666 --max 0.6933333333333334 --time: 18.915911436080933\n",
      "train acc:  0.8939857288481141  train loss:  0.140175426783769 --time: 14.828585147857666\n",
      "validation:  0.6833333333333333 --max 0.6933333333333334 --time: 15.749226331710815\n",
      "train acc:  0.8956846755011892  train loss:  0.13698702597099802 --time: 14.779799938201904\n",
      "validation:  0.6766666666666666 --max 0.6933333333333334 --time: 15.61754322052002\n",
      "train acc:  0.8837920489296636  train loss:  0.13757348676090655 --time: 13.639117002487183\n",
      "validation:  0.6966666666666667 --max 0.6966666666666667 --time: 14.420227766036987\n",
      "train acc:  0.8922867821950391  train loss:  0.13505911211604657 --time: 14.589624881744385\n",
      "validation:  0.6833333333333333 --max 0.6966666666666667 --time: 15.293522834777832\n",
      "train acc:  0.9011213047910296  train loss:  0.1329628825187683 --time: 13.399297952651978\n",
      "validation:  0.6933333333333334 --max 0.6966666666666667 --time: 14.121703624725342\n",
      "train acc:  0.9011213047910296  train loss:  0.1317799369926038 --time: 13.536574840545654\n",
      "validation:  0.7 --max 0.7 --time: 14.239866256713867\n",
      "train acc:  0.8950050968399592  train loss:  0.13309852001459702 --time: 13.538352251052856\n",
      "validation:  0.6866666666666666 --max 0.7 --time: 14.3282630443573\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([ [1,2,3], [10,20,30],[100,200,300], [1000,2000,3000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1]), torch.Size([4, 3]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    2,    3],\n",
       "        [  10,   20,   30],\n",
       "        [ 100,  200,  300],\n",
       "        [1000, 2000, 3000]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     2,     3],\n",
       "        [   20,    40,    60],\n",
       "        [  300,   600,   900],\n",
       "        [ 4000,  8000, 12000]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.0000,   2.0000,   3.0000],\n",
       "        [  5.0000,  10.0000,  15.0000],\n",
       "        [ 33.3333,  66.6667, 100.0000],\n",
       "        [250.0000, 500.0000, 750.0000]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.float()/a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
