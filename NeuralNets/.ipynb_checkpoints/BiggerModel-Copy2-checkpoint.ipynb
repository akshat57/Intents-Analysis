{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    all_intents = ['increase', 'decrease', 'activate', 'deactivate', 'bring', 'change language']\n",
    "    return all_intents\n",
    "\n",
    "def get_intents():\n",
    "    all_intents = [\n",
    "        'activate|lamp',\n",
    "        'activate|lights|bedroom',\n",
    "        'activate|lights|kitchen',\n",
    "        'activate|lights|none',\n",
    "        'activate|lights|washroom',\n",
    "        'activate|music',\n",
    "        'bring|juice',\n",
    "        'bring|newspaper',\n",
    "        'bring|shoes',\n",
    "        'bring|socks',\n",
    "        'change language|Chinese',\n",
    "        'change language|English',\n",
    "        'change language|German',\n",
    "        'change language|Korean',\n",
    "        'change language|none',\n",
    "        'deactivate|lamp',\n",
    "        'deactivate|lights|bedroom',\n",
    "        'deactivate|lights|kitchen',\n",
    "        'deactivate|lights|none',\n",
    "        'deactivate|lights|washroom',\n",
    "        'deactivate|music',\n",
    "        'decrease|heat|bedroom',\n",
    "        'decrease|heat|kitchen',\n",
    "        'decrease|heat|none',\n",
    "        'decrease|heat|washroom',\n",
    "        'decrease|volume',\n",
    "        'increase|heat|bedroom',\n",
    "        'increase|heat|kitchen',\n",
    "        'increase|heat|none',\n",
    "        'increase|heat|washroom',\n",
    "        'increase|volume'\n",
    "        ]\n",
    "\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    if class_type == 'domain':\n",
    "        all_intents = get_domains()\n",
    "    else:\n",
    "        all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "split = 'train'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "train_file = '../FSC/fsc_' + class_type + '_' + split + '.pkl'\n",
    "test_file = '../FSC/fsc_' + class_type + '_test.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=50, embed_size=128, hidden_size=256, label_size=31):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(50, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 256)\n",
       "  (linear): Linear(in_features=256, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents train\n",
      "train acc:  0.5485056874702652  train loss:  1.488811171845178 --time: 2.7095887660980225\n",
      "validation:  0.8022151898734177 --max 0.8022151898734177 --time: 3.0723462104797363\n",
      "train acc:  0.8132001211020284  train loss:  0.6075217684329544 --time: 2.5941929817199707\n",
      "validation:  0.8652426160337553 --max 0.8652426160337553 --time: 2.9746084213256836\n",
      "train acc:  0.873491630984819  train loss:  0.4044355870281135 --time: 2.6442859172821045\n",
      "validation:  0.8873945147679325 --max 0.8873945147679325 --time: 3.0086188316345215\n",
      "train acc:  0.9045024004152069  train loss:  0.30485586056393155 --time: 2.6236870288848877\n",
      "validation:  0.9011075949367089 --max 0.9011075949367089 --time: 3.000606060028076\n",
      "train acc:  0.9290688119025994  train loss:  0.23029193545573323 --time: 2.6114673614501953\n",
      "validation:  0.9037447257383966 --max 0.9037447257383966 --time: 2.9888968467712402\n",
      "train acc:  0.9481856321093378  train loss:  0.17140647987140475 --time: 2.5857369899749756\n",
      "validation:  0.9005801687763713 --max 0.9037447257383966 --time: 2.9495251178741455\n",
      "train acc:  0.9628908784222135  train loss:  0.12333072586118846 --time: 2.6146669387817383\n",
      "validation:  0.9103375527426161 --max 0.9103375527426161 --time: 2.97896409034729\n",
      "train acc:  0.9708490117209463  train loss:  0.10182201590656575 --time: 2.604771614074707\n",
      "validation:  0.9058544303797469 --max 0.9103375527426161 --time: 2.974073886871338\n",
      "train acc:  0.9762986030016003  train loss:  0.08238332083664876 --time: 2.6016409397125244\n",
      "validation:  0.9145569620253164 --max 0.9145569620253164 --time: 2.9681396484375\n",
      "train acc:  0.9826132087712469  train loss:  0.0647683417249615 --time: 2.934753656387329\n",
      "validation:  0.9082278481012658 --max 0.9145569620253164 --time: 3.3323261737823486\n",
      "train acc:  0.9875437913585052  train loss:  0.04906069931383785 --time: 2.6324448585510254\n",
      "validation:  0.9053270042194093 --max 0.9145569620253164 --time: 2.9974634647369385\n",
      "train acc:  0.9903983391721811  train loss:  0.038478194612453984 --time: 2.59733247756958\n",
      "validation:  0.9087552742616034 --max 0.9145569620253164 --time: 2.9675161838531494\n",
      "train acc:  0.9936421435059037  train loss:  0.02838000446527126 --time: 2.591615915298462\n",
      "validation:  0.9129746835443038 --max 0.9145569620253164 --time: 2.9523396492004395\n",
      "train acc:  0.9926473768435621  train loss:  0.03026484065169458 --time: 2.6265628337860107\n",
      "validation:  0.9055907172995781 --max 0.9145569620253164 --time: 2.9891247749328613\n",
      "train acc:  0.9830457160157433  train loss:  0.05731519342024517 --time: 2.6065282821655273\n",
      "validation:  0.9029535864978903 --max 0.9145569620253164 --time: 2.9653472900390625\n",
      "train acc:  0.9868950304917607  train loss:  0.04528468811771323 --time: 2.591836929321289\n",
      "validation:  0.9061181434599156 --max 0.9145569620253164 --time: 2.9569644927978516\n",
      "train acc:  0.992431123221314  train loss:  0.02936768447157016 --time: 2.591242551803589\n",
      "validation:  0.9037447257383966 --max 0.9145569620253164 --time: 2.9559545516967773\n",
      "train acc:  0.9935988927814541  train loss:  0.02491100381632742 --time: 2.606706380844116\n",
      "validation:  0.9063818565400844 --max 0.9145569620253164 --time: 2.9723198413848877\n",
      "train acc:  0.9929501319147096  train loss:  0.026271493302771398 --time: 2.598632335662842\n",
      "validation:  0.9055907172995781 --max 0.9145569620253164 --time: 2.967618227005005\n",
      "train acc:  0.9948964145149432  train loss:  0.019699470281539205 --time: 2.6073591709136963\n",
      "validation:  0.9103375527426161 --max 0.9145569620253164 --time: 2.967743396759033\n",
      "train acc:  0.9910471000389256  train loss:  0.02994846390915304 --time: 2.6101491451263428\n",
      "validation:  0.9047995780590717 --max 0.9145569620253164 --time: 2.98555326461792\n",
      "train acc:  0.9893603217853899  train loss:  0.03494334644355003 --time: 2.6139533519744873\n",
      "validation:  0.9063818565400844 --max 0.9145569620253164 --time: 2.976836681365967\n",
      "train acc:  0.9920851174257169  train loss:  0.026642133134484127 --time: 2.621933698654175\n",
      "validation:  0.9021624472573839 --max 0.9145569620253164 --time: 2.981290340423584\n",
      "train acc:  0.9954154232083388  train loss:  0.018803481204249992 --time: 2.6010284423828125\n",
      "validation:  0.9100738396624473 --max 0.9145569620253164 --time: 2.9779324531555176\n",
      "train acc:  0.9958046797283855  train loss:  0.014054371496194264 --time: 2.5981569290161133\n",
      "validation:  0.9079641350210971 --max 0.9145569620253164 --time: 2.9702680110931396\n",
      "train acc:  0.9975347087063708  train loss:  0.010136003362899904 --time: 2.6073992252349854\n",
      "validation:  0.9135021097046413 --max 0.9145569620253164 --time: 2.9701409339904785\n",
      "train acc:  0.9978807145019679  train loss:  0.007825315500627586 --time: 2.6038026809692383\n",
      "validation:  0.9098101265822784 --max 0.9145569620253164 --time: 2.9702842235565186\n",
      "train acc:  0.9927771290169111  train loss:  0.023239808906389412 --time: 2.6214778423309326\n",
      "validation:  0.9042721518987342 --max 0.9145569620253164 --time: 2.98524808883667\n",
      "train acc:  0.9749578305436616  train loss:  0.07727450716890683 --time: 2.5864717960357666\n",
      "validation:  0.8984704641350211 --max 0.9145569620253164 --time: 2.949678659439087\n",
      "train acc:  0.9922148695990658  train loss:  0.028165085929399612 --time: 2.5954244136810303\n",
      "validation:  0.9106012658227848 --max 0.9145569620253164 --time: 2.956475019454956\n",
      "train acc:  0.9963669391462306  train loss:  0.013617543954510262 --time: 2.6289360523223877\n",
      "validation:  0.9063818565400844 --max 0.9145569620253164 --time: 2.995861053466797\n",
      "train acc:  0.9980969681242161  train loss:  0.007444285978987628 --time: 2.5992965698242188\n",
      "validation:  0.9111286919831224 --max 0.9145569620253164 --time: 2.96451735496521\n",
      "train acc:  0.9979239652264176  train loss:  0.005920309597852728 --time: 2.583416223526001\n",
      "validation:  0.9169303797468354 --max 0.9169303797468354 --time: 2.94938588142395\n",
      "train acc:  0.998226720297565  train loss:  0.005209909905891773 --time: 2.610443353652954\n",
      "validation:  0.917457805907173 --max 0.917457805907173 --time: 2.9750306606292725\n",
      "train acc:  0.9981402188486657  train loss:  0.004891843218240599 --time: 2.620149850845337\n",
      "validation:  0.915084388185654 --max 0.917457805907173 --time: 2.99013614654541\n",
      "train acc:  0.9983132217464643  train loss:  0.0047639198209710925 --time: 2.6163196563720703\n",
      "validation:  0.9187763713080169 --max 0.9187763713080169 --time: 2.9827163219451904\n",
      "train acc:  0.9984862246442628  train loss:  0.004207569786372533 --time: 2.6146631240844727\n",
      "validation:  0.9193037974683544 --max 0.9193037974683544 --time: 2.991584300994873\n",
      "train acc:  0.9983132217464643  train loss:  0.004390656901780743 --time: 2.5895895957946777\n",
      "validation:  0.9179852320675106 --max 0.9193037974683544 --time: 2.956585645675659\n",
      "train acc:  0.9982699710220146  train loss:  0.004451995254339887 --time: 2.607903242111206\n",
      "validation:  0.9185126582278481 --max 0.9193037974683544 --time: 2.9783546924591064\n",
      "train acc:  0.9983132217464643  train loss:  0.004338423876659121 --time: 2.5963497161865234\n",
      "validation:  0.917457805907173 --max 0.9193037974683544 --time: 2.9603867530822754\n",
      "train acc:  0.9984429739198132  train loss:  0.0038893076037899306 --time: 2.6065447330474854\n",
      "validation:  0.9179852320675106 --max 0.9193037974683544 --time: 2.979149341583252\n",
      "train acc:  0.9984862246442628  train loss:  0.004030974469281403 --time: 2.5826985836029053\n",
      "validation:  0.9171940928270043 --max 0.9193037974683544 --time: 2.943126678466797\n",
      "train acc:  0.9983132217464643  train loss:  0.003963156251176659 --time: 2.5922653675079346\n",
      "validation:  0.9193037974683544 --max 0.9193037974683544 --time: 2.9574148654937744\n",
      "train acc:  0.987933047878552  train loss:  0.035740584889905995 --time: 2.5868704319000244\n",
      "validation:  0.8507383966244726 --max 0.9193037974683544 --time: 2.9408788681030273\n",
      "train acc:  0.9405302538817525  train loss:  0.17901051817307487 --time: 2.6065280437469482\n",
      "validation:  0.9042721518987342 --max 0.9193037974683544 --time: 2.968769073486328\n",
      "train acc:  0.9890143159897928  train loss:  0.036336335981295584 --time: 2.5888938903808594\n",
      "validation:  0.9087552742616034 --max 0.9193037974683544 --time: 2.9489171504974365\n",
      "train acc:  0.9971887029107738  train loss:  0.010831412937085644 --time: 2.6058597564697266\n",
      "validation:  0.9069092827004219 --max 0.9193037974683544 --time: 2.9716124534606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9982699710220146  train loss:  0.006838134475160411 --time: 2.581949234008789\n",
      "validation:  0.9145569620253164 --max 0.9193037974683544 --time: 2.948815107345581\n",
      "train acc:  0.9980537173997664  train loss:  0.0052661268067551335 --time: 2.5987207889556885\n",
      "validation:  0.9161392405063291 --max 0.9193037974683544 --time: 2.9632251262664795\n",
      "train acc:  0.9983132217464643  train loss:  0.004412903250240108 --time: 2.5874972343444824\n",
      "validation:  0.9135021097046413 --max 0.9193037974683544 --time: 2.9562432765960693\n",
      "train acc:  0.9984862246442628  train loss:  0.004030886473930415 --time: 2.6179215908050537\n",
      "validation:  0.9158755274261603 --max 0.9193037974683544 --time: 2.977778434753418\n",
      "train acc:  0.998226720297565  train loss:  0.004248723991662881 --time: 2.5835962295532227\n",
      "validation:  0.9140295358649789 --max 0.9193037974683544 --time: 2.9504096508026123\n",
      "train acc:  0.9982699710220146  train loss:  0.004196935604411326 --time: 2.5926015377044678\n",
      "validation:  0.9156118143459916 --max 0.9193037974683544 --time: 2.9593565464019775\n",
      "train acc:  0.9982699710220146  train loss:  0.004089603562985974 --time: 2.596797466278076\n",
      "validation:  0.9132383966244726 --max 0.9193037974683544 --time: 2.9609150886535645\n",
      "train acc:  0.9983997231953635  train loss:  0.004041075551636692 --time: 2.602099895477295\n",
      "validation:  0.915084388185654 --max 0.9193037974683544 --time: 2.977726697921753\n",
      "train acc:  0.998226720297565  train loss:  0.004000994851825584 --time: 2.637962818145752\n",
      "validation:  0.9166666666666666 --max 0.9193037974683544 --time: 2.9983456134796143\n",
      "train acc:  0.9981834695731153  train loss:  0.004246488895228015 --time: 2.590402126312256\n",
      "validation:  0.9166666666666666 --max 0.9193037974683544 --time: 2.9525413513183594\n",
      "train acc:  0.9985294753687124  train loss:  0.003783718868704173 --time: 2.6066505908966064\n",
      "validation:  0.9129746835443038 --max 0.9193037974683544 --time: 2.9727346897125244\n",
      "train acc:  0.9983997231953635  train loss:  0.003852395506820755 --time: 2.5999834537506104\n",
      "validation:  0.9153481012658228 --max 0.9193037974683544 --time: 2.969712257385254\n",
      "train acc:  0.9983564724709139  train loss:  0.003925290831106777 --time: 2.6018569469451904\n",
      "validation:  0.9164029535864979 --max 0.9193037974683544 --time: 2.9776899814605713\n",
      "train acc:  0.9978807145019679  train loss:  0.00458981416476646 --time: 2.5891575813293457\n",
      "validation:  0.9100738396624473 --max 0.9193037974683544 --time: 2.9564433097839355\n",
      "train acc:  0.9518186929631072  train loss:  0.14426836584442557 --time: 2.5948898792266846\n",
      "validation:  0.8916139240506329 --max 0.9193037974683544 --time: 2.9591774940490723\n",
      "train acc:  0.9837377276069375  train loss:  0.05022948523417362 --time: 2.5892884731292725\n",
      "validation:  0.9127109704641351 --max 0.9193037974683544 --time: 2.9524216651916504\n",
      "train acc:  0.995847930452835  train loss:  0.015004875441339093 --time: 2.594980001449585\n",
      "validation:  0.9164029535864979 --max 0.9193037974683544 --time: 2.962329864501953\n",
      "train acc:  0.9977942130530686  train loss:  0.007708888923497202 --time: 2.6021788120269775\n",
      "validation:  0.9119198312236287 --max 0.9193037974683544 --time: 2.970881938934326\n",
      "train acc:  0.9976644608797197  train loss:  0.007412913373416773 --time: 2.604342460632324\n",
      "validation:  0.9119198312236287 --max 0.9193037974683544 --time: 2.9683079719543457\n",
      "train acc:  0.9983564724709139  train loss:  0.005299888464406092 --time: 2.600832223892212\n",
      "validation:  0.9127109704641351 --max 0.9193037974683544 --time: 2.9746012687683105\n",
      "train acc:  0.998226720297565  train loss:  0.004514840617229216 --time: 2.6125433444976807\n",
      "validation:  0.9121835443037974 --max 0.9193037974683544 --time: 3.414179563522339\n",
      "train acc:  0.9985294753687124  train loss:  0.0040469973662273845 --time: 6.8974950313568115\n",
      "validation:  0.9127109704641351 --max 0.9193037974683544 --time: 7.548882961273193\n",
      "train acc:  0.9981834695731153  train loss:  0.004128684413565313 --time: 6.995481252670288\n",
      "validation:  0.9103375527426161 --max 0.9193037974683544 --time: 7.438272953033447\n",
      "train acc:  0.9983564724709139  train loss:  0.0040261134571519305 --time: 7.221386194229126\n",
      "validation:  0.9106012658227848 --max 0.9193037974683544 --time: 7.8377580642700195\n",
      "train acc:  0.9983132217464643  train loss:  0.0038144680332438765 --time: 7.322439908981323\n",
      "validation:  0.9121835443037974 --max 0.9193037974683544 --time: 8.15898323059082\n",
      "train acc:  0.9983564724709139  train loss:  0.0037594917569584753 --time: 7.376587629318237\n",
      "validation:  0.9121835443037974 --max 0.9193037974683544 --time: 8.183937072753906\n",
      "train acc:  0.9984429739198132  train loss:  0.003831966083402905 --time: 7.388194561004639\n",
      "validation:  0.9121835443037974 --max 0.9193037974683544 --time: 8.102144241333008\n"
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
