{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'hindi'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_lang_train_split/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=63, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(63, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "#opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi hindi\n",
      "train acc:  0.2711518858307849  train loss:  1.7330094835032588 --time: 2.295708417892456\n",
      "validation:  0.22 --max 0.22 --time: 2.7241954803466797\n",
      "train acc:  0.30988786952089703  train loss:  1.6893179157505864 --time: 2.280566453933716\n",
      "validation:  0.23333333333333334 --max 0.23333333333333334 --time: 2.5856168270111084\n",
      "train acc:  0.31668365613319743  train loss:  1.6663473585377568 --time: 1.5838277339935303\n",
      "validation:  0.24 --max 0.24 --time: 1.9086940288543701\n",
      "train acc:  0.3367312266394835  train loss:  1.6380265692005986 --time: 1.5194120407104492\n",
      "validation:  0.24666666666666667 --max 0.24666666666666667 --time: 1.9298865795135498\n",
      "train acc:  0.34658511722731905  train loss:  1.616543464038683 --time: 2.248326063156128\n",
      "validation:  0.27 --max 0.27 --time: 2.6638596057891846\n",
      "train acc:  0.36119605844376484  train loss:  1.594642955323924 --time: 2.406214714050293\n",
      "validation:  0.2833333333333333 --max 0.2833333333333333 --time: 2.755075454711914\n",
      "train acc:  0.3724091063540605  train loss:  1.5725502708683843 --time: 2.498302698135376\n",
      "validation:  0.24666666666666667 --max 0.2833333333333333 --time: 2.822221279144287\n",
      "train acc:  0.3788651036357458  train loss:  1.5640644975330518 --time: 2.6167685985565186\n",
      "validation:  0.28 --max 0.2833333333333333 --time: 2.8994314670562744\n",
      "train acc:  0.40706761807679237  train loss:  1.5292213118594626 --time: 2.6876401901245117\n",
      "validation:  0.31666666666666665 --max 0.31666666666666665 --time: 2.9784090518951416\n",
      "train acc:  0.42099898063200814  train loss:  1.507392178411069 --time: 2.636708974838257\n",
      "validation:  0.2966666666666667 --max 0.31666666666666665 --time: 2.9743380546569824\n",
      "train acc:  0.41182466870540263  train loss:  1.5096119072126306 --time: 2.7176952362060547\n",
      "validation:  0.3233333333333333 --max 0.3233333333333333 --time: 3.00986909866333\n",
      "train acc:  0.43662928984029903  train loss:  1.4824735807335896 --time: 2.6567325592041016\n",
      "validation:  0.4266666666666667 --max 0.4266666666666667 --time: 2.9574055671691895\n",
      "train acc:  0.47910295616717635  train loss:  1.3950501835864524 --time: 2.6209328174591064\n",
      "validation:  0.41333333333333333 --max 0.4266666666666667 --time: 3.0058839321136475\n",
      "train acc:  0.4855589534488617  train loss:  1.3657659758692202 --time: 2.540952444076538\n",
      "validation:  0.43333333333333335 --max 0.43333333333333335 --time: 2.886061668395996\n",
      "train acc:  0.5409446143391098  train loss:  1.2913651777350383 --time: 2.606912612915039\n",
      "validation:  0.49333333333333335 --max 0.49333333333333335 --time: 2.980069637298584\n",
      "train acc:  0.5868161739721373  train loss:  1.1771537946618122 --time: 2.676828384399414\n",
      "validation:  0.53 --max 0.53 --time: 2.9849765300750732\n",
      "train acc:  0.6394835202174651  train loss:  1.0563734147859656 --time: 2.6959540843963623\n",
      "validation:  0.38 --max 0.53 --time: 3.0385756492614746\n",
      "train acc:  0.673462453278967  train loss:  0.9873956675114839 --time: 2.5014822483062744\n",
      "validation:  0.6133333333333333 --max 0.6133333333333333 --time: 2.7985286712646484\n",
      "train acc:  0.7050628610261638  train loss:  0.8960734191148177 --time: 2.6280863285064697\n",
      "validation:  0.6433333333333333 --max 0.6433333333333333 --time: 2.973423957824707\n",
      "train acc:  0.7628270472307169  train loss:  0.7337027269860973 --time: 2.5746195316314697\n",
      "validation:  0.7133333333333334 --max 0.7133333333333334 --time: 2.9678187370300293\n",
      "train acc:  0.7791369351002378  train loss:  0.6931502430335336 --time: 2.532205820083618\n",
      "validation:  0.7166666666666667 --max 0.7166666666666667 --time: 2.8705735206604004\n",
      "train acc:  0.817533129459735  train loss:  0.6021112721899281 --time: 2.595240592956543\n",
      "validation:  0.73 --max 0.73 --time: 2.995685338973999\n",
      "train acc:  0.8287461773700305  train loss:  0.549949182116467 --time: 2.7184226512908936\n",
      "validation:  0.7333333333333333 --max 0.7333333333333333 --time: 3.020496129989624\n",
      "train acc:  0.8589874277947672  train loss:  0.47521429735681286 --time: 2.750183582305908\n",
      "validation:  0.7366666666666667 --max 0.7366666666666667 --time: 3.0633153915405273\n",
      "train acc:  0.8297655453618756  train loss:  0.5175731026608011 --time: 2.8011627197265625\n",
      "validation:  0.7733333333333333 --max 0.7733333333333333 --time: 3.1614410877227783\n",
      "train acc:  0.8769962623173633  train loss:  0.3990927545920662 --time: 2.6911511421203613\n",
      "validation:  0.7066666666666667 --max 0.7733333333333333 --time: 2.9821970462799072\n",
      "train acc:  0.8953448861705743  train loss:  0.3734885382911433 --time: 2.756521463394165\n",
      "validation:  0.75 --max 0.7733333333333333 --time: 3.1711857318878174\n",
      "train acc:  0.9011213047910296  train loss:  0.33911631483098736 --time: 2.6522696018218994\n",
      "validation:  0.81 --max 0.81 --time: 3.022343158721924\n",
      "train acc:  0.8882093102276588  train loss:  0.35002751194912457 --time: 2.635653018951416\n",
      "validation:  0.7966666666666666 --max 0.81 --time: 3.0018959045410156\n",
      "train acc:  0.927964661909616  train loss:  0.24909121316412222 --time: 2.5537519454956055\n",
      "validation:  0.79 --max 0.81 --time: 2.8433144092559814\n",
      "train acc:  0.9310227658851512  train loss:  0.24338714648847995 --time: 2.666538953781128\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 2.952911376953125\n",
      "train acc:  0.9211688752973156  train loss:  0.27616651809733844 --time: 2.662109375\n",
      "validation:  0.81 --max 0.81 --time: 2.9750943183898926\n",
      "train acc:  0.9469928644240571  train loss:  0.18866493455741717 --time: 2.815483570098877\n",
      "validation:  0.8033333333333333 --max 0.81 --time: 3.126737356185913\n",
      "train acc:  0.8776758409785933  train loss:  0.39447347269110056 --time: 2.7481935024261475\n",
      "validation:  0.8266666666666667 --max 0.8266666666666667 --time: 3.0553338527679443\n",
      "train acc:  0.9544682296975875  train loss:  0.18733694533938947 --time: 2.589911937713623\n",
      "validation:  0.7966666666666666 --max 0.8266666666666667 --time: 3.03603196144104\n",
      "train acc:  0.9700985389058784  train loss:  0.12587243448133054 --time: 2.626023769378662\n",
      "validation:  0.8466666666666667 --max 0.8466666666666667 --time: 3.0321123600006104\n",
      "train acc:  0.9520897043832823  train loss:  0.17143898457288742 --time: 2.595848798751831\n",
      "validation:  0.8366666666666667 --max 0.8466666666666667 --time: 2.9057235717773438\n",
      "train acc:  0.9847094801223242  train loss:  0.08521263706295387 --time: 2.7736332416534424\n",
      "validation:  0.8433333333333334 --max 0.8466666666666667 --time: 3.137324571609497\n",
      "train acc:  0.9843696907917091  train loss:  0.08118161008409831 --time: 2.8742496967315674\n",
      "validation:  0.8633333333333333 --max 0.8633333333333333 --time: 3.2295961380004883\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
