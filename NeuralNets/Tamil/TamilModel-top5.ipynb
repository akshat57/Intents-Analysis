{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/akshatgu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshatgu/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'eos':1, 'unk': 2}#Padding indx = 0, eos = 1, unkown_idx = 2, indexing starts from 3\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 3\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLMDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        output_vector = self.all_data[index][0][1:] + [1]\n",
    "\n",
    "        return input_vector, output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_LM(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "    \n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    y = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "        y[i,:len(x_np)] = torch.tensor(y_lst[i])\n",
    "\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intents():\n",
    "    all_intents = ['1', '2', '3', '4', '5', '6']\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "split = '1'\n",
    "train_file = '../../Tamil_Dataset/datasplit_top5_split1/tamil_train_split_' + split + '.pkl'\n",
    "test_file = '../../Tamil_Dataset/datasplit_top5_split1/tamil_test_split_' + split + '.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "vocab_size = len(phone_to_idx) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyLMDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=64)\n",
    "train_loader_LM = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=64)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, pretrained_emb = None, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm_LM = nn.LSTM(embed_size, hidden_size, num_layers=1)\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=1)\n",
    "\n",
    "        self.linear_LM = nn.Linear(hidden_size, embed_size)\n",
    "        self.output_LM = nn.Linear(embed_size, vocab_size)\n",
    "        self.output_LM.weight = self.embed.weight\n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "        \n",
    "        if pretrained_emb != None:\n",
    "            self.pretrained_embed = nn.Embedding(vocab_size, embed_size)\n",
    "            self.pretrained_embed.weight = nn.Parameter(pretrained_emb)\n",
    "            \n",
    "\n",
    "    def forward(self, x, lengths, lm = True, pretrained = False):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "        # B,T,H\n",
    "        \n",
    "        if pretrained == False:\n",
    "            input = self.embed(x)\n",
    "        \n",
    "        if lm:\n",
    "            pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "            output, (hn, cn) = self.lstm_LM(pack_tensor)\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "            output = F.relu(self.linear_LM(output))\n",
    "            logits = self.output_LM(output)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            ######if using pre-trained embeddings for top 5 \n",
    "            if pretrained:\n",
    "                batch_size = x.size(0)\n",
    "                max_len = x.size(1)\n",
    "                new_max_len = max_len//6 + 1\n",
    "                input = torch.zeros((batch_size, new_max_len, self.embed_size ))\n",
    "\n",
    "                counter = 0\n",
    "                for j in range(0, max_len, 6):\n",
    "                    temp = self.pretrained_embed(x[:,j:j+5])\n",
    "                    temp = temp.mean(dim = 1)\n",
    "                    input[:,counter,:] = temp\n",
    "                    counter += 1\n",
    "\n",
    "                lengths = lengths//6 + 1\n",
    "\n",
    "                if cuda:\n",
    "                    input = input.cuda()\n",
    "            ##########ending using top 5\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            # (B,T,H) -> (B,H,T)\n",
    "            input = input.transpose(1,2)\n",
    "\n",
    "            #cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "            cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "\n",
    "            # (B,H,T)\n",
    "            input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "            input = input.transpose(1,2)\n",
    "\n",
    "            pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "            output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "            logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(48, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm_LM): LSTM(128, 128)\n",
       "  (lstm): LSTM(256, 128)\n",
       "  (linear_LM): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (output_LM): Linear(in_features=128, out_features=48, bias=True)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       "  (pretrained_embed): Embedding(48, 128)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained = True\n",
    "if pretrained:\n",
    "    pretrained_emb = torch.from_numpy(np.load('classifier_embedding_weights.npy'))\n",
    "    model = RNNClassifier(vocab_size, pretrained_emb)\n",
    "else:\n",
    "    model = RNNClassifier(vocab_size)\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.01)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents 1\n",
      "0  train loss:  4.189269924163819 --time: 6.7287726402282715\n",
      "1  train loss:  3.8274669647216797 --time: 7.1290366649627686\n",
      "2  train loss:  3.5428661346435546 --time: 7.128594636917114\n",
      "3  train loss:  3.2561341285705567 --time: 6.790801763534546\n",
      "4  train loss:  3.0536319732666017 --time: 7.0973474979400635\n",
      "5  train loss:  2.8292887687683104 --time: 6.8741655349731445\n",
      "6  train loss:  2.659030294418335 --time: 7.137286424636841\n",
      "7  train loss:  2.4670814514160155 --time: 6.927358388900757\n",
      "8  train loss:  2.2899849891662596 --time: 6.797726154327393\n",
      "9  train loss:  2.1423481941223144 --time: 6.861461639404297\n",
      "10  train loss:  1.9984972715377807 --time: 6.8908960819244385\n",
      "11  train loss:  1.8685822010040283 --time: 6.82082200050354\n",
      "12  train loss:  1.740103578567505 --time: 7.1625916957855225\n",
      "13  train loss:  1.6260349750518799 --time: 7.093270540237427\n",
      "14  train loss:  1.5216002702713012 --time: 6.957597255706787\n",
      "15  train loss:  1.4139403581619263 --time: 7.179672002792358\n",
      "16  train loss:  1.326194167137146 --time: 7.094298601150513\n",
      "17  train loss:  1.259308886528015 --time: 6.887744903564453\n",
      "18  train loss:  1.1642268180847168 --time: 7.1833250522613525\n",
      "19  train loss:  1.105217409133911 --time: 7.093922138214111\n",
      "20  train loss:  1.051594042778015 --time: 7.082468271255493\n",
      "21  train loss:  1.0009232044219971 --time: 7.096299409866333\n",
      "22  train loss:  0.9704441785812378 --time: 6.964352130889893\n",
      "23  train loss:  0.9193856716156006 --time: 7.079978704452515\n",
      "24  train loss:  0.9351363778114319 --time: 6.707574129104614\n",
      "25  train loss:  0.8646384358406067 --time: 6.999697923660278\n",
      "26  train loss:  0.8574948787689209 --time: 6.799348831176758\n",
      "27  train loss:  0.8032716512680054 --time: 7.105835199356079\n",
      "28  train loss:  0.7836394786834717 --time: 7.11827278137207\n",
      "29  train loss:  0.8324900507926941 --time: 6.528321981430054\n",
      "30  train loss:  0.7717594146728516 --time: 6.899547338485718\n",
      "31  train loss:  0.7648416638374329 --time: 6.822268962860107\n",
      "32  train loss:  0.7157631516456604 --time: 7.14381217956543\n",
      "33  train loss:  0.7122520446777344 --time: 7.060115337371826\n",
      "34  train loss:  0.7230495810508728 --time: 6.827826738357544\n",
      "35  train loss:  0.7043245792388916 --time: 6.949750661849976\n",
      "36  train loss:  0.6766855239868164 --time: 7.105353593826294\n",
      "37  train loss:  0.7753705739974975 --time: 6.372028827667236\n",
      "38  train loss:  0.742070484161377 --time: 6.614645719528198\n",
      "39  train loss:  0.6914134383201599 --time: 6.846225261688232\n",
      "40  train loss:  0.6689266085624694 --time: 6.880862474441528\n",
      "41  train loss:  0.6585585713386536 --time: 6.947545528411865\n",
      "42  train loss:  0.6573511481285095 --time: 6.866155624389648\n",
      "43  train loss:  0.6240300893783569 --time: 7.122673511505127\n",
      "44  train loss:  0.6969023585319519 --time: 6.508767366409302\n",
      "45  train loss:  0.6137035965919495 --time: 7.109216213226318\n",
      "46  train loss:  0.6110352873802185 --time: 7.062216758728027\n",
      "47  train loss:  0.5985380411148071 --time: 7.144237518310547\n",
      "48  train loss:  0.6530161619186401 --time: 6.607496023178101\n",
      "49  train loss:  0.6019830703735352 --time: 7.0156824588775635\n",
      "50  train loss:  0.594700288772583 --time: 7.024278879165649\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f8f4cc4942f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-91a6aeb9fff1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths, lm)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mpack_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 585\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train language model\n",
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(100):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader_LM):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "        \n",
    "        loss = criterion(logits.permute(0,2,1), y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "                    \n",
    "\n",
    "    print(j, \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    parameters2 = model.embed.weight.detach().cpu().numpy()\n",
    "    np.save('classifier_embedding_weights.npy', parameters2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents 1\n",
      "train acc:  0.234375  train loss:  1.774974513053894 --time: 0.6522302627563477\n",
      "0 validation:  0.2875 --max 0.2875 --time: 1.0606937408447266\n",
      "train acc:  0.3375  train loss:  1.6742762565612792 --time: 0.6787009239196777\n",
      "1 validation:  0.3625 --max 0.3625 --time: 1.08732008934021\n",
      "train acc:  0.36875  train loss:  1.5840079545974732 --time: 0.6678898334503174\n",
      "2 validation:  0.35 --max 0.3625 --time: 1.0899434089660645\n",
      "train acc:  0.4625  train loss:  1.4209823846817016 --time: 0.7056839466094971\n",
      "3 validation:  0.525 --max 0.525 --time: 1.1270074844360352\n",
      "train acc:  0.575  train loss:  1.2795114278793336 --time: 0.66668701171875\n",
      "4 validation:  0.525 --max 0.525 --time: 1.0753934383392334\n",
      "train acc:  0.54375  train loss:  1.2264013528823852 --time: 0.6768698692321777\n",
      "5 validation:  0.55 --max 0.55 --time: 1.0857722759246826\n",
      "train acc:  0.69375  train loss:  1.0105744361877442 --time: 0.6704621315002441\n",
      "6 validation:  0.6125 --max 0.6125 --time: 1.0791642665863037\n",
      "train acc:  0.721875  train loss:  0.8335596084594726 --time: 0.673591136932373\n",
      "7 validation:  0.6625 --max 0.6625 --time: 1.102287769317627\n",
      "train acc:  0.809375  train loss:  0.7132004976272583 --time: 0.6794071197509766\n",
      "8 validation:  0.6625 --max 0.6625 --time: 1.0880584716796875\n",
      "train acc:  0.7625  train loss:  0.6477899193763733 --time: 0.6500976085662842\n",
      "9 validation:  0.6375 --max 0.6625 --time: 1.0596191883087158\n",
      "train acc:  0.859375  train loss:  0.5254978239536285 --time: 0.6836097240447998\n",
      "10 validation:  0.7125 --max 0.7125 --time: 1.0927073955535889\n",
      "train acc:  0.865625  train loss:  0.4325542330741882 --time: 0.6801362037658691\n",
      "11 validation:  0.8 --max 0.8 --time: 1.0898518562316895\n",
      "train acc:  0.91875  train loss:  0.332291179895401 --time: 0.6763415336608887\n",
      "12 validation:  0.725 --max 0.8 --time: 1.0864801406860352\n",
      "train acc:  0.91875  train loss:  0.289553165435791 --time: 0.6782991886138916\n",
      "13 validation:  0.7375 --max 0.8 --time: 1.0879795551300049\n",
      "train acc:  0.95625  train loss:  0.2247060775756836 --time: 0.6855201721191406\n",
      "14 validation:  0.825 --max 0.825 --time: 1.0953130722045898\n",
      "train acc:  0.975  train loss:  0.1456162825226784 --time: 0.676337480545044\n",
      "15 validation:  0.8125 --max 0.825 --time: 1.085402250289917\n",
      "train acc:  0.99375  train loss:  0.10430337935686111 --time: 0.6493186950683594\n",
      "16 validation:  0.825 --max 0.825 --time: 1.0637516975402832\n",
      "train acc:  1.0  train loss:  0.0708063781261444 --time: 0.6604900360107422\n",
      "17 validation:  0.8375 --max 0.8375 --time: 1.075340747833252\n",
      "train acc:  0.9875  train loss:  0.07699878439307213 --time: 0.6614072322845459\n",
      "18 validation:  0.7875 --max 0.8375 --time: 1.0702776908874512\n",
      "train acc:  0.99375  train loss:  0.06479733735322953 --time: 0.707456111907959\n",
      "19 validation:  0.775 --max 0.8375 --time: 1.1170902252197266\n",
      "train acc:  0.9875  train loss:  0.07191476225852966 --time: 0.6495044231414795\n",
      "20 validation:  0.8375 --max 0.8375 --time: 1.062772274017334\n",
      "train acc:  1.0  train loss:  0.050228486955165866 --time: 0.6806933879852295\n",
      "21 validation:  0.85 --max 0.85 --time: 1.0929527282714844\n",
      "train acc:  1.0  train loss:  0.04393127486109734 --time: 0.6607508659362793\n",
      "22 validation:  0.85 --max 0.85 --time: 1.0728859901428223\n",
      "train acc:  1.0  train loss:  0.029542366042733192 --time: 0.6626091003417969\n",
      "23 validation:  0.8875 --max 0.8875 --time: 1.0753865242004395\n",
      "train acc:  0.996875  train loss:  0.02518426366150379 --time: 0.6243200302124023\n",
      "24 validation:  0.8625 --max 0.8875 --time: 1.036712884902954\n",
      "train acc:  1.0  train loss:  0.016892424412071706 --time: 0.654536247253418\n",
      "25 validation:  0.8875 --max 0.8875 --time: 1.0670366287231445\n",
      "train acc:  1.0  train loss:  0.015591339953243733 --time: 0.6552002429962158\n",
      "26 validation:  0.8875 --max 0.8875 --time: 1.0674998760223389\n",
      "train acc:  1.0  train loss:  0.010697814263403415 --time: 0.7025346755981445\n",
      "27 validation:  0.8625 --max 0.8875 --time: 1.115020751953125\n",
      "train acc:  1.0  train loss:  0.008367562759667635 --time: 0.6700477600097656\n",
      "28 validation:  0.875 --max 0.8875 --time: 1.0817430019378662\n",
      "train acc:  1.0  train loss:  0.006646918784826994 --time: 0.6936221122741699\n",
      "29 validation:  0.8875 --max 0.8875 --time: 1.1062066555023193\n",
      "train acc:  1.0  train loss:  0.007152437418699265 --time: 0.6260042190551758\n",
      "30 validation:  0.8875 --max 0.8875 --time: 1.0382487773895264\n",
      "train acc:  1.0  train loss:  0.005000207107514143 --time: 0.6869664192199707\n",
      "31 validation:  0.8875 --max 0.8875 --time: 1.0995252132415771\n",
      "train acc:  1.0  train loss:  0.004381064558401704 --time: 0.6650822162628174\n",
      "32 validation:  0.9 --max 0.9 --time: 1.0765657424926758\n",
      "train acc:  1.0  train loss:  0.004316287906840443 --time: 0.656611442565918\n",
      "33 validation:  0.9 --max 0.9 --time: 1.0689666271209717\n",
      "train acc:  1.0  train loss:  0.004079961916431785 --time: 0.63327956199646\n",
      "34 validation:  0.9 --max 0.9 --time: 1.0456645488739014\n",
      "train acc:  1.0  train loss:  0.00358152249827981 --time: 0.6704726219177246\n",
      "35 validation:  0.9 --max 0.9 --time: 1.0968992710113525\n",
      "train acc:  1.0  train loss:  0.003319661691784859 --time: 0.6725399494171143\n",
      "36 validation:  0.9 --max 0.9 --time: 1.0959179401397705\n",
      "train acc:  1.0  train loss:  0.003071590978652239 --time: 0.6638751029968262\n",
      "37 validation:  0.9 --max 0.9 --time: 1.0726516246795654\n",
      "train acc:  1.0  train loss:  0.0029802761506289243 --time: 0.6646637916564941\n",
      "38 validation:  0.9 --max 0.9 --time: 1.074040412902832\n",
      "train acc:  1.0  train loss:  0.002966170432046056 --time: 0.6592221260070801\n",
      "39 validation:  0.9 --max 0.9 --time: 1.0686731338500977\n",
      "train acc:  1.0  train loss:  0.0030027320608496666 --time: 0.6389083862304688\n",
      "40 validation:  0.9 --max 0.9 --time: 1.0483450889587402\n",
      "train acc:  1.0  train loss:  0.0025289994198828937 --time: 0.6540098190307617\n",
      "41 validation:  0.9 --max 0.9 --time: 1.0631680488586426\n",
      "train acc:  1.0  train loss:  0.002520426013506949 --time: 0.67950439453125\n",
      "42 validation:  0.9 --max 0.9 --time: 1.0887842178344727\n",
      "train acc:  1.0  train loss:  0.0024302954319864512 --time: 0.6748569011688232\n",
      "43 validation:  0.9 --max 0.9 --time: 1.093461036682129\n",
      "train acc:  1.0  train loss:  0.0022634023800492285 --time: 0.6661255359649658\n",
      "44 validation:  0.9 --max 0.9 --time: 1.0744876861572266\n",
      "train acc:  1.0  train loss:  0.0024166930932551624 --time: 0.6246433258056641\n",
      "45 validation:  0.9 --max 0.9 --time: 1.0333197116851807\n",
      "train acc:  1.0  train loss:  0.0020899719558656214 --time: 0.6993765830993652\n",
      "46 validation:  0.9 --max 0.9 --time: 1.1087210178375244\n",
      "train acc:  1.0  train loss:  0.002020423160865903 --time: 0.7080738544464111\n",
      "47 validation:  0.9 --max 0.9 --time: 1.1170949935913086\n",
      "train acc:  1.0  train loss:  0.0019257733598351478 --time: 0.6879830360412598\n",
      "48 validation:  0.9 --max 0.9 --time: 1.1056935787200928\n",
      "train acc:  1.0  train loss:  0.001909766998142004 --time: 0.678654670715332\n",
      "49 validation:  0.9 --max 0.9 --time: 1.087550401687622\n",
      "train acc:  1.0  train loss:  0.0018548897933214903 --time: 0.6738936901092529\n",
      "50 validation:  0.9 --max 0.9 --time: 1.0837085247039795\n",
      "train acc:  1.0  train loss:  0.0018762875348329543 --time: 0.6671404838562012\n",
      "51 validation:  0.9 --max 0.9 --time: 1.0773861408233643\n",
      "train acc:  1.0  train loss:  0.0017602863488718867 --time: 0.6654024124145508\n",
      "52 validation:  0.9 --max 0.9 --time: 1.0738601684570312\n",
      "train acc:  1.0  train loss:  0.0016847281716763974 --time: 0.6788110733032227\n",
      "53 validation:  0.9 --max 0.9 --time: 1.0884959697723389\n",
      "train acc:  1.0  train loss:  0.0017339314799755812 --time: 0.7001802921295166\n",
      "54 validation:  0.9 --max 0.9 --time: 1.109668254852295\n",
      "train acc:  1.0  train loss:  0.001605100859887898 --time: 0.6748528480529785\n",
      "55 validation:  0.9 --max 0.9 --time: 1.0844042301177979\n",
      "train acc:  1.0  train loss:  0.0016534419730305672 --time: 0.6400852203369141\n",
      "56 validation:  0.9 --max 0.9 --time: 1.0493149757385254\n",
      "train acc:  1.0  train loss:  0.002102584159001708 --time: 0.6087393760681152\n",
      "57 validation:  0.9 --max 0.9 --time: 1.0180668830871582\n",
      "train acc:  1.0  train loss:  0.0016576331807300448 --time: 0.6297929286956787\n",
      "58 validation:  0.9 --max 0.9 --time: 1.0384612083435059\n",
      "train acc:  1.0  train loss:  0.0015134745510295034 --time: 0.6790461540222168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 validation:  0.9 --max 0.9 --time: 1.0878159999847412\n",
      "train acc:  1.0  train loss:  0.0015195765998214483 --time: 0.6682863235473633\n",
      "60 validation:  0.9 --max 0.9 --time: 1.0768110752105713\n",
      "train acc:  1.0  train loss:  0.0013429088285192847 --time: 0.685553789138794\n",
      "61 validation:  0.9 --max 0.9 --time: 1.0945963859558105\n",
      "train acc:  1.0  train loss:  0.0013704062439501286 --time: 0.6707956790924072\n",
      "62 validation:  0.9 --max 0.9 --time: 1.0798630714416504\n",
      "train acc:  1.0  train loss:  0.0012689422583207488 --time: 0.6631002426147461\n",
      "63 validation:  0.9 --max 0.9 --time: 1.0724318027496338\n",
      "train acc:  1.0  train loss:  0.001248785015195608 --time: 0.6506338119506836\n",
      "64 validation:  0.9 --max 0.9 --time: 1.059323787689209\n",
      "train acc:  1.0  train loss:  0.001261731586419046 --time: 0.6642398834228516\n",
      "65 validation:  0.9 --max 0.9 --time: 1.0853466987609863\n",
      "train acc:  1.0  train loss:  0.001174606056883931 --time: 0.7132470607757568\n",
      "66 validation:  0.9 --max 0.9 --time: 1.1221733093261719\n",
      "train acc:  1.0  train loss:  0.0011488943127915263 --time: 0.6785588264465332\n",
      "67 validation:  0.9 --max 0.9 --time: 1.0878562927246094\n",
      "train acc:  1.0  train loss:  0.0010985273867845536 --time: 0.6743919849395752\n",
      "68 validation:  0.9 --max 0.9 --time: 1.0838072299957275\n",
      "train acc:  1.0  train loss:  0.0012421716237440705 --time: 0.6594014167785645\n",
      "69 validation:  0.9 --max 0.9 --time: 1.0688679218292236\n",
      "train acc:  1.0  train loss:  0.0010795784182846546 --time: 0.6902172565460205\n",
      "70 validation:  0.9 --max 0.9 --time: 1.1117870807647705\n",
      "train acc:  1.0  train loss:  0.0012544202851131558 --time: 0.6470746994018555\n",
      "71 validation:  0.9 --max 0.9 --time: 1.055412769317627\n",
      "train acc:  1.0  train loss:  0.0011710857506841421 --time: 0.6444289684295654\n",
      "72 validation:  0.9 --max 0.9 --time: 1.0531611442565918\n",
      "train acc:  1.0  train loss:  0.0010033193160779775 --time: 0.6747415065765381\n",
      "73 validation:  0.9 --max 0.9 --time: 1.0838005542755127\n",
      "train acc:  1.0  train loss:  0.0010405582026578486 --time: 0.6571090221405029\n",
      "74 validation:  0.9 --max 0.9 --time: 1.082374095916748\n",
      "train acc:  1.0  train loss:  0.0009671740233898162 --time: 0.6734580993652344\n",
      "75 validation:  0.9 --max 0.9 --time: 1.0822930335998535\n",
      "train acc:  1.0  train loss:  0.0009701397852040827 --time: 0.6563053131103516\n",
      "76 validation:  0.9 --max 0.9 --time: 1.0654816627502441\n",
      "train acc:  1.0  train loss:  0.0010481582954525947 --time: 0.6625733375549316\n",
      "77 validation:  0.9 --max 0.9 --time: 1.0746676921844482\n",
      "train acc:  1.0  train loss:  0.0009532644762657583 --time: 0.6464588642120361\n",
      "78 validation:  0.9 --max 0.9 --time: 1.0585505962371826\n",
      "train acc:  1.0  train loss:  0.0010264969547279178 --time: 0.6227233409881592\n",
      "79 validation:  0.9 --max 0.9 --time: 1.035538911819458\n",
      "train acc:  1.0  train loss:  0.000946850108448416 --time: 0.6395297050476074\n",
      "80 validation:  0.9 --max 0.9 --time: 1.0511469841003418\n",
      "train acc:  1.0  train loss:  0.0008191163069568574 --time: 0.6585369110107422\n",
      "81 validation:  0.9 --max 0.9 --time: 1.0710070133209229\n",
      "train acc:  1.0  train loss:  0.000797859556041658 --time: 0.6682229042053223\n",
      "82 validation:  0.9 --max 0.9 --time: 1.0913419723510742\n",
      "train acc:  1.0  train loss:  0.0008117049233987928 --time: 0.6885550022125244\n",
      "83 validation:  0.9 --max 0.9 --time: 1.098494291305542\n",
      "train acc:  1.0  train loss:  0.0009124790434725582 --time: 0.6722159385681152\n",
      "84 validation:  0.9 --max 0.9 --time: 1.0808253288269043\n",
      "train acc:  1.0  train loss:  0.0007785117835737765 --time: 0.6753981113433838\n",
      "85 validation:  0.9 --max 0.9 --time: 1.0843019485473633\n",
      "train acc:  1.0  train loss:  0.0007673347019590438 --time: 0.6474413871765137\n",
      "86 validation:  0.9 --max 0.9 --time: 1.0564961433410645\n",
      "train acc:  1.0  train loss:  0.0007549356203526258 --time: 0.6629114151000977\n",
      "87 validation:  0.9 --max 0.9 --time: 1.0726988315582275\n",
      "train acc:  1.0  train loss:  0.0007590233930386603 --time: 0.6496415138244629\n",
      "88 validation:  0.9 --max 0.9 --time: 1.05859375\n",
      "train acc:  1.0  train loss:  0.0007024173391982913 --time: 0.6662700176239014\n",
      "89 validation:  0.9 --max 0.9 --time: 1.0798850059509277\n",
      "train acc:  1.0  train loss:  0.0007292879978194833 --time: 0.6430506706237793\n",
      "90 validation:  0.9 --max 0.9 --time: 1.0548365116119385\n",
      "train acc:  1.0  train loss:  0.0006747080711647868 --time: 0.6896109580993652\n",
      "91 validation:  0.9 --max 0.9 --time: 1.100862979888916\n",
      "train acc:  1.0  train loss:  0.0006522162933833897 --time: 0.6730954647064209\n",
      "92 validation:  0.9 --max 0.9 --time: 1.0852811336517334\n",
      "train acc:  1.0  train loss:  0.0006949514616280794 --time: 0.6670265197753906\n",
      "93 validation:  0.9 --max 0.9 --time: 1.0793206691741943\n",
      "train acc:  1.0  train loss:  0.0006637927028350532 --time: 0.6635017395019531\n",
      "94 validation:  0.9 --max 0.9 --time: 1.0761597156524658\n",
      "train acc:  1.0  train loss:  0.0006708492524921894 --time: 0.6623423099517822\n",
      "95 validation:  0.9 --max 0.9 --time: 1.0748138427734375\n",
      "train acc:  1.0  train loss:  0.0006938701262697577 --time: 0.6220357418060303\n",
      "96 validation:  0.9 --max 0.9 --time: 1.033837080001831\n",
      "train acc:  1.0  train loss:  0.0006588442367501557 --time: 0.6453416347503662\n",
      "97 validation:  0.8875 --max 0.9 --time: 1.0575881004333496\n",
      "train acc:  1.0  train loss:  0.0005967929144389927 --time: 0.6762166023254395\n",
      "98 validation:  0.9 --max 0.9 --time: 1.0888023376464844\n",
      "train acc:  1.0  train loss:  0.0006210614810697734 --time: 0.6571857929229736\n",
      "99 validation:  0.9 --max 0.9 --time: 1.0694379806518555\n",
      "train acc:  1.0  train loss:  0.0006066352827474475 --time: 0.6539537906646729\n",
      "100 validation:  0.9 --max 0.9 --time: 1.066382884979248\n",
      "train acc:  1.0  train loss:  0.0005684933857992291 --time: 0.6680705547332764\n",
      "101 validation:  0.9 --max 0.9 --time: 1.0936319828033447\n",
      "train acc:  1.0  train loss:  0.0007194565376266837 --time: 0.6428732872009277\n",
      "102 validation:  0.8875 --max 0.9 --time: 1.0549254417419434\n",
      "train acc:  1.0  train loss:  0.0005499388673342765 --time: 0.6660759449005127\n",
      "103 validation:  0.8875 --max 0.9 --time: 1.0776019096374512\n",
      "train acc:  1.0  train loss:  0.0005549668101593852 --time: 0.6812043190002441\n",
      "104 validation:  0.8875 --max 0.9 --time: 1.0928997993469238\n",
      "train acc:  1.0  train loss:  0.0005327660182956606 --time: 0.6960020065307617\n",
      "105 validation:  0.8875 --max 0.9 --time: 1.1077942848205566\n",
      "train acc:  1.0  train loss:  0.0005454804166220128 --time: 0.6723263263702393\n",
      "106 validation:  0.8875 --max 0.9 --time: 1.0842998027801514\n",
      "train acc:  1.0  train loss:  0.0005033996945712716 --time: 0.6906096935272217\n",
      "107 validation:  0.9 --max 0.9 --time: 1.1027305126190186\n",
      "train acc:  1.0  train loss:  0.000524705759016797 --time: 0.6533405780792236\n",
      "108 validation:  0.9 --max 0.9 --time: 1.0670342445373535\n",
      "train acc:  1.0  train loss:  0.0004966344917193055 --time: 0.6534948348999023\n",
      "109 validation:  0.9 --max 0.9 --time: 1.0668227672576904\n",
      "train acc:  1.0  train loss:  0.0004960800346452743 --time: 0.68442702293396\n",
      "110 validation:  0.9 --max 0.9 --time: 1.097590446472168\n",
      "train acc:  1.0  train loss:  0.00047349876840598883 --time: 0.6838104724884033\n",
      "111 validation:  0.9 --max 0.9 --time: 1.0962882041931152\n",
      "train acc:  1.0  train loss:  0.00048751747235655787 --time: 0.6461443901062012\n",
      "112 validation:  0.8875 --max 0.9 --time: 1.0584053993225098\n",
      "train acc:  1.0  train loss:  0.00047082676901482045 --time: 0.6605932712554932\n",
      "113 validation:  0.9 --max 0.9 --time: 1.0721626281738281\n",
      "train acc:  1.0  train loss:  0.0005242374900262803 --time: 0.6446318626403809\n",
      "114 validation:  0.8875 --max 0.9 --time: 1.0570027828216553\n",
      "train acc:  1.0  train loss:  0.00044534970656968654 --time: 0.6616301536560059\n",
      "115 validation:  0.9 --max 0.9 --time: 1.0736749172210693\n",
      "train acc:  1.0  train loss:  0.00045266150846146047 --time: 0.6332106590270996\n",
      "116 validation:  0.8875 --max 0.9 --time: 1.0449268817901611\n",
      "train acc:  1.0  train loss:  0.0004557299718726426 --time: 0.6833937168121338\n",
      "117 validation:  0.9 --max 0.9 --time: 1.095871925354004\n",
      "train acc:  1.0  train loss:  0.00042707037646323444 --time: 0.6904385089874268\n",
      "118 validation:  0.9 --max 0.9 --time: 1.1018695831298828\n",
      "train acc:  1.0  train loss:  0.00043774343794211745 --time: 0.6782526969909668\n",
      "119 validation:  0.9 --max 0.9 --time: 1.1051218509674072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  1.0  train loss:  0.0004204548429697752 --time: 0.6956043243408203\n",
      "120 validation:  0.9 --max 0.9 --time: 1.1139469146728516\n",
      "train acc:  1.0  train loss:  0.0004224275762680918 --time: 0.6468467712402344\n",
      "121 validation:  0.9 --max 0.9 --time: 1.0597503185272217\n",
      "train acc:  1.0  train loss:  0.0004350206349045038 --time: 0.6633293628692627\n",
      "122 validation:  0.8875 --max 0.9 --time: 1.0752029418945312\n",
      "train acc:  1.0  train loss:  0.00040714633651077746 --time: 0.6584987640380859\n",
      "123 validation:  0.8875 --max 0.9 --time: 1.0728037357330322\n",
      "train acc:  1.0  train loss:  0.0004149166110437363 --time: 0.6467995643615723\n",
      "124 validation:  0.8875 --max 0.9 --time: 1.0585107803344727\n",
      "train acc:  1.0  train loss:  0.0003998524509370327 --time: 0.6976003646850586\n",
      "125 validation:  0.9 --max 0.9 --time: 1.1092073917388916\n",
      "train acc:  1.0  train loss:  0.00038583410205319526 --time: 0.6617221832275391\n",
      "126 validation:  0.9 --max 0.9 --time: 1.074068307876587\n",
      "train acc:  1.0  train loss:  0.0003818399680312723 --time: 0.6904957294464111\n",
      "127 validation:  0.9 --max 0.9 --time: 1.1032371520996094\n",
      "train acc:  1.0  train loss:  0.0003794523130636662 --time: 0.6820173263549805\n",
      "128 validation:  0.9 --max 0.9 --time: 1.0939714908599854\n",
      "train acc:  1.0  train loss:  0.0003776214725803584 --time: 0.6522698402404785\n",
      "129 validation:  0.8875 --max 0.9 --time: 1.0636875629425049\n",
      "train acc:  1.0  train loss:  0.0004099614801816642 --time: 0.6433918476104736\n",
      "130 validation:  0.8875 --max 0.9 --time: 1.0554585456848145\n",
      "train acc:  1.0  train loss:  0.0003745307738427073 --time: 0.6709144115447998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f70e0bcb7c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mout_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ff4d744c2711>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths, lm, pretrained)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(200):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths, False, pretrained)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths, False, pretrained)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "    \n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 layer, 512, only 2 CNN contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
