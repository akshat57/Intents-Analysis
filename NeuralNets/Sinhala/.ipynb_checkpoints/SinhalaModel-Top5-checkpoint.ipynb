{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/akshatgu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshatgu/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                if len(utterance) != 0:\n",
    "                    utterance_to_idx = []\n",
    "\n",
    "                    for phone in utterance:\n",
    "                        if phone not in phone_to_idx:\n",
    "                            phone = 'unk'\n",
    "\n",
    "                        utterance_to_idx.append(phone_to_idx[phone])\n",
    "\n",
    "                    self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.tensor(y_lst)\n",
    "\n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intents():\n",
    "    all_intents = ['1', '2', '3', '4', '5', '6']\n",
    "    return all_intents\n",
    "\n",
    "def get_intent_labels(class_type):\n",
    "    all_intents = get_intents()\n",
    "        \n",
    "    intent_labels = {}\n",
    "    labels_to_intents = {}\n",
    "    for i, intent in enumerate(all_intents):\n",
    "        intent_labels[intent] = i\n",
    "        labels_to_intents[i] = intent\n",
    "        \n",
    "    return intent_labels, labels_to_intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "class_type = 'intents'\n",
    "\n",
    "intent_labels, labels_to_intents = get_intent_labels(class_type)\n",
    "\n",
    "#Loading data\n",
    "split = '1'\n",
    "train_file = '../../Sinhala_Dataset/datasplit1/sinhala_train_split_' + split + '.pkl'\n",
    "test_file = '../../Sinhala_Dataset/datasplit1/sinhala_test_split_' + split + '.pkl'\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=64)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=37, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        #self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*2)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        #cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input)], dim=1)\n",
    "        \n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "        \n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(37, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents 1\n",
      "train acc:  0.7461046416270297  train loss:  0.7365960123327872 --time: 11.006566047668457\n",
      "0 validation:  0.8740157480314961 --max 0.8740157480314961 --time: 16.560962438583374\n",
      "train acc:  0.9325897982614401  train loss:  0.21424799241746464 --time: 10.780134439468384\n",
      "1 validation:  0.9396325459317585 --max 0.9396325459317585 --time: 16.347294092178345\n",
      "train acc:  0.9558799409545679  train loss:  0.14464169422475 --time: 10.649631261825562\n",
      "2 validation:  0.9409448818897638 --max 0.9409448818897638 --time: 16.22294569015503\n",
      "train acc:  0.9712973593570609  train loss:  0.09785027856317659 --time: 10.65075397491455\n",
      "3 validation:  0.9416010498687664 --max 0.9416010498687664 --time: 16.20781922340393\n",
      "train acc:  0.9785140232901427  train loss:  0.07069561533474673 --time: 10.67843246459961\n",
      "4 validation:  0.9363517060367454 --max 0.9416010498687664 --time: 16.26165533065796\n",
      "train acc:  0.9757257667705429  train loss:  0.07569409540155903 --time: 10.688835620880127\n",
      "5 validation:  0.9599737532808399 --max 0.9599737532808399 --time: 16.20818281173706\n",
      "train acc:  0.9862227324913893  train loss:  0.04202198790881084 --time: 10.65245795249939\n",
      "6 validation:  0.9553805774278216 --max 0.9599737532808399 --time: 16.199629306793213\n",
      "train acc:  0.9899950795473184  train loss:  0.030903676681191428 --time: 10.501930713653564\n",
      "7 validation:  0.9442257217847769 --max 0.9599737532808399 --time: 16.026485681533813\n",
      "train acc:  0.9936034115138592  train loss:  0.022433913120039506 --time: 10.645910501480103\n",
      "8 validation:  0.952755905511811 --max 0.9599737532808399 --time: 16.198084592819214\n",
      "train acc:  0.9931113662456946  train loss:  0.020644337457876343 --time: 10.613626480102539\n",
      "9 validation:  0.9455380577427821 --max 0.9599737532808399 --time: 16.13822102546692\n",
      "train acc:  0.9940954567820239  train loss:  0.019755389235191007 --time: 10.598670482635498\n",
      "10 validation:  0.952755905511811 --max 0.9599737532808399 --time: 16.129159927368164\n",
      "train acc:  0.9924553058881417  train loss:  0.022644134995061904 --time: 10.716445922851562\n",
      "11 validation:  0.9514435695538058 --max 0.9599737532808399 --time: 16.296683073043823\n",
      "train acc:  0.9891750041003773  train loss:  0.038264274330382854 --time: 10.73499083518982\n",
      "12 validation:  0.9507874015748031 --max 0.9599737532808399 --time: 16.325755834579468\n",
      "train acc:  0.9850746268656716  train loss:  0.043155606519576395 --time: 10.697498321533203\n",
      "13 validation:  0.9698162729658792 --max 0.9698162729658792 --time: 16.262752771377563\n",
      "train acc:  0.9968837133016237  train loss:  0.011231449057049758 --time: 10.597426176071167\n",
      "14 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.17171835899353\n",
      "train acc:  0.9988518943742825  train loss:  0.004530302165221656 --time: 10.648441791534424\n",
      "15 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.229064464569092\n",
      "train acc:  0.9991799245530589  train loss:  0.003338871768088817 --time: 10.582314252853394\n",
      "16 validation:  0.9606299212598425 --max 0.9698162729658792 --time: 16.180303812026978\n",
      "train acc:  0.9990159094636707  train loss:  0.0035093996420982876 --time: 10.649186611175537\n",
      "17 validation:  0.9547244094488189 --max 0.9698162729658792 --time: 16.245208024978638\n",
      "train acc:  0.9988518943742825  train loss:  0.004598592845923122 --time: 10.584885358810425\n",
      "18 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.180272817611694\n",
      "train acc:  0.9991799245530589  train loss:  0.0026083672879243145 --time: 10.694723844528198\n",
      "19 validation:  0.9619422572178478 --max 0.9698162729658792 --time: 16.264031410217285\n",
      "train acc:  0.9998359849106118  train loss:  0.0015929655166170658 --time: 10.701148509979248\n",
      "20 validation:  0.9645669291338582 --max 0.9698162729658792 --time: 16.266300439834595\n",
      "train acc:  0.998523864195506  train loss:  0.004091014814018005 --time: 10.579416036605835\n",
      "21 validation:  0.9553805774278216 --max 0.9698162729658792 --time: 16.143516778945923\n",
      "train acc:  0.999343939642447  train loss:  0.0027018134946956707 --time: 10.60993480682373\n",
      "22 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.19449257850647\n",
      "train acc:  0.999343939642447  train loss:  0.0017737018721769953 --time: 10.616132736206055\n",
      "23 validation:  0.9652230971128609 --max 0.9698162729658792 --time: 16.19706082344055\n",
      "train acc:  0.9996719698212235  train loss:  0.0010222205023637798 --time: 10.593949794769287\n",
      "24 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.138782262802124\n",
      "train acc:  0.9996719698212235  train loss:  0.0007774086886911391 --time: 10.623689889907837\n",
      "25 validation:  0.9671916010498688 --max 0.9698162729658792 --time: 16.15473699569702\n",
      "train acc:  0.9998359849106118  train loss:  0.0006462832689824912 --time: 10.712033987045288\n",
      "26 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.254793643951416\n",
      "train acc:  0.9998359849106118  train loss:  0.0006078259472512096 --time: 10.659953832626343\n",
      "27 validation:  0.9652230971128609 --max 0.9698162729658792 --time: 16.22237491607666\n",
      "train acc:  0.9996719698212235  train loss:  0.0005102221818636584 --time: 10.721680641174316\n",
      "28 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.335714101791382\n",
      "train acc:  0.9998359849106118  train loss:  0.0004567410372449861 --time: 10.78010892868042\n",
      "29 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.378289699554443\n",
      "train acc:  0.9998359849106118  train loss:  0.0005962959164662607 --time: 10.67956829071045\n",
      "30 validation:  0.9652230971128609 --max 0.9698162729658792 --time: 16.253735303878784\n",
      "train acc:  0.9996719698212235  train loss:  0.000624800491057916 --time: 10.671547651290894\n",
      "31 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.24897789955139\n",
      "train acc:  0.9998359849106118  train loss:  0.0004891556723691792 --time: 10.615036249160767\n",
      "32 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.190271377563477\n",
      "train acc:  0.9998359849106118  train loss:  0.0004637614417409471 --time: 10.708223342895508\n",
      "33 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.28347945213318\n",
      "train acc:  0.9996719698212235  train loss:  0.0004463790515956134 --time: 10.784618139266968\n",
      "34 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.364824771881104\n",
      "train acc:  0.9998359849106118  train loss:  0.0003519801788153624 --time: 10.610779762268066\n",
      "35 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.190027236938477\n",
      "train acc:  0.9996719698212235  train loss:  0.0004118201136312412 --time: 10.659926891326904\n",
      "36 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.24371838569641\n",
      "train acc:  0.9998359849106118  train loss:  0.00034710138111411953 --time: 10.775679588317871\n",
      "37 validation:  0.9645669291338582 --max 0.9698162729658792 --time: 16.349717378616333\n",
      "train acc:  0.9996719698212235  train loss:  0.0004138047330760249 --time: 10.614035367965698\n",
      "38 validation:  0.9645669291338582 --max 0.9698162729658792 --time: 16.231878757476807\n",
      "train acc:  0.9996719698212235  train loss:  0.0003733094553505604 --time: 10.619227647781372\n",
      "39 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.205654621124268\n",
      "train acc:  0.9996719698212235  train loss:  0.00039020736403472256 --time: 10.61989688873291\n",
      "40 validation:  0.9645669291338582 --max 0.9698162729658792 --time: 16.198235273361206\n",
      "train acc:  0.9998359849106118  train loss:  0.00037864774880821034 --time: 10.586332559585571\n",
      "41 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.198428630828857\n",
      "train acc:  0.9998359849106118  train loss:  0.0003888448451524103 --time: 10.734366178512573\n",
      "42 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.29447603225708\n",
      "train acc:  0.9996719698212235  train loss:  0.0004371793963192279 --time: 10.631526708602905\n",
      "43 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.174665689468384\n",
      "train acc:  0.9998359849106118  train loss:  0.0003615800761356998 --time: 10.630517959594727\n",
      "44 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.162408113479614\n",
      "train acc:  0.9996719698212235  train loss:  0.0003696028961333771 --time: 10.577227354049683\n",
      "45 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.11948037147522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9998359849106118  train loss:  0.0003588955394207005 --time: 10.588608503341675\n",
      "46 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.149760723114014\n",
      "train acc:  0.9998359849106118  train loss:  0.000374831695779676 --time: 10.620946884155273\n",
      "47 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.163105249404907\n",
      "train acc:  0.9998359849106118  train loss:  0.00032545182335752543 --time: 10.730013608932495\n",
      "48 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.31227421760559\n",
      "train acc:  0.9996719698212235  train loss:  0.00036807389475749613 --time: 10.686828136444092\n",
      "49 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.303526163101196\n",
      "train acc:  0.9998359849106118  train loss:  0.00035418814071401056 --time: 10.702310562133789\n",
      "50 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.275645971298218\n",
      "train acc:  0.9998359849106118  train loss:  0.00035479063688133766 --time: 10.682584524154663\n",
      "51 validation:  0.9639107611548556 --max 0.9698162729658792 --time: 16.263392448425293\n",
      "train acc:  0.9998359849106118  train loss:  0.00028335792883164385 --time: 10.754754066467285\n",
      "52 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.347517490386963\n",
      "train acc:  0.9998359849106118  train loss:  0.0003457685509242007 --time: 10.701631784439087\n",
      "53 validation:  0.9619422572178478 --max 0.9698162729658792 --time: 16.288705348968506\n",
      "train acc:  0.9998359849106118  train loss:  0.00035100970606076015 --time: 10.699378490447998\n",
      "54 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.303524017333984\n",
      "train acc:  0.9998359849106118  train loss:  0.00033920041404182183 --time: 10.633415699005127\n",
      "55 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.23562479019165\n",
      "train acc:  0.9998359849106118  train loss:  0.0002769001110560036 --time: 10.623367547988892\n",
      "56 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.202152252197266\n",
      "train acc:  0.9998359849106118  train loss:  0.00033625325895059177 --time: 10.61132287979126\n",
      "57 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.21617293357849\n",
      "train acc:  0.9996719698212235  train loss:  0.00034935053682734935 --time: 10.763670206069946\n",
      "58 validation:  0.9619422572178478 --max 0.9698162729658792 --time: 16.346721172332764\n",
      "train acc:  0.9998359849106118  train loss:  0.0003372419315468278 --time: 10.700613975524902\n",
      "59 validation:  0.9619422572178478 --max 0.9698162729658792 --time: 16.254242181777954\n",
      "train acc:  0.9998359849106118  train loss:  0.00035321530095681436 --time: 10.6515052318573\n",
      "60 validation:  0.963254593175853 --max 0.9698162729658792 --time: 16.21104669570923\n",
      "train acc:  0.9996719698212235  train loss:  0.00035633920227458776 --time: 10.659413814544678\n",
      "61 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.243059635162354\n",
      "train acc:  0.9996719698212235  train loss:  0.00036920275013585524 --time: 10.61000108718872\n",
      "62 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.151562452316284\n",
      "train acc:  0.9996719698212235  train loss:  0.00036872103971556197 --time: 10.709053754806519\n",
      "63 validation:  0.9625984251968503 --max 0.9698162729658792 --time: 16.263967275619507\n",
      "train acc:  0.9996719698212235  train loss:  0.0003696333096172566 --time: 10.667775630950928\n",
      "64 validation:  0.9619422572178478 --max 0.9698162729658792 --time: 16.267205953598022\n",
      "train acc:  0.9996719698212235  train loss:  0.00037114339005484 --time: 10.738783836364746\n",
      "65 validation:  0.9606299212598425 --max 0.9698162729658792 --time: 16.340259552001953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d482af09133f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(class_type, split)\n",
    "max_acc = 0\n",
    "\n",
    "for j in range(150):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "                    \n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        tar_indices = y\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(j, \"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 layer, 512, only 2 CNN contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
