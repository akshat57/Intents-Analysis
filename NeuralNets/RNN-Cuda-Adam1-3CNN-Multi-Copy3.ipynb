{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ubuntu/Intents-Analysis/Analysis')\n",
    "#sys.path.insert(1, '/Users/manjugupta/Desktop/CMU_Courses/Intents/getting_intents/Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_vocab import load_data, get_vocab\n",
    "from get_frequency import get_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is True\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Needed Functions\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_vocabulary(train_file):\n",
    "    '''This function creates an indexed vocabulary dictionary from the training file'''\n",
    "    \n",
    "    vocab, _ = get_vocab(1, train_file)\n",
    "    \n",
    "    phone_to_idx = {'unk': 1}#Padding indx = 0, unkown_idx = 1, indexing starts from 2\n",
    "    for i, phone in enumerate(vocab):\n",
    "        phone_to_idx[phone] = i + 2\n",
    "        \n",
    "    return phone_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_file, intent_labels, phone_to_idx):\n",
    "        data = load_data(data_file)\n",
    "        self.all_data = []\n",
    "        \n",
    "        for intent in data:\n",
    "            for utterance in data[intent]:\n",
    "                utterance_to_idx = []\n",
    "                \n",
    "                for phone in utterance:\n",
    "                    if phone not in phone_to_idx:\n",
    "                        phone = 'unk'\n",
    "    \n",
    "                    utterance_to_idx.append(phone_to_idx[phone])\n",
    "                \n",
    "                self.all_data.append([utterance_to_idx, intent_labels[intent]])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_vector = self.all_data[index][0]\n",
    "        label = self.all_data[index][1]\n",
    "\n",
    "        return input_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_indic(tuple_lst):\n",
    "\n",
    "    x_lst = [x[0] for x in tuple_lst]\n",
    "    y_lst = [x[1] for x in tuple_lst]\n",
    "\n",
    "    # collate x\n",
    "    B = len(tuple_lst)#Number of training samples\n",
    "    T = max(len(x) for x in x_lst)#Max length of a sentence\n",
    "\n",
    "    # x values\n",
    "    x = torch.zeros([B, T], dtype=torch.int64)\n",
    "    lengths = torch.zeros(B, dtype=torch.int64)\n",
    "\n",
    "    for i, x_np in enumerate(x_lst):\n",
    "        lengths[i] = len(x_np)\n",
    "        x[i,:len(x_np)] = torch.tensor(x_np)\n",
    "\n",
    "    # collate y\n",
    "    y = torch.zeros([B, 6])\n",
    "    for i, y_label in enumerate(y_lst):\n",
    "        y[i][y_label] = 1\n",
    "        \n",
    "    ids = torch.argsort(lengths, descending=True)\n",
    "\n",
    "    return x[ids], lengths[ids], y[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "#Defining constants and labels\n",
    "intent_labels = {'movie-tickets':0, 'auto-repair':1, 'restaurant-table':2, 'pizza-ordering':3, 'uber-lyft':4, 'coffee-ordering':5}\n",
    "train_language = 'gujarati_marathi_bengali_hindi_20'\n",
    "test_language = 'hindi'\n",
    "\n",
    "#Loading data\n",
    "train_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/3_lang_variations/taskmaster_training_' + train_language + '.pkl'\n",
    "test_file = '/home/ubuntu/Intents-Analysis/TaskMasterData/Get_Phones_Combos/1_language/taskmaster_testing_' + test_language + '.pkl'\n",
    "\n",
    "#create vocabulary and phone_to_idx\n",
    "phone_to_idx = create_vocabulary(train_file)\n",
    "print(len(phone_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_file, intent_labels, phone_to_idx)\n",
    "train_loader_args = dict(shuffle=True, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args, collate_fn=collate_indic)\n",
    "\n",
    "test_dataset = MyDataset(test_file, intent_labels, phone_to_idx)\n",
    "test_loader_args = dict(shuffle=False, batch_size=128, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=False, batch_size=1)\n",
    "valid_loader = DataLoader(test_dataset, **test_loader_args, collate_fn=collate_indic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=70, embed_size=128, hidden_size=128, label_size=6):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.cnn  = nn.Conv1d(embed_size, embed_size, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(embed_size, embed_size, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(embed_size, embed_size, kernel_size=7, padding=3)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_size*3)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size*3, hidden_size, num_layers=2)\n",
    "        self.linear = nn.Linear(hidden_size, label_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        padded_x: (B,T) padded LongTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # B,T,H\n",
    "        input = self.embed(x)\n",
    "\n",
    "        # (B,T,H) -> (B,H,T)\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        cnn_output = torch.cat([self.cnn(input), self.cnn2(input), self.cnn3(input)], dim=1)\n",
    "\n",
    "        # (B,H,T)\n",
    "        input = F.relu(self.batchnorm(cnn_output))\n",
    "\n",
    "        input = input.transpose(1,2)\n",
    "\n",
    "        pack_tensor = nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=True)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(pack_tensor)\n",
    "\n",
    "        #output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        #output = torch.cat([hn[0], hn[1]], dim=1)\n",
    "        logits = self.linear(hn[0])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embed): Embedding(70, 128)\n",
       "  (cnn): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (cnn2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (cnn3): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (batchnorm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(384, 128, num_layers=2)\n",
       "  (linear): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#opt = SGD(model.parameters(), lr=0.05)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gujarati_marathi_bengali_hindi_20 hindi\n",
      "train acc:  0.27217125382262997  train loss:  0.4633424489394478 --time: 4.786038398742676\n",
      "validation:  0.16 --max 0.16 --time: 5.238126993179321\n",
      "train acc:  0.30173292558613657  train loss:  0.4316557114538939 --time: 4.301440715789795\n",
      "validation:  0.17666666666666667 --max 0.17666666666666667 --time: 4.62121057510376\n",
      "train acc:  0.33401291199456334  train loss:  0.41968049562495685 --time: 4.922960519790649\n",
      "validation:  0.17333333333333334 --max 0.17666666666666667 --time: 5.299213647842407\n",
      "train acc:  0.37954468229697585  train loss:  0.4052054622898931 --time: 4.873610973358154\n",
      "validation:  0.23 --max 0.23 --time: 5.375410318374634\n",
      "train acc:  0.4260958205912334  train loss:  0.39071877365526947 --time: 4.776900768280029\n",
      "validation:  0.2633333333333333 --max 0.2633333333333333 --time: 5.369486093521118\n",
      "train acc:  0.5015290519877675  train loss:  0.364674836397171 --time: 4.6566221714019775\n",
      "validation:  0.36333333333333334 --max 0.36333333333333334 --time: 5.155598163604736\n",
      "train acc:  0.5538566089024805  train loss:  0.3334816383278888 --time: 4.645954608917236\n",
      "validation:  0.4766666666666667 --max 0.4766666666666667 --time: 5.019190311431885\n",
      "train acc:  0.6133197417601087  train loss:  0.2953140243240025 --time: 4.406708240509033\n",
      "validation:  0.49333333333333335 --max 0.49333333333333335 --time: 4.887489080429077\n",
      "train acc:  0.6591913013931363  train loss:  0.2617896555558495 --time: 4.379506349563599\n",
      "validation:  0.52 --max 0.52 --time: 4.866953611373901\n",
      "train acc:  0.6826367652055726  train loss:  0.23614665347596872 --time: 3.9741103649139404\n",
      "validation:  0.52 --max 0.52 --time: 4.620511531829834\n",
      "train acc:  0.6819571865443425  train loss:  0.23979013834310614 --time: 3.9392123222351074\n",
      "validation:  0.47 --max 0.52 --time: 4.527337074279785\n",
      "train acc:  0.7200135915732246  train loss:  0.21402928362721982 --time: 4.095068693161011\n",
      "validation:  0.5533333333333333 --max 0.5533333333333333 --time: 4.745140552520752\n",
      "train acc:  0.7526333673122664  train loss:  0.19540596526602041 --time: 4.079442501068115\n",
      "validation:  0.5766666666666667 --max 0.5766666666666667 --time: 4.73184609413147\n",
      "train acc:  0.7866123003737683  train loss:  0.17786431312561035 --time: 4.019097089767456\n",
      "validation:  0.6 --max 0.6 --time: 4.625506401062012\n",
      "train acc:  0.8267074413863404  train loss:  0.15942516922950745 --time: 4.266934394836426\n",
      "validation:  0.6366666666666667 --max 0.6366666666666667 --time: 4.90972113609314\n",
      "train acc:  0.8457356439007815  train loss:  0.14539763979289844 --time: 4.18944525718689\n",
      "validation:  0.66 --max 0.66 --time: 4.788461923599243\n",
      "train acc:  0.8647638464152225  train loss:  0.12939054162605948 --time: 3.9267454147338867\n",
      "validation:  0.6333333333333333 --max 0.66 --time: 4.539205312728882\n",
      "train acc:  0.8732585796805981  train loss:  0.12219230344761974 --time: 4.038028240203857\n",
      "validation:  0.68 --max 0.68 --time: 4.661861181259155\n",
      "train acc:  0.8892286782195039  train loss:  0.10975082639766776 --time: 4.104393005371094\n",
      "validation:  0.6766666666666666 --max 0.68 --time: 4.769655704498291\n",
      "train acc:  0.8810737342847434  train loss:  0.11173723638057709 --time: 4.3053600788116455\n",
      "validation:  0.66 --max 0.68 --time: 4.9355409145355225\n",
      "train acc:  0.890587835541964  train loss:  0.1086827053324036 --time: 4.369762659072876\n",
      "validation:  0.6966666666666667 --max 0.6966666666666667 --time: 5.010317325592041\n",
      "train acc:  0.9099558273870201  train loss:  0.09072662242080855 --time: 4.196214437484741\n",
      "validation:  0.6833333333333333 --max 0.6966666666666667 --time: 4.773461580276489\n",
      "train acc:  0.9157322460074754  train loss:  0.08158935185359872 --time: 4.176356077194214\n",
      "validation:  0.6933333333333334 --max 0.6966666666666667 --time: 4.66859245300293\n",
      "train acc:  0.9238871899422358  train loss:  0.07834164393336876 --time: 4.311760902404785\n",
      "validation:  0.7133333333333334 --max 0.7133333333333334 --time: 4.857492208480835\n",
      "train acc:  0.9442745497791369  train loss:  0.06565622207911118 --time: 4.319641590118408\n",
      "validation:  0.73 --max 0.73 --time: 4.935986042022705\n",
      "train acc:  0.9490316004077471  train loss:  0.05703759501161783 --time: 4.442384481430054\n",
      "validation:  0.7 --max 0.73 --time: 4.9690492153167725\n",
      "train acc:  0.9554875976894326  train loss:  0.054454953774161964 --time: 4.526024580001831\n",
      "validation:  0.7266666666666667 --max 0.73 --time: 5.168257474899292\n",
      "train acc:  0.9643221202854231  train loss:  0.04868072400922361 --time: 4.464309453964233\n",
      "validation:  0.74 --max 0.74 --time: 4.967408895492554\n",
      "train acc:  0.9721372748895685  train loss:  0.03933601551081823 --time: 3.8927040100097656\n",
      "validation:  0.72 --max 0.74 --time: 4.487404108047485\n",
      "train acc:  0.9833503227998641  train loss:  0.030324791922517445 --time: 3.855992317199707\n",
      "validation:  0.7233333333333334 --max 0.74 --time: 4.523940801620483\n",
      "train acc:  0.9707781175671084  train loss:  0.03894196620777897 --time: 3.576160192489624\n",
      "validation:  0.7033333333333334 --max 0.74 --time: 4.209258794784546\n",
      "train acc:  0.9840299014610941  train loss:  0.029050986889911735 --time: 3.5868401527404785\n",
      "validation:  0.7333333333333333 --max 0.74 --time: 4.2288525104522705\n",
      "train acc:  0.9870880054366293  train loss:  0.02499448856257874 --time: 3.826014757156372\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 4.423825979232788\n",
      "train acc:  0.9721372748895685  train loss:  0.03232890140751134 --time: 4.125828504562378\n",
      "validation:  0.7 --max 0.7533333333333333 --time: 4.699496269226074\n",
      "train acc:  0.9537886510363575  train loss:  0.051213421413432 --time: 4.314734220504761\n",
      "validation:  0.69 --max 0.7533333333333333 --time: 4.681276321411133\n",
      "train acc:  0.9585457016649678  train loss:  0.04598912540013376 --time: 4.7606048583984375\n",
      "validation:  0.7433333333333333 --max 0.7533333333333333 --time: 5.0948405265808105\n",
      "train acc:  0.9779136935100238  train loss:  0.028340416555495365 --time: 4.626398324966431\n",
      "validation:  0.7333333333333333 --max 0.7533333333333333 --time: 4.982473134994507\n",
      "train acc:  0.982330954808019  train loss:  0.024154204915723076 --time: 4.584577322006226\n",
      "validation:  0.7533333333333333 --max 0.7533333333333333 --time: 5.003137588500977\n",
      "train acc:  0.9826707441386341  train loss:  0.02589153806152551 --time: 4.714179992675781\n",
      "validation:  0.7433333333333333 --max 0.7533333333333333 --time: 5.238340139389038\n",
      "train acc:  0.9942235813795447  train loss:  0.013044527407897554 --time: 4.797746181488037\n",
      "validation:  0.75 --max 0.7533333333333333 --time: 5.316795825958252\n",
      "train acc:  0.9935440027183147  train loss:  0.012083016578917917 --time: 4.616617918014526\n",
      "validation:  0.7433333333333333 --max 0.7533333333333333 --time: 5.062090158462524\n",
      "train acc:  0.9921848453958546  train loss:  0.012828186680765255 --time: 4.548668146133423\n",
      "validation:  0.75 --max 0.7533333333333333 --time: 4.92823600769043\n",
      "train acc:  0.9962623173632348  train loss:  0.010362890731219364 --time: 4.285751581192017\n",
      "validation:  0.7466666666666667 --max 0.7533333333333333 --time: 4.8430540561676025\n",
      "train acc:  0.9986408426775399  train loss:  0.007310237695017587 --time: 4.214340686798096\n",
      "validation:  0.75 --max 0.7533333333333333 --time: 4.809034585952759\n",
      "train acc:  0.9966021066938499  train loss:  0.007157114172435325 --time: 3.8301291465759277\n",
      "validation:  0.7733333333333333 --max 0.7733333333333333 --time: 4.497567415237427\n",
      "train acc:  0.9925246347264696  train loss:  0.01117406593149771 --time: 3.486454963684082\n",
      "validation:  0.73 --max 0.7733333333333333 --time: 4.120376110076904\n",
      "train acc:  0.9942235813795447  train loss:  0.011136297035314466 --time: 4.014959335327148\n",
      "validation:  0.76 --max 0.7733333333333333 --time: 4.620283842086792\n",
      "train acc:  0.9979612640163099  train loss:  0.007429608872727207 --time: 3.8632190227508545\n",
      "validation:  0.7566666666666667 --max 0.7733333333333333 --time: 4.459353446960449\n",
      "train acc:  0.9989806320081549  train loss:  0.005201820446097333 --time: 4.187210559844971\n",
      "validation:  0.77 --max 0.7733333333333333 --time: 4.811595439910889\n",
      "train acc:  0.9983010533469249  train loss:  0.004564434411409108 --time: 4.058198928833008\n",
      "validation:  0.7766666666666666 --max 0.7766666666666666 --time: 4.610534906387329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.99932042133877  train loss:  0.0038450973958748837 --time: 4.1427741050720215\n",
      "validation:  0.7733333333333333 --max 0.7766666666666666 --time: 4.678637981414795\n",
      "train acc:  1.0  train loss:  0.0033665136737829966 --time: 4.548661947250366\n",
      "validation:  0.78 --max 0.78 --time: 5.155264377593994\n",
      "train acc:  1.0  train loss:  0.002814120046146538 --time: 4.457099676132202\n",
      "validation:  0.7733333333333333 --max 0.78 --time: 5.086412191390991\n",
      "train acc:  1.0  train loss:  0.0024613531043186135 --time: 4.475693941116333\n",
      "validation:  0.7766666666666666 --max 0.78 --time: 5.096670627593994\n",
      "train acc:  1.0  train loss:  0.002278698102125655 --time: 4.4670937061309814\n",
      "validation:  0.77 --max 0.78 --time: 5.032593727111816\n",
      "train acc:  1.0  train loss:  0.002138580291774934 --time: 4.393969774246216\n",
      "validation:  0.76 --max 0.78 --time: 5.004671335220337\n",
      "train acc:  1.0  train loss:  0.0020105725042926874 --time: 4.3120763301849365\n",
      "validation:  0.7533333333333333 --max 0.78 --time: 4.890139102935791\n",
      "train acc:  1.0  train loss:  0.0019107617883254652 --time: 4.53348445892334\n",
      "validation:  0.7533333333333333 --max 0.78 --time: 5.06605863571167\n",
      "train acc:  1.0  train loss:  0.0017850756513602707 --time: 4.358498811721802\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 4.8302412033081055\n",
      "train acc:  1.0  train loss:  0.0017723324000025573 --time: 4.205363035202026\n",
      "validation:  0.76 --max 0.78 --time: 4.885125160217285\n",
      "train acc:  1.0  train loss:  0.001641859437096054 --time: 3.996333599090576\n",
      "validation:  0.75 --max 0.78 --time: 4.580184459686279\n",
      "train acc:  1.0  train loss:  0.001561037259702773 --time: 3.7215614318847656\n",
      "validation:  0.7533333333333333 --max 0.78 --time: 4.343069553375244\n",
      "train acc:  1.0  train loss:  0.0014937191887799165 --time: 3.6752419471740723\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 4.368107080459595\n",
      "train acc:  1.0  train loss:  0.0014036895818603427 --time: 4.057651042938232\n",
      "validation:  0.75 --max 0.78 --time: 4.63320517539978\n",
      "train acc:  0.9996602106693849  train loss:  0.0015011870241521494 --time: 4.081874132156372\n",
      "validation:  0.74 --max 0.78 --time: 4.728878736495972\n",
      "train acc:  1.0  train loss:  0.0013252833969486148 --time: 4.24216890335083\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 4.840062141418457\n",
      "train acc:  1.0  train loss:  0.001260754515659874 --time: 4.122502088546753\n",
      "validation:  0.74 --max 0.78 --time: 4.737399101257324\n",
      "train acc:  1.0  train loss:  0.0011951531961803203 --time: 4.355262517929077\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 5.011112928390503\n",
      "train acc:  1.0  train loss:  0.0011552647009248967 --time: 4.190006256103516\n",
      "validation:  0.75 --max 0.78 --time: 4.754772663116455\n",
      "train acc:  1.0  train loss:  0.0011224077746231594 --time: 4.443896293640137\n",
      "validation:  0.74 --max 0.78 --time: 5.030624151229858\n",
      "train acc:  1.0  train loss:  0.0010767769620186932 --time: 4.427779197692871\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 4.8727171421051025\n",
      "train acc:  1.0  train loss:  0.001016455459529939 --time: 4.465850114822388\n",
      "validation:  0.74 --max 0.78 --time: 5.023510932922363\n",
      "train acc:  1.0  train loss:  0.0009764063313765371 --time: 4.509273290634155\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 5.098787784576416\n",
      "train acc:  1.0  train loss:  0.0009434430723321502 --time: 4.540740966796875\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 4.942288637161255\n",
      "train acc:  1.0  train loss:  0.0009208709411525532 --time: 4.526385068893433\n",
      "validation:  0.75 --max 0.78 --time: 4.9343132972717285\n",
      "train acc:  1.0  train loss:  0.0008824003794554459 --time: 4.392540693283081\n",
      "validation:  0.7533333333333333 --max 0.78 --time: 5.0035927295684814\n",
      "train acc:  1.0  train loss:  0.0008533279102741052 --time: 4.179906606674194\n",
      "validation:  0.75 --max 0.78 --time: 4.774768829345703\n",
      "train acc:  1.0  train loss:  0.0008417676005552968 --time: 3.415264129638672\n",
      "validation:  0.7366666666666667 --max 0.78 --time: 4.0244364738464355\n",
      "train acc:  1.0  train loss:  0.0008043501774132576 --time: 3.8705389499664307\n",
      "validation:  0.74 --max 0.78 --time: 4.502173185348511\n",
      "train acc:  1.0  train loss:  0.0008104053128551205 --time: 3.723620653152466\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 4.394955635070801\n",
      "train acc:  1.0  train loss:  0.0007561898243654033 --time: 3.9872264862060547\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 4.664621591567993\n",
      "train acc:  1.0  train loss:  0.0007293322933432848 --time: 4.374874830245972\n",
      "validation:  0.75 --max 0.78 --time: 4.972581386566162\n",
      "train acc:  1.0  train loss:  0.0007131283232213362 --time: 4.484258413314819\n",
      "validation:  0.7566666666666667 --max 0.78 --time: 5.089672565460205\n",
      "train acc:  1.0  train loss:  0.0006849137767541991 --time: 4.301382064819336\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 4.817490100860596\n",
      "train acc:  1.0  train loss:  0.0006643639897684689 --time: 4.539234399795532\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 5.192758560180664\n",
      "train acc:  1.0  train loss:  0.0006474063645683876 --time: 4.814063549041748\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 5.639616250991821\n",
      "train acc:  1.0  train loss:  0.0006261482460262334 --time: 7.900642156600952\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 8.736325740814209\n",
      "train acc:  1.0  train loss:  0.0006085920790412828 --time: 7.239982604980469\n",
      "validation:  0.7433333333333333 --max 0.78 --time: 8.133830070495605\n",
      "train acc:  1.0  train loss:  0.0006024672791523778 --time: 7.942229509353638\n",
      "validation:  0.7533333333333333 --max 0.78 --time: 8.963706016540527\n",
      "train acc:  1.0  train loss:  0.0005875461400769975 --time: 6.978883981704712\n",
      "validation:  0.75 --max 0.78 --time: 8.051666736602783\n",
      "train acc:  1.0  train loss:  0.0005636986967621614 --time: 10.070087671279907\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 11.224454641342163\n",
      "train acc:  1.0  train loss:  0.0005494646434469715 --time: 10.39565658569336\n",
      "validation:  0.7466666666666667 --max 0.78 --time: 11.649166822433472\n"
     ]
    }
   ],
   "source": [
    "print(train_language, test_language)\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(\"epoch \", i)\n",
    "    loss_accum = 0.0\n",
    "    batch_cnt = 0\n",
    "\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch, (x, lengths, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss_score = loss.cpu().item()\n",
    "\n",
    "        loss_accum += loss_score\n",
    "        batch_cnt += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    print(\"train acc: \", acc_cnt/(err_cnt+acc_cnt), \" train loss: \", loss_accum / batch_cnt, '--time:', time.time() - start_time)\n",
    "\n",
    "    model.eval()\n",
    "    acc_cnt = 0\n",
    "    err_cnt = 0\n",
    "\n",
    "    #start_time = time.time()\n",
    "    for x, lengths, y in valid_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x, lengths)\n",
    "\n",
    "        out_val, out_indices = torch.max(logits, dim=1)\n",
    "        target_val, tar_indices = torch.max(y, dim=1)\n",
    "\n",
    "        for i in range(len(out_indices)):\n",
    "            if out_indices[i] == tar_indices[i]:\n",
    "                acc_cnt += 1\n",
    "            else:\n",
    "                err_cnt += 1\n",
    "\n",
    "    current_acc = acc_cnt/(err_cnt+acc_cnt)\n",
    "    if current_acc > max_acc:\n",
    "        max_acc = current_acc\n",
    "                \n",
    "    print(\"validation: \", current_acc, '--max', max_acc, '--time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
